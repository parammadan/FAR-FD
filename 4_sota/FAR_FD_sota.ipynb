{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "vLGyxxRwcNFq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import average_precision_score\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCdTUn0Zbt7O",
        "outputId": "319cd6c1-35bb-49e0-d242-9ba54812b565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(118108, 32)\n"
          ]
        }
      ],
      "source": [
        "# loading the entire test embeddings\n",
        "path = \"../5_embeddings/cls_embeddings_time.struct_time(tm_year=2025, tm_mon=2, tm_mday=7, tm_hour=19, tm_min=40, tm_sec=22, tm_wday=4, tm_yday=38, tm_isdst=0).npy\"\n",
        "\n",
        "cls_embeddings = np.load(path)\n",
        "print(cls_embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cls</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card4</th>\n",
              "      <th>card6</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>...</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.291883</td>\n",
              "      <td>-0.329939</td>\n",
              "      <td>0.108390</td>\n",
              "      <td>-0.145421</td>\n",
              "      <td>-0.399322</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0.892993</td>\n",
              "      <td>0.871243</td>\n",
              "      <td>-0.359702</td>\n",
              "      <td>0.680504</td>\n",
              "      <td>-0.412094</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.030054</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.594876</td>\n",
              "      <td>-1.467121</td>\n",
              "      <td>8.134522</td>\n",
              "      <td>-0.109308</td>\n",
              "      <td>0.711822</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.123148</td>\n",
              "      <td>-0.156138</td>\n",
              "      <td>-0.422421</td>\n",
              "      <td>1.487250</td>\n",
              "      <td>-0.265218</td>\n",
              "      <td>...</td>\n",
              "      <td>0.341765</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>1.611964</td>\n",
              "      <td>1.677853</td>\n",
              "      <td>-0.317889</td>\n",
              "      <td>-0.081355</td>\n",
              "      <td>-0.265218</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76765</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>-1.327921</td>\n",
              "      <td>-1.260784</td>\n",
              "      <td>-0.113217</td>\n",
              "      <td>-0.653259</td>\n",
              "      <td>-1.606254</td>\n",
              "      <td>...</td>\n",
              "      <td>1.009878</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76766</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>25</td>\n",
              "      <td>0.675641</td>\n",
              "      <td>0.648266</td>\n",
              "      <td>-0.075376</td>\n",
              "      <td>-0.002802</td>\n",
              "      <td>0.756523</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76767</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>0.418154</td>\n",
              "      <td>0.377752</td>\n",
              "      <td>0.150412</td>\n",
              "      <td>-1.485918</td>\n",
              "      <td>-0.226903</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>0.393449</td>\n",
              "      <td>0.090945</td>\n",
              "      <td>0.276274</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76768</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>0.605578</td>\n",
              "      <td>0.576883</td>\n",
              "      <td>-0.322279</td>\n",
              "      <td>-0.182963</td>\n",
              "      <td>0.577719</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76769</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>53</td>\n",
              "      <td>0.944849</td>\n",
              "      <td>0.929392</td>\n",
              "      <td>-0.317889</td>\n",
              "      <td>1.061025</td>\n",
              "      <td>1.229079</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>0.393449</td>\n",
              "      <td>0.090945</td>\n",
              "      <td>0.276274</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>76770 rows Ã— 182 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       cls  ProductCD  card4  card6  P_emaildomain  Unnamed: 0  TransactionDT  \\\n",
              "0        0          4      2      1              2   -0.291883      -0.329939   \n",
              "1        0          4      3      2             16    0.892993       0.871243   \n",
              "2        0          4      2      2              1   -1.594876      -1.467121   \n",
              "3        0          4      3      2             19   -0.123148      -0.156138   \n",
              "4        0          4      3      2             16    1.611964       1.677853   \n",
              "...    ...        ...    ...    ...            ...         ...            ...   \n",
              "76765    0          4      3      2             16   -1.327921      -1.260784   \n",
              "76766    0          4      2      2             25    0.675641       0.648266   \n",
              "76767    0          4      1      1             16    0.418154       0.377752   \n",
              "76768    0          4      3      2             19    0.605578       0.576883   \n",
              "76769    0          4      3      2             53    0.944849       0.929392   \n",
              "\n",
              "       TransactionAmt     card1     card2  ...      V312      V313      V314  \\\n",
              "0            0.108390 -0.145421 -0.399322  ... -0.227583 -0.222385 -0.249222   \n",
              "1           -0.359702  0.680504 -0.412094  ... -0.030054 -0.222385 -0.249222   \n",
              "2            8.134522 -0.109308  0.711822  ... -0.227583 -0.222385 -0.249222   \n",
              "3           -0.422421  1.487250 -0.265218  ...  0.341765 -0.222385 -0.249222   \n",
              "4           -0.317889 -0.081355 -0.265218  ... -0.227583 -0.222385 -0.249222   \n",
              "...               ...       ...       ...  ...       ...       ...       ...   \n",
              "76765       -0.113217 -0.653259 -1.606254  ...  1.009878 -0.222385 -0.249222   \n",
              "76766       -0.075376 -0.002802  0.756523  ... -0.227583 -0.222385 -0.249222   \n",
              "76767        0.150412 -1.485918 -0.226903  ... -0.227583  0.393449  0.090945   \n",
              "76768       -0.322279 -0.182963  0.577719  ... -0.227583 -0.222385 -0.249222   \n",
              "76769       -0.317889  1.061025  1.229079  ... -0.227583  0.393449  0.090945   \n",
              "\n",
              "           V315      V316      V317      V318      V319      V320      V321  \n",
              "0     -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "1     -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "2     -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "3     -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "4     -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "76765 -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "76766 -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "76767  0.276274 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "76768 -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "76769  0.276274 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "\n",
              "[76770 rows x 182 columns]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"../2_dataset/final/train_df.csv\")\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(76770, 1)"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_y = pd.read_csv(\"../2_dataset/final/train_y_df.csv\")\n",
        "# train_y['isFraud'] = train_y['isFraud'].astype(np.float32)\n",
        "train_y.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(23622, 182)"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df = pd.read_csv(\"../2_dataset/final/test_df.csv\")\n",
        "test_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_y_df = pd.read_csv(\"../2_dataset/final/test_y_df.csv\")\n",
        "test_y_df['isFraud'] = test_y_df['isFraud'].astype(np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        0.0\n",
              "1        0.0\n",
              "2        0.0\n",
              "3        0.0\n",
              "4        0.0\n",
              "        ... \n",
              "23617    0.0\n",
              "23618    0.0\n",
              "23619    1.0\n",
              "23620    0.0\n",
              "23621    0.0\n",
              "Name: isFraud, Length: 23622, dtype: float32"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_y_df['isFraud']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_df = pd.read_csv(\"../2_dataset/final/val_df.csv\")\n",
        "val_y_df = pd.read_csv(\"../2_dataset/final/val_y_df.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cls\n",
            "ProductCD\n",
            "card4\n",
            "card6\n",
            "P_emaildomain\n",
            "Unnamed: 0\n",
            "TransactionDT\n",
            "TransactionAmt\n",
            "card1\n",
            "card2\n",
            "card3\n",
            "card5\n",
            "addr1\n",
            "addr2\n",
            "C1\n",
            "C2\n",
            "C3\n",
            "C4\n",
            "C5\n",
            "C6\n",
            "C7\n",
            "C8\n",
            "C9\n",
            "C10\n",
            "C11\n",
            "C12\n",
            "C13\n",
            "C14\n",
            "D1\n",
            "D10\n",
            "D15\n",
            "V12\n",
            "V13\n",
            "V14\n",
            "V15\n",
            "V16\n",
            "V17\n",
            "V18\n",
            "V19\n",
            "V20\n",
            "V21\n",
            "V22\n",
            "V23\n",
            "V24\n",
            "V25\n",
            "V26\n",
            "V27\n",
            "V28\n",
            "V29\n",
            "V30\n",
            "V31\n",
            "V32\n",
            "V33\n",
            "V34\n",
            "V53\n",
            "V54\n",
            "V55\n",
            "V56\n",
            "V57\n",
            "V58\n",
            "V59\n",
            "V60\n",
            "V61\n",
            "V62\n",
            "V63\n",
            "V64\n",
            "V65\n",
            "V66\n",
            "V67\n",
            "V68\n",
            "V69\n",
            "V70\n",
            "V71\n",
            "V72\n",
            "V73\n",
            "V74\n",
            "V75\n",
            "V76\n",
            "V77\n",
            "V78\n",
            "V79\n",
            "V80\n",
            "V81\n",
            "V82\n",
            "V83\n",
            "V84\n",
            "V85\n",
            "V86\n",
            "V87\n",
            "V88\n",
            "V89\n",
            "V90\n",
            "V91\n",
            "V92\n",
            "V93\n",
            "V94\n",
            "V95\n",
            "V96\n",
            "V97\n",
            "V98\n",
            "V99\n",
            "V100\n",
            "V101\n",
            "V102\n",
            "V103\n",
            "V104\n",
            "V105\n",
            "V106\n",
            "V107\n",
            "V108\n",
            "V109\n",
            "V110\n",
            "V111\n",
            "V112\n",
            "V113\n",
            "V114\n",
            "V115\n",
            "V116\n",
            "V117\n",
            "V118\n",
            "V119\n",
            "V120\n",
            "V121\n",
            "V122\n",
            "V123\n",
            "V124\n",
            "V125\n",
            "V126\n",
            "V127\n",
            "V128\n",
            "V129\n",
            "V130\n",
            "V131\n",
            "V132\n",
            "V133\n",
            "V134\n",
            "V135\n",
            "V136\n",
            "V137\n",
            "V279\n",
            "V280\n",
            "V281\n",
            "V282\n",
            "V283\n",
            "V284\n",
            "V285\n",
            "V286\n",
            "V287\n",
            "V288\n",
            "V289\n",
            "V290\n",
            "V291\n",
            "V292\n",
            "V293\n",
            "V294\n",
            "V295\n",
            "V296\n",
            "V297\n",
            "V298\n",
            "V299\n",
            "V300\n",
            "V301\n",
            "V302\n",
            "V303\n",
            "V304\n",
            "V305\n",
            "V306\n",
            "V307\n",
            "V308\n",
            "V309\n",
            "V310\n",
            "V311\n",
            "V312\n",
            "V313\n",
            "V314\n",
            "V315\n",
            "V316\n",
            "V317\n",
            "V318\n",
            "V319\n",
            "V320\n",
            "V321\n",
            "isFraud\n"
          ]
        }
      ],
      "source": [
        "train_combined = pd.concat([train_df, train_y], axis=1)\n",
        "for i in train_combined.columns:\n",
        "    print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.23860373552977954\n",
            "0.2335443492578088\n",
            "0.17876509632033027\n",
            "0.17208818086369293\n",
            "0.17140777222739775\n",
            "0.17129754151082915\n",
            "0.1694509533059694\n",
            "0.16869624549983075\n",
            "0.16856514581061696\n",
            "0.16807522199501426\n",
            "0.16784609669254302\n",
            "0.16673519181013005\n",
            "0.16494405336149928\n",
            "0.1635479726062006\n",
            "0.16223883339984835\n",
            "0.1609492818314699\n",
            "0.15539194280744686\n",
            "0.1532637205184111\n",
            "0.15285396198355342\n",
            "0.15242327369684092\n",
            "0.15207969986721587\n",
            "0.15136940132184074\n",
            "0.15019903227643822\n",
            "0.14906566950159525\n",
            "0.1471338535487183\n",
            "0.14695599712549678\n",
            "0.1451580952545996\n",
            "0.14005471970353808\n",
            "0.13982554431256586\n",
            "0.1368602756038374\n",
            "0.13448693076368148\n",
            "0.13371257061792763\n",
            "0.13296303386586886\n",
            "0.1319550131385903\n",
            "0.13012982907439313\n",
            "0.13002389050113175\n",
            "0.12737547370279503\n",
            "0.12611801988193777\n",
            "0.12436065686779674\n",
            "0.12012175924886781\n",
            "0.12004693996679176\n",
            "0.11508280384875622\n",
            "0.10609496545951427\n",
            "0.10446228468029638\n",
            "0.09977475748977936\n",
            "0.09723238428976462\n",
            "0.09565859643023265\n",
            "0.08350499518970603\n",
            "0.08319386048403277\n",
            "0.08036134635849733\n",
            "0.06550928503462541\n",
            "0.06538456005335085\n",
            "0.061621834056766064\n",
            "0.060243384101252566\n",
            "0.05887530419733354\n",
            "0.05750396584721437\n",
            "0.04928486765226218\n",
            "0.04664200677914096\n",
            "0.0433670327989245\n",
            "0.042901386075026506\n",
            "0.04043601812937324\n",
            "0.039804665339588576\n",
            "0.03859727098759947\n",
            "0.03788719047732132\n",
            "0.03633544132151876\n",
            "0.03497619712678459\n",
            "0.03453895873216309\n",
            "0.034362966219872025\n",
            "0.034314120682642495\n",
            "0.03395552654329176\n",
            "0.032814161460500674\n",
            "0.03244281406830534\n",
            "0.031225054732188843\n",
            "0.030945423793205643\n",
            "0.030585489900198017\n",
            "0.030028701756984542\n",
            "0.029674065815715994\n",
            "0.029215399453471843\n",
            "0.029206196305272332\n",
            "0.028831099962005172\n",
            "0.028817710375873\n",
            "0.028635118784702988\n",
            "0.02821114775006738\n",
            "0.026917265338126745\n",
            "0.026349998330496373\n",
            "0.02626076377693249\n",
            "0.024359133421743378\n",
            "0.02419178251695025\n",
            "0.02409583472695414\n",
            "0.020508650689608752\n",
            "0.017299699467691994\n",
            "0.01624372425809288\n",
            "0.01514187790548084\n",
            "0.0150106929395986\n",
            "0.014214471006744639\n",
            "0.012086549997765796\n",
            "0.010863597903923996\n",
            "0.010307630479138307\n",
            "0.010221546396727992\n",
            "0.009864264674066136\n",
            "0.009577043148853067\n",
            "0.008988722774044408\n",
            "0.00889483101353241\n",
            "0.007863366061578553\n",
            "0.006122435477935741\n",
            "0.005705160806915783\n",
            "0.005652938562295218\n",
            "0.005467270922213576\n",
            "0.00489851606702102\n",
            "0.004324792035367803\n",
            "0.0039894393491256615\n",
            "0.003939605067890587\n",
            "0.0036728528733150335\n",
            "0.003452717629028396\n",
            "0.0028189969950266447\n",
            "0.002055009409809052\n",
            "0.0010196391767705055\n",
            "0.000962626039669884\n",
            "0.0008123724779407376\n",
            "0.0007381389630645173\n",
            "0.0005265988945421774\n",
            "0.00036847106962822326\n",
            "0.0003268429029397017\n",
            "0.0002692881903297088\n",
            "-0.00011264640960626655\n",
            "-0.00033349745301826465\n",
            "-0.0009250029757590554\n",
            "-0.0010037052471762655\n",
            "-0.001150690528541956\n",
            "-0.001227629740305078\n",
            "-0.0022541364091207762\n",
            "-0.002287312879314893\n",
            "-0.002326011773615641\n",
            "-0.0030768202890210593\n",
            "-0.003413665997223901\n",
            "-0.0034544235552503534\n",
            "-0.0034574478223782\n",
            "-0.0035356798626179943\n",
            "-0.003601984626502935\n",
            "-0.003760511317467775\n",
            "-0.0038716613425970837\n",
            "-0.0038747222179659566\n",
            "-0.004087997939017246\n",
            "-0.004240813566458423\n",
            "-0.0043014581484777615\n",
            "-0.004441461002938856\n",
            "-0.00453096388801924\n",
            "-0.004577072829069865\n",
            "-0.004674321112535578\n",
            "-0.004689786226801865\n",
            "-0.004851861912496841\n",
            "-0.005460819931474402\n",
            "-0.006254421214946906\n",
            "-0.012375863152342072\n",
            "-0.012957316659020668\n",
            "-0.013170874596403877\n",
            "-0.014914174942790478\n",
            "-0.020015135156388076\n",
            "-0.022497498718379645\n",
            "-0.022687032468562376\n",
            "-0.029686346031532154\n",
            "-0.03029793068311646\n",
            "-0.03148042976707473\n",
            "-0.03167877236938937\n",
            "-0.03543321926530082\n",
            "-0.03837534322839086\n",
            "-0.041720070043006374\n",
            "-0.04288561630158555\n",
            "-0.04577474040755437\n",
            "-0.06606098153059671\n",
            "-0.07344203297881602\n",
            "-0.07811129808496244\n",
            "-0.09651232352834042\n",
            "-0.09755349913354665\n",
            "-0.09766327180951229\n",
            "-0.10145437397151041\n",
            "-0.10151749106984723\n",
            "-0.10157896387142525\n",
            "-0.10176485689739848\n",
            "-0.16344667975008556\n",
            "nan\n",
            "nan\n"
          ]
        }
      ],
      "source": [
        "correlation_matrix = train_combined.corr()\n",
        "\n",
        "# Extract correlation with target variable 'isFraud'\n",
        "target_correlation = correlation_matrix[\"isFraud\"].sort_values(ascending=False)\n",
        "\n",
        "# Plot the heatmap\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.barh(target_correlation.index, target_correlation.values, color=\"skyblue\")\n",
        "# plt.xlabel(\"Correlation Coefficient\")\n",
        "# plt.ylabel(\"Features\")\n",
        "# plt.title(\"Feature Correlation with isFraud\")\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "\n",
        "# Display the correlation matrix as a table\n",
        "target_correlation = correlation_matrix[\"isFraud\"].sort_values(ascending=False)\n",
        "\n",
        "# Print correlation with target variable\n",
        "# print(\"Correlation with isFraud:\\n\", target_correlation[0:15])\n",
        "\n",
        "for i in target_correlation:\n",
        "    print(i)\n",
        "\n",
        "\n",
        "# V86          0.238604\n",
        "# V87          0.233544\n",
        "# V79          0.178765\n",
        "# V94          0.172088\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (1855591777.py, line 5)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[98], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"Range of 'V86': {column_range} low: {train_df[\"V86\"].min()}  high: {train_df[\"V86\"].max()}\")\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "train_combined[\"V86\"]\n",
        "column_range = train_df[\"V86\"].max() - train_df[\"V86\"].min()\n",
        "\n",
        "# Print the range\n",
        "print(f\"Range of 'V86': {column_range} low: {train_df[\"V86\"].min()}  high: {train_df[\"V86\"].max()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_combined[\"V86\"].unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_combined[\"V94\"].unique()\n",
        "\n",
        "# V86          0.238604\n",
        "# V87          0.233544\n",
        "# V79          0.178765\n",
        "# V94          0.172088\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Splitting Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute 65% of the total rows\n",
        "total_rows = cls_embeddings.shape[0]\n",
        "train_size = int(0.65 * total_rows)  # 65% of 118108\n",
        "\n",
        "# Slice the top 65%\n",
        "train_embeddings = cls_embeddings[:train_size]  # First 65%\n",
        "\n",
        "# print(f\"Total embed shape: {cls_embeddings.shape}\")\n",
        "# print(f\"Train embed shape: {train_embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_rows = cls_embeddings.shape[0]\n",
        "test_size = int(0.8 * total_rows) \n",
        "\n",
        "test_embeddings = cls_embeddings[test_size:]\n",
        "# print(f\"Total embed shape: {cls_embeddings.shape}\")\n",
        "# print(f\"Train embed shape: {test_embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17716"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_embeddings = cls_embeddings[train_size:test_size]\n",
        "len(val_embeddings) # 15% of total rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Faiss Index and Similarity Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA6ULff8bxV-",
        "outputId": "33b81774-3e8c-48c4-cd3a-f81370fc6775"
      },
      "outputs": [],
      "source": [
        "def create_index(num_embeddings, dimension):\n",
        "\n",
        "    # num_embeddings = 76770\n",
        "    # dimension = 32\n",
        "\n",
        "    index = faiss.IndexFlatL2(dimension)  # L2 similarity\n",
        "\n",
        "    index.add(train_embeddings)  # index of pre-computed embeddings\n",
        "\n",
        "    # k = 120  # as best result for 120\n",
        "\n",
        "    return index\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_faiss(query_vector):\n",
        "\n",
        "    index = create_index(76770, 32)\n",
        "\n",
        "    # Convert PyTorch tensor to NumPy\n",
        "    if isinstance(query_vector, torch.Tensor):\n",
        "        query_vector = query_vector.detach().cpu().numpy()\n",
        "\n",
        "    query_vector = query_vector.astype(\"float32\").reshape(1, -1)\n",
        "\n",
        "\n",
        "    query_vector = query_vector.astype(\"float32\").reshape(1, -1)\n",
        "\n",
        "    distances, indices = index.search(query_vector, k=120) # by default using euclidean distance for similarity\n",
        "    indices = indices.flatten()\n",
        "\n",
        "\n",
        "    return distances, indices\n",
        "\n",
        "# distances, indices = index.search(\n",
        "#     query_vector, k\n",
        "# )  \n",
        "\n",
        "# print(\"Input Sample embedding:\", query_vector)\n",
        "# print(\"Indices of nearest neighbors:\", indices)\n",
        "# print(\"L2 norm distances\", distances)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## L2 Distance Component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJLNSJdcbznQ"
      },
      "outputs": [],
      "source": [
        "def compute_similarity(distances, dropout=0.2):\n",
        "\n",
        "    # print(f\"distances.shape before: {distances.shape}\")\n",
        "    distances = distances.flatten()\n",
        "    # print(f\"distances.shape after: {distances.shape}\")\n",
        "\n",
        "    # Apply softmax to the negative distances\n",
        "    similarities = np.exp(-distances)\n",
        "    softmax_scores = similarities / np.sum(similarities)\n",
        "\n",
        "    # Apply dropout (randomly zero out some softmax scores)\n",
        "    dropout_mask = np.random.binomial(1, 1 - dropout, size=softmax_scores.shape)\n",
        "    dropped_softmax_scores = softmax_scores * dropout_mask\n",
        "\n",
        "    final_softmax = dropped_softmax_scores / np.sum(dropped_softmax_scores)\n",
        "    # how to weigh in the final embedding?\n",
        "    return final_softmax, distances\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mask To Drop The Dropped Out Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_by_mask(arr1, arr2, arr3):\n",
        "    \"\"\"\n",
        "    Removes elements from arr2 and arr3 where corresponding indices in arr1 are zero.\n",
        "    \"\"\"\n",
        "    mask = arr1 != 0  # Create a boolean mask where arr1 is nonzero\n",
        "    return arr1[mask], arr2[mask], arr3[mask]\n",
        "\n",
        "# S_x_xi, indices, distances = filter_by_mask(S_x_xi, indices, distances)\n",
        "\n",
        "# print(\"Filtered S_x_xi:\", S_x_xi)\n",
        "# print(\"Filtered indices:\", indices)\n",
        "# print(\"Filtered distances:\", distances)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Trasnform(with MLPs), Value Component and Shap Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlB6RGt6cLid",
        "outputId": "27a8197e-bb5c-4bd2-f934-6fcc9eb8cb6f"
      },
      "outputs": [],
      "source": [
        "class MLP_L1(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MLP_L1, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "        self.activation1 = nn.SiLU()\n",
        "\n",
        "        self.layer2 = nn.Linear(32, 32)\n",
        "        self.activation2 = nn.SiLU()\n",
        "        self.dropout2 = nn.Dropout(p=0.2)\n",
        "\n",
        "        self.layer3 = nn.Linear(32, 32)  # Fix: Output should be 32\n",
        "        self.activation3 = nn.SiLU()  # Fix: Apply SiLU activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation1(self.layer1(x))\n",
        "        x = self.dropout2(self.activation2(self.layer2(x)))\n",
        "        x = self.activation3(self.layer3(x))  # Fix: Apply activation & dropout\n",
        "        return x\n",
        "\n",
        "\n",
        "# # Create model instance\n",
        "# model = MLP_L1(l1_dist.shape[0])\n",
        "\n",
        "# # Convert input to tensor and pass it through the model\n",
        "# l1_dist_tensor = torch.tensor(l1_dist, dtype=torch.float32)\n",
        "\n",
        "# w_v_l1 = model(l1_dist_tensor).detach().numpy()\n",
        "# print(w_v_l1)\n",
        "\n",
        "# print(w_v_l1.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXAE9Vzwcn2T",
        "outputId": "5541a00b-ec38-4c32-e4c2-0f6f8998f8fc"
      },
      "outputs": [],
      "source": [
        "def compute_l1(distances):\n",
        "    l1_dist = np.sqrt(distances)\n",
        "    # print(l1_dist, l1_dist.shape)\n",
        "\n",
        "    model = MLP_L1(l1_dist.shape[0])\n",
        "\n",
        "    # Convert input to tensor and pass it through the model\n",
        "    l1_dist_tensor = torch.tensor(l1_dist, dtype=torch.float32)\n",
        "\n",
        "    w_v_l1 = model(l1_dist_tensor).detach().numpy()\n",
        "    # print(w_v_l1)\n",
        "\n",
        "    # print(w_v_l1.shape)\n",
        "\n",
        "    return w_v_l1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modified MLP to output shape (60,)\n",
        "class MLP_Wy(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MLP_Wy, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)  # First layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 32)  # Output layer (1 neuron)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)  # Shape (60, 1)\n",
        "        return x.squeeze(1)  # Shape (60,)\n",
        "\n",
        "# Instantiate MLP with input_dim=32 (from Wy)\n",
        " # Expected: (60,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_wy(indices):\n",
        "    y_i = train_y['isFraud'].loc[indices].values\n",
        "    # print(f\"y_i.shape: {y_i.shape}\")\n",
        "\n",
        "\n",
        "# Convert to PyTorch tensor\n",
        "    feature_target_tensor = torch.tensor(y_i, dtype=torch.long)\n",
        "\n",
        "# Define Wy: An embedding layer to map to 32-dim space\n",
        "    embedding_dim = 32\n",
        "    num_classes = 2  # Since input values are 0 or 1\n",
        "\n",
        "    Wy = nn.Embedding(num_classes, embedding_dim)\n",
        "\n",
        "    mlp = MLP_Wy(input_dim=embedding_dim)\n",
        "\n",
        "# Compute embeddings using Wy\n",
        "    embeddings = Wy(feature_target_tensor)  # Shape: (60, 32)\n",
        "\n",
        "    # Pass embeddings through MLP\n",
        "    w_y = mlp(embeddings)  # Shape: (60,)\n",
        "\n",
        "    # print(\"MLP Output Shape:\", w_y.shape) \n",
        "\n",
        "    return w_y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_imp_features(indices):\n",
        "    v_86 = train_df['C14'].loc[indices].values\n",
        "    feature_target_tensor = torch.tensor(v_86, dtype=torch.float32).unsqueeze(-1)\n",
        "    # print(f\"V86 : {v_86}\")\n",
        "\n",
        "\n",
        "    v_87 = train_df['C13'].loc[indices].values\n",
        "    feature_target_tensor_1 = torch.tensor(v_87, dtype=torch.float32).unsqueeze(-1)\n",
        "\n",
        "    v_79 = train_df['C1'].loc[indices].values\n",
        "    feature_target_tensor_2 = torch.tensor(v_79, dtype=torch.float32).unsqueeze(-1)\n",
        "\n",
        "    v_94 = train_df['C2'].loc[indices].values\n",
        "    feature_target_tensor_3 = torch.tensor(v_94, dtype=torch.float32).unsqueeze(-1)\n",
        "    # print(feature_target_tensor, feature_target_tensor_1, feature_target_tensor_2, feature_target_tensor_3)\n",
        "\n",
        "    # Convert to PyTorch tensor\n",
        "\n",
        "    # Define Wx: A linear layer to project to 32-dim space\n",
        "    input_dim = 1  # Since each input is a single continuous value\n",
        "    embedding_dim = 32\n",
        "\n",
        "    # Define MLP and projection layers\n",
        "    W_feat = nn.Linear(input_dim, embedding_dim)\n",
        "\n",
        "    mlp = MLP_Wy(input_dim=embedding_dim)\n",
        "\n",
        "    # Process first variable\n",
        "    embeddings_1 = W_feat(feature_target_tensor)  \n",
        "    feat_x1 = mlp(embeddings_1)  \n",
        "    # print(\"MLP Output Shape for Var 1:\", feat_x1.shape)\n",
        "    feat_x1 = feat_x1.detach().numpy()\n",
        "\n",
        "    # Process second variable\n",
        "    embeddings_2 = W_feat(feature_target_tensor_1)  \n",
        "    feat_x2 = mlp(embeddings_2)  \n",
        "    # print(\"MLP Output Shape for Var 2:\", feat_x2.shape)\n",
        "    feat_x2 = feat_x2.detach().numpy()\n",
        "\n",
        "    # Process third variable\n",
        "    embeddings_3 = W_feat(feature_target_tensor_2)  \n",
        "    feat_x3 = mlp(embeddings_3)  \n",
        "    # print(\"MLP Output Shape for Var 3:\", feat_x3.shape)\n",
        "    feat_x3 = feat_x3.detach().numpy()\n",
        "\n",
        "    # Process fourth variable\n",
        "    embeddings_4 = W_feat(feature_target_tensor_3)  \n",
        "    feat_x4 = mlp(embeddings_4)  \n",
        "    # print(\"MLP Output Shape for Var 3:\", feat_x4.shape)\n",
        "    feat_x4 = feat_x4.detach().numpy()\n",
        "\n",
        "\n",
        "    return feat_x1, feat_x2, feat_x3, feat_x4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_value(w_v_l1, w_y):\n",
        "    # Compute the dot product of w_v_l1 and w_y\n",
        "    w_y_npy = w_y.detach().numpy()\n",
        "    value = w_y_npy + w_v_l1\n",
        "    return value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Reshaping S to do S * V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_z_in(S_x_xi, value):\n",
        "    S_x_xi = S_x_xi.reshape(1, -1)\n",
        "\n",
        "    # Assuming S_x_xi is (1, 95) and value is (95, 32)\n",
        "    numerator = np.sum(S_x_xi @ value)  # Sum the weighted contributions (scalar)\n",
        "    denominator = np.sum(S_x_xi)        # Total sum of weights (scalar)\n",
        "    z_in = numerator / denominator         # Weighted average as a single scalar\n",
        "\n",
        "    return z_in\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(182,)"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_test = test_df.iloc[44].values\n",
        "x_test.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processing Samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Old Code for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# test_i = test_df.iloc[10].values #shape (182,)\n",
        "# query_vector = test_embeddings[10] #shape (32,)\n",
        "# distances, indices = search_faiss(query_vector) # both shape (120,) and flatten\n",
        "# S_x_xi, distances = compute_similarity(distances)\n",
        "# S_x_xi, indices, distances = filter_by_mask(S_x_xi, indices, distances)\n",
        "# w_v_l1 = compute_l1(distances) #shape (32,)\n",
        "# w_y = compute_wy(indices) #shape (32,)\n",
        "\n",
        "\n",
        "# feat_x_1, feat_x2, feat_x3, feat_x4 = compute_imp_features(indices=indices)\n",
        "# #z_in \n",
        "# f_z_1 = compute_z_in(S_x_xi, feat_x_1)\n",
        "# f_z_2 = compute_z_in(S_x_xi, feat_x2)\n",
        "# f_z_3 = compute_z_in(S_x_xi, feat_x3)\n",
        "# f_z_4 = compute_z_in(S_x_xi, feat_x4)\n",
        "\n",
        "#f_z_1, f_z_2, f_z_3, f_z_4\n",
        "\n",
        "# value = compute_value(w_v_l1, w_y)\n",
        "# value.shape\n",
        "\n",
        "# labels = test_y_df['isFraud']\n",
        "# labels = torch.tensor(test_y_df['isFraud'].values, dtype=torch.float32)\n",
        "# type(labels)\n",
        "\n",
        "# type(test_df.iloc[34].values)\n",
        "\n",
        "\n",
        "#----------------Without Batch Processing----------------\n",
        "# def process_samples(test_df, test_embeddings):\n",
        "#     \"\"\"Processes all samples and returns input tensor and labels.\"\"\"\n",
        "#     input_list = []\n",
        "#     labels = []\n",
        "\n",
        "\n",
        "#     for i in range(len(test_df)):  # Process all samples\n",
        "#         test_i = torch.tensor(test_df.iloc[i].values, dtype=torch.float32)  # shape (182,)\n",
        "#         query_embedding = torch.tensor(test_embeddings[i], dtype=torch.float32)  # shape (32,)\n",
        "\n",
        "#         distances, indices = search_faiss(query_embedding)\n",
        "#         S_x_xi, distances = compute_similarity(distances)\n",
        "#         S_x_xi, indices, distances = filter_by_mask(S_x_xi, indices, distances)\n",
        "#         w_v_l1 = compute_l1(distances)  # shape (32,)\n",
        "#         w_y = compute_wy(indices)  # shape (32,)\n",
        "#         value = compute_value(w_v_l1, w_y)\n",
        "#         z_in = compute_z_in(S_x_xi, value)  # Scalar (single value)\n",
        "\n",
        "#         feat_x_1, feat_x2, feat_x3, feat_x4 = compute_imp_features(indices=indices) #v86 feature\n",
        "#         f_z_in_1 = compute_z_in(S_x_xi, feat_x_1)\n",
        "\n",
        "#         f_z_in_2 = compute_z_in(S_x_xi, feat_x2)\n",
        "\n",
        "#         f_z_in_3 = compute_z_in(S_x_xi, feat_x3)\n",
        "\n",
        "#         f_z_in_4 = compute_z_in(S_x_xi, feat_x4)\n",
        "\n",
        "#         # Append only test_i (input_emb) and z_in (weighted_avg), not query_embedding\n",
        "#         z_in = torch.tensor(z_in, dtype=torch.float32)\n",
        "#         f_z_in_1 = torch.tensor(f_z_in_1, dtype=torch.float32)\n",
        "#         f_z_in_2 = torch.tensor(f_z_in_2, dtype=torch.float32)\n",
        "#         f_z_in_3 = torch.tensor(f_z_in_3, dtype=torch.float32)\n",
        "#         f_z_in_4 = torch.tensor(f_z_in_4, dtype=torch.float32)\n",
        "\n",
        "#         input_list.append((test_i, z_in, f_z_in_1, f_z_in_2, f_z_in_3, f_z_in_4))\n",
        "\n",
        "#         labels.append(torch.tensor(test_y_df.iloc[i]['isFraud'], dtype=torch.float32))  # Assuming label is in df\n",
        "\n",
        "\n",
        "\n",
        "#     return input_list, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_samples(test_df, test_embeddings, batch_size=1000, delay=0.5):\n",
        "    \"\"\"Processes samples in batches to prevent memory overflow.\"\"\"\n",
        "    input_list = []\n",
        "    labels = []\n",
        "\n",
        "    total_samples = len(test_df)\n",
        "    num_batches = (total_samples + batch_size - 1) // batch_size  # Compute number of batches\n",
        "\n",
        "    for batch_idx in range(num_batches):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min((batch_idx + 1) * batch_size, total_samples)\n",
        "\n",
        "        batch_inputs = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for i in range(start_idx, end_idx):  # Process each sample in batch\n",
        "            test_i = torch.tensor(test_df.iloc[i].values, dtype=torch.float32)  # shape (182,)\n",
        "            query_embedding = torch.tensor(test_embeddings[i], dtype=torch.float32)  # shape (32,)\n",
        "\n",
        "            distances, indices = search_faiss(query_embedding)\n",
        "            S_x_xi, distances = compute_similarity(distances)\n",
        "            S_x_xi, indices, distances = filter_by_mask(S_x_xi, indices, distances)\n",
        "            w_v_l1 = compute_l1(distances)  # shape (32,)\n",
        "            w_y = compute_wy(indices)  # shape (32,)\n",
        "            value = compute_value(w_v_l1, w_y)\n",
        "            z_in = compute_z_in(S_x_xi, value)  # Scalar (single value)\n",
        "\n",
        "            feat_x_1, feat_x2, feat_x3, feat_x4 = compute_imp_features(indices=indices) #v86 feature\n",
        "            f_z_in_1 = compute_z_in(S_x_xi, feat_x_1)\n",
        "            f_z_in_2 = compute_z_in(S_x_xi, feat_x2)\n",
        "            f_z_in_3 = compute_z_in(S_x_xi, feat_x3)\n",
        "            f_z_in_4 = compute_z_in(S_x_xi, feat_x4)\n",
        "\n",
        "            # Convert to tensors\n",
        "            z_in = torch.tensor(z_in, dtype=torch.float32)\n",
        "            f_z_in_1 = torch.tensor(f_z_in_1, dtype=torch.float32)\n",
        "            f_z_in_2 = torch.tensor(f_z_in_2, dtype=torch.float32)\n",
        "            f_z_in_3 = torch.tensor(f_z_in_3, dtype=torch.float32)\n",
        "            f_z_in_4 = torch.tensor(f_z_in_4, dtype=torch.float32)\n",
        "\n",
        "            batch_inputs.append((test_i, z_in, f_z_in_1, f_z_in_2, f_z_in_3, f_z_in_4))\n",
        "            batch_labels.append(torch.tensor(train_y.iloc[i]['isFraud'], dtype=torch.float32))\n",
        "\n",
        "        input_list.extend(batch_inputs)\n",
        "        labels.extend(batch_labels)\n",
        "        \n",
        "        time.sleep(delay)  # Add delay to avoid system overload\n",
        "    \n",
        "    return input_list, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "def val_process_samples(test_df, test_embeddings, batch_size=1000, delay=0.5):\n",
        "    \"\"\"Processes samples in batches to prevent memory overflow.\"\"\"\n",
        "    input_list = []\n",
        "    labels = []\n",
        "\n",
        "    total_samples = len(test_df)\n",
        "    num_batches = (total_samples + batch_size - 1) // batch_size  # Compute number of batches\n",
        "\n",
        "    for batch_idx in range(num_batches):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min((batch_idx + 1) * batch_size, total_samples)\n",
        "\n",
        "        batch_inputs = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for i in range(start_idx, end_idx):  # Process each sample in batch\n",
        "            test_i = torch.tensor(test_df.iloc[i].values, dtype=torch.float32)  # shape (182,)\n",
        "            query_embedding = torch.tensor(test_embeddings[i], dtype=torch.float32)  # shape (32,)\n",
        "\n",
        "            distances, indices = search_faiss(query_embedding)\n",
        "            S_x_xi, distances = compute_similarity(distances)\n",
        "            S_x_xi, indices, distances = filter_by_mask(S_x_xi, indices, distances)\n",
        "            w_v_l1 = compute_l1(distances)  # shape (32,)\n",
        "            w_y = compute_wy(indices)  # shape (32,)\n",
        "            value = compute_value(w_v_l1, w_y)\n",
        "            z_in = compute_z_in(S_x_xi, value)  # Scalar (single value)\n",
        "\n",
        "            feat_x_1, feat_x2, feat_x3, feat_x4 = compute_imp_features(indices=indices) #v86 feature\n",
        "            f_z_in_1 = compute_z_in(S_x_xi, feat_x_1)\n",
        "            f_z_in_2 = compute_z_in(S_x_xi, feat_x2)\n",
        "            f_z_in_3 = compute_z_in(S_x_xi, feat_x3)\n",
        "            f_z_in_4 = compute_z_in(S_x_xi, feat_x4)\n",
        "\n",
        "            # Convert to tensors\n",
        "            z_in = torch.tensor(z_in, dtype=torch.float32)\n",
        "            f_z_in_1 = torch.tensor(f_z_in_1, dtype=torch.float32)\n",
        "            f_z_in_2 = torch.tensor(f_z_in_2, dtype=torch.float32)\n",
        "            f_z_in_3 = torch.tensor(f_z_in_3, dtype=torch.float32)\n",
        "            f_z_in_4 = torch.tensor(f_z_in_4, dtype=torch.float32)\n",
        "\n",
        "            batch_inputs.append((test_i, z_in, f_z_in_1, f_z_in_2, f_z_in_3, f_z_in_4))\n",
        "            batch_labels.append(torch.tensor(val_y_df.iloc[i]['isFraud'], dtype=torch.float32))\n",
        "\n",
        "        input_list.extend(batch_inputs)\n",
        "        labels.extend(batch_labels)\n",
        "        \n",
        "        time.sleep(delay)  # Add delay to avoid system overload\n",
        "    \n",
        "    return input_list, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ThreeBlockModel(nn.Module):\n",
        "    def __init__(self, input_emb_dim, hidden_dim=32, dropout_prob=0.2):\n",
        "        super(ThreeBlockModel, self).__init__()\n",
        "\n",
        "        self.input_dim = input_emb_dim + 4  # Adding 2 for weighted_avg and f_z_in\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.LayerNorm(self.input_dim),\n",
        "            nn.Linear(self.input_dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "        )\n",
        "        \n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "        )\n",
        "        \n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "        )\n",
        "        \n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, input_emb, weighted_avg, f_z_in, f_z_in_2, f_z_in_3, f_z_in_4):\n",
        "        # Ensure correct shape for scalar inputs\n",
        "        weighted_avg = weighted_avg.unsqueeze(-1)  \n",
        "        f_z_in = f_z_in.unsqueeze(-1)  \n",
        "        f_z_in_2 = f_z_in_2.unsqueeze(-1)\n",
        "        f_z_in_3 = f_z_in_3.unsqueeze(-1)\n",
        "        f_z_in_4 = f_z_in_4.unsqueeze(-1)\n",
        "\n",
        "        # Concatenate all inputs\n",
        "        combined = torch.cat([input_emb, weighted_avg, f_z_in, f_z_in_3, f_z_in_4], dim=-1)\n",
        "\n",
        "        # Pass through MLP blocks\n",
        "        x = self.block1(combined)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Train Loss: 0.2793, Train AUCPR: 0.0381, Val Loss: 0.1369, Val AUCPR: 0.1864\n",
            "Epoch [2/100], Train Loss: 0.1322, Train AUCPR: 0.2211, Val Loss: 0.1265, Val AUCPR: 0.2825\n",
            "Epoch [3/100], Train Loss: 0.1239, Train AUCPR: 0.2828, Val Loss: 0.1184, Val AUCPR: 0.3272\n",
            "Epoch [4/100], Train Loss: 0.1186, Train AUCPR: 0.3122, Val Loss: 0.1155, Val AUCPR: 0.3371\n",
            "Epoch [5/100], Train Loss: 0.1162, Train AUCPR: 0.3276, Val Loss: 0.1141, Val AUCPR: 0.3420\n",
            "Epoch [6/100], Train Loss: 0.1148, Train AUCPR: 0.3378, Val Loss: 0.1132, Val AUCPR: 0.3459\n",
            "Epoch [7/100], Train Loss: 0.1138, Train AUCPR: 0.3459, Val Loss: 0.1126, Val AUCPR: 0.3499\n",
            "Epoch [8/100], Train Loss: 0.1130, Train AUCPR: 0.3523, Val Loss: 0.1122, Val AUCPR: 0.3532\n",
            "Epoch [9/100], Train Loss: 0.1124, Train AUCPR: 0.3580, Val Loss: 0.1118, Val AUCPR: 0.3559\n",
            "Epoch [10/100], Train Loss: 0.1119, Train AUCPR: 0.3627, Val Loss: 0.1115, Val AUCPR: 0.3584\n",
            "Epoch [11/100], Train Loss: 0.1114, Train AUCPR: 0.3670, Val Loss: 0.1113, Val AUCPR: 0.3606\n",
            "Epoch [12/100], Train Loss: 0.1109, Train AUCPR: 0.3710, Val Loss: 0.1111, Val AUCPR: 0.3625\n",
            "Epoch [13/100], Train Loss: 0.1105, Train AUCPR: 0.3748, Val Loss: 0.1109, Val AUCPR: 0.3645\n",
            "Epoch [14/100], Train Loss: 0.1101, Train AUCPR: 0.3784, Val Loss: 0.1107, Val AUCPR: 0.3665\n",
            "Epoch [15/100], Train Loss: 0.1097, Train AUCPR: 0.3819, Val Loss: 0.1105, Val AUCPR: 0.3679\n",
            "Epoch [16/100], Train Loss: 0.1093, Train AUCPR: 0.3853, Val Loss: 0.1103, Val AUCPR: 0.3696\n",
            "Epoch [17/100], Train Loss: 0.1089, Train AUCPR: 0.3886, Val Loss: 0.1102, Val AUCPR: 0.3712\n",
            "Epoch [18/100], Train Loss: 0.1085, Train AUCPR: 0.3919, Val Loss: 0.1100, Val AUCPR: 0.3728\n",
            "Epoch [19/100], Train Loss: 0.1081, Train AUCPR: 0.3952, Val Loss: 0.1099, Val AUCPR: 0.3741\n",
            "Epoch [20/100], Train Loss: 0.1077, Train AUCPR: 0.3984, Val Loss: 0.1098, Val AUCPR: 0.3753\n",
            "Epoch [21/100], Train Loss: 0.1073, Train AUCPR: 0.4016, Val Loss: 0.1096, Val AUCPR: 0.3767\n",
            "Epoch [22/100], Train Loss: 0.1069, Train AUCPR: 0.4048, Val Loss: 0.1095, Val AUCPR: 0.3781\n",
            "Epoch [23/100], Train Loss: 0.1066, Train AUCPR: 0.4081, Val Loss: 0.1094, Val AUCPR: 0.3793\n",
            "Epoch [24/100], Train Loss: 0.1062, Train AUCPR: 0.4113, Val Loss: 0.1093, Val AUCPR: 0.3804\n",
            "Epoch [25/100], Train Loss: 0.1058, Train AUCPR: 0.4146, Val Loss: 0.1092, Val AUCPR: 0.3819\n",
            "Epoch [26/100], Train Loss: 0.1054, Train AUCPR: 0.4180, Val Loss: 0.1091, Val AUCPR: 0.3835\n",
            "Epoch [27/100], Train Loss: 0.1050, Train AUCPR: 0.4214, Val Loss: 0.1090, Val AUCPR: 0.3848\n",
            "Epoch [28/100], Train Loss: 0.1046, Train AUCPR: 0.4247, Val Loss: 0.1089, Val AUCPR: 0.3864\n",
            "Epoch [29/100], Train Loss: 0.1042, Train AUCPR: 0.4280, Val Loss: 0.1088, Val AUCPR: 0.3881\n",
            "Epoch [30/100], Train Loss: 0.1038, Train AUCPR: 0.4314, Val Loss: 0.1086, Val AUCPR: 0.3898\n",
            "Epoch [31/100], Train Loss: 0.1034, Train AUCPR: 0.4346, Val Loss: 0.1085, Val AUCPR: 0.3914\n",
            "Epoch [32/100], Train Loss: 0.1030, Train AUCPR: 0.4379, Val Loss: 0.1084, Val AUCPR: 0.3930\n",
            "Epoch [33/100], Train Loss: 0.1027, Train AUCPR: 0.4411, Val Loss: 0.1083, Val AUCPR: 0.3945\n",
            "Epoch [34/100], Train Loss: 0.1023, Train AUCPR: 0.4443, Val Loss: 0.1081, Val AUCPR: 0.3962\n",
            "Epoch [35/100], Train Loss: 0.1019, Train AUCPR: 0.4475, Val Loss: 0.1080, Val AUCPR: 0.3980\n",
            "Epoch [36/100], Train Loss: 0.1015, Train AUCPR: 0.4506, Val Loss: 0.1079, Val AUCPR: 0.3992\n",
            "Epoch [37/100], Train Loss: 0.1012, Train AUCPR: 0.4537, Val Loss: 0.1077, Val AUCPR: 0.4004\n",
            "Epoch [38/100], Train Loss: 0.1008, Train AUCPR: 0.4567, Val Loss: 0.1076, Val AUCPR: 0.4017\n",
            "Epoch [39/100], Train Loss: 0.1005, Train AUCPR: 0.4597, Val Loss: 0.1075, Val AUCPR: 0.4031\n",
            "Epoch [40/100], Train Loss: 0.1001, Train AUCPR: 0.4626, Val Loss: 0.1073, Val AUCPR: 0.4044\n",
            "Epoch [41/100], Train Loss: 0.0998, Train AUCPR: 0.4654, Val Loss: 0.1072, Val AUCPR: 0.4055\n",
            "Epoch [42/100], Train Loss: 0.0995, Train AUCPR: 0.4682, Val Loss: 0.1071, Val AUCPR: 0.4067\n",
            "Epoch [43/100], Train Loss: 0.0992, Train AUCPR: 0.4710, Val Loss: 0.1070, Val AUCPR: 0.4078\n",
            "Epoch [44/100], Train Loss: 0.0988, Train AUCPR: 0.4737, Val Loss: 0.1069, Val AUCPR: 0.4090\n",
            "Epoch [45/100], Train Loss: 0.0985, Train AUCPR: 0.4764, Val Loss: 0.1068, Val AUCPR: 0.4102\n",
            "Epoch [46/100], Train Loss: 0.0982, Train AUCPR: 0.4791, Val Loss: 0.1067, Val AUCPR: 0.4112\n",
            "Epoch [47/100], Train Loss: 0.0979, Train AUCPR: 0.4818, Val Loss: 0.1067, Val AUCPR: 0.4123\n",
            "Epoch [48/100], Train Loss: 0.0976, Train AUCPR: 0.4844, Val Loss: 0.1066, Val AUCPR: 0.4135\n",
            "Epoch [49/100], Train Loss: 0.0973, Train AUCPR: 0.4870, Val Loss: 0.1066, Val AUCPR: 0.4147\n",
            "Epoch [50/100], Train Loss: 0.0970, Train AUCPR: 0.4896, Val Loss: 0.1065, Val AUCPR: 0.4156\n",
            "Epoch [51/100], Train Loss: 0.0967, Train AUCPR: 0.4922, Val Loss: 0.1065, Val AUCPR: 0.4164\n",
            "Epoch [52/100], Train Loss: 0.0964, Train AUCPR: 0.4948, Val Loss: 0.1065, Val AUCPR: 0.4170\n",
            "Epoch [53/100], Train Loss: 0.0961, Train AUCPR: 0.4974, Val Loss: 0.1064, Val AUCPR: 0.4175\n",
            "Epoch [54/100], Train Loss: 0.0958, Train AUCPR: 0.5000, Val Loss: 0.1064, Val AUCPR: 0.4182\n",
            "Epoch [55/100], Train Loss: 0.0955, Train AUCPR: 0.5025, Val Loss: 0.1064, Val AUCPR: 0.4189\n",
            "Epoch [56/100], Train Loss: 0.0952, Train AUCPR: 0.5051, Val Loss: 0.1064, Val AUCPR: 0.4196\n",
            "Epoch [57/100], Train Loss: 0.0949, Train AUCPR: 0.5076, Val Loss: 0.1064, Val AUCPR: 0.4200\n",
            "Epoch [58/100], Train Loss: 0.0946, Train AUCPR: 0.5100, Val Loss: 0.1065, Val AUCPR: 0.4205\n",
            "Epoch [59/100], Train Loss: 0.0944, Train AUCPR: 0.5125, Val Loss: 0.1065, Val AUCPR: 0.4210\n",
            "Epoch [60/100], Train Loss: 0.0941, Train AUCPR: 0.5149, Val Loss: 0.1065, Val AUCPR: 0.4214\n",
            "Epoch [61/100], Train Loss: 0.0938, Train AUCPR: 0.5174, Val Loss: 0.1065, Val AUCPR: 0.4219\n",
            "Epoch [62/100], Train Loss: 0.0935, Train AUCPR: 0.5197, Val Loss: 0.1066, Val AUCPR: 0.4224\n",
            "Epoch [63/100], Train Loss: 0.0933, Train AUCPR: 0.5220, Val Loss: 0.1066, Val AUCPR: 0.4228\n",
            "Epoch [64/100], Train Loss: 0.0930, Train AUCPR: 0.5242, Val Loss: 0.1067, Val AUCPR: 0.4231\n",
            "Epoch [65/100], Train Loss: 0.0928, Train AUCPR: 0.5264, Val Loss: 0.1067, Val AUCPR: 0.4236\n",
            "Epoch [66/100], Train Loss: 0.0925, Train AUCPR: 0.5287, Val Loss: 0.1067, Val AUCPR: 0.4239\n",
            "Epoch [67/100], Train Loss: 0.0922, Train AUCPR: 0.5309, Val Loss: 0.1068, Val AUCPR: 0.4243\n",
            "Epoch [68/100], Train Loss: 0.0920, Train AUCPR: 0.5330, Val Loss: 0.1068, Val AUCPR: 0.4250\n",
            "Epoch [69/100], Train Loss: 0.0918, Train AUCPR: 0.5350, Val Loss: 0.1069, Val AUCPR: 0.4255\n",
            "Epoch [70/100], Train Loss: 0.0915, Train AUCPR: 0.5371, Val Loss: 0.1069, Val AUCPR: 0.4258\n",
            "Epoch [71/100], Train Loss: 0.0913, Train AUCPR: 0.5391, Val Loss: 0.1069, Val AUCPR: 0.4260\n",
            "Epoch [72/100], Train Loss: 0.0910, Train AUCPR: 0.5411, Val Loss: 0.1070, Val AUCPR: 0.4265\n",
            "Epoch [73/100], Train Loss: 0.0908, Train AUCPR: 0.5431, Val Loss: 0.1070, Val AUCPR: 0.4269\n",
            "Epoch [74/100], Train Loss: 0.0906, Train AUCPR: 0.5450, Val Loss: 0.1071, Val AUCPR: 0.4273\n",
            "Epoch [75/100], Train Loss: 0.0903, Train AUCPR: 0.5469, Val Loss: 0.1071, Val AUCPR: 0.4276\n",
            "Epoch [76/100], Train Loss: 0.0901, Train AUCPR: 0.5488, Val Loss: 0.1072, Val AUCPR: 0.4279\n",
            "Epoch [77/100], Train Loss: 0.0899, Train AUCPR: 0.5507, Val Loss: 0.1072, Val AUCPR: 0.4283\n",
            "Epoch [78/100], Train Loss: 0.0897, Train AUCPR: 0.5526, Val Loss: 0.1072, Val AUCPR: 0.4288\n",
            "Epoch [79/100], Train Loss: 0.0895, Train AUCPR: 0.5544, Val Loss: 0.1073, Val AUCPR: 0.4292\n",
            "Epoch [80/100], Train Loss: 0.0892, Train AUCPR: 0.5562, Val Loss: 0.1073, Val AUCPR: 0.4294\n",
            "Epoch [81/100], Train Loss: 0.0890, Train AUCPR: 0.5580, Val Loss: 0.1074, Val AUCPR: 0.4298\n",
            "Epoch [82/100], Train Loss: 0.0888, Train AUCPR: 0.5598, Val Loss: 0.1074, Val AUCPR: 0.4302\n",
            "Epoch [83/100], Train Loss: 0.0886, Train AUCPR: 0.5616, Val Loss: 0.1075, Val AUCPR: 0.4304\n",
            "Epoch [84/100], Train Loss: 0.0884, Train AUCPR: 0.5633, Val Loss: 0.1075, Val AUCPR: 0.4306\n",
            "Epoch [85/100], Train Loss: 0.0882, Train AUCPR: 0.5650, Val Loss: 0.1076, Val AUCPR: 0.4309\n",
            "Epoch [86/100], Train Loss: 0.0880, Train AUCPR: 0.5667, Val Loss: 0.1076, Val AUCPR: 0.4314\n",
            "Epoch [87/100], Train Loss: 0.0878, Train AUCPR: 0.5683, Val Loss: 0.1077, Val AUCPR: 0.4316\n",
            "Epoch [88/100], Train Loss: 0.0876, Train AUCPR: 0.5700, Val Loss: 0.1077, Val AUCPR: 0.4319\n",
            "Epoch [89/100], Train Loss: 0.0874, Train AUCPR: 0.5716, Val Loss: 0.1078, Val AUCPR: 0.4321\n",
            "Epoch [90/100], Train Loss: 0.0872, Train AUCPR: 0.5732, Val Loss: 0.1078, Val AUCPR: 0.4325\n",
            "Epoch [91/100], Train Loss: 0.0870, Train AUCPR: 0.5748, Val Loss: 0.1079, Val AUCPR: 0.4328\n",
            "Epoch [92/100], Train Loss: 0.0868, Train AUCPR: 0.5763, Val Loss: 0.1080, Val AUCPR: 0.4332\n",
            "Epoch [93/100], Train Loss: 0.0866, Train AUCPR: 0.5779, Val Loss: 0.1080, Val AUCPR: 0.4335\n",
            "Epoch [94/100], Train Loss: 0.0864, Train AUCPR: 0.5795, Val Loss: 0.1081, Val AUCPR: 0.4340\n",
            "Epoch [95/100], Train Loss: 0.0862, Train AUCPR: 0.5810, Val Loss: 0.1081, Val AUCPR: 0.4343\n",
            "Epoch [96/100], Train Loss: 0.0860, Train AUCPR: 0.5825, Val Loss: 0.1082, Val AUCPR: 0.4345\n",
            "Epoch [97/100], Train Loss: 0.0859, Train AUCPR: 0.5840, Val Loss: 0.1083, Val AUCPR: 0.4343\n",
            "Epoch [98/100], Train Loss: 0.0857, Train AUCPR: 0.5854, Val Loss: 0.1083, Val AUCPR: 0.4346\n",
            "Epoch [99/100], Train Loss: 0.0855, Train AUCPR: 0.5869, Val Loss: 0.1084, Val AUCPR: 0.4348\n",
            "Epoch [100/100], Train Loss: 0.0853, Train AUCPR: 0.5883, Val Loss: 0.1085, Val AUCPR: 0.4349\n"
          ]
        }
      ],
      "source": [
        "def train_model(test_df, test_embeddings, model, optimizer, criterion, batch_size=256, epochs=25):\n",
        "    model.train()\n",
        "    \n",
        "    # Process data\n",
        "    #TODO!!!\n",
        "    input_list, labels = process_samples(train_df, train_embeddings)\n",
        "    val_input_list, val_labels = val_process_samples(val_df, val_embeddings)\n",
        "\n",
        "\n",
        "    # Create DataLoader with `f_z_in`\n",
        "    dataset = TensorDataset(\n",
        "        torch.stack([item[0] for item in input_list]),  # test_i (input_emb)\n",
        "        torch.stack([item[1] for item in input_list]),  # z_in (weighted_avg)\n",
        "        torch.stack([item[2] for item in input_list]),  # f_z_in (new scalar input)\n",
        "        torch.stack([item[3] for item in input_list]),\n",
        "        torch.stack([item[4] for item in input_list]),\n",
        "        torch.stack([item[5] for item in input_list]),\n",
        "        torch.stack(labels)  # Labels\n",
        "    )\n",
        "\n",
        "    val_dataset = TensorDataset(\n",
        "        torch.stack([item[0] for item in val_input_list]),  # test_i (input_emb)\n",
        "        torch.stack([item[1] for item in val_input_list]),  # z_in (weighted_avg)\n",
        "        torch.stack([item[2] for item in val_input_list]),  # f_z_in (new scalar input)\n",
        "        torch.stack([item[3] for item in val_input_list]),\n",
        "        torch.stack([item[4] for item in val_input_list]),\n",
        "        torch.stack([item[5] for item in val_input_list]),\n",
        "        torch.stack(val_labels)  # Labels\n",
        "    )\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "    # # Open file to log results\n",
        "    # with open(log_file, \"w\") as f:\n",
        "    #     f.write(\"Epoch,Loss,AUCPR\\n\")  # Write header\n",
        "\n",
        "    # Training loop for multiple epochs\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0  # Track loss for each epoch\n",
        "        all_targets = []\n",
        "        all_outputs = []\n",
        "\n",
        "        for batch in dataloader:\n",
        "            input_emb, z_in, f_z_in, f_z_in_2, f_z_in_3, f_z_in_4, target = batch\n",
        "            target = target.unsqueeze(-1)  # Make target shape (batch_size, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_emb, z_in, f_z_in, f_z_in_2, f_z_in_3, f_z_in_4)  # Pass f_z_in to model\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()  # Accumulate loss for epoch\n",
        "            \n",
        "            # Collect predictions & targets for AUCPR\n",
        "            all_outputs.append(output.detach().cpu())  # Move to CPU to avoid memory issues\n",
        "            all_targets.append(target.detach().cpu())\n",
        "\n",
        "        # Compute AUCPR at the end of the epoch\n",
        "        all_outputs = torch.cat(all_outputs).numpy()\n",
        "        all_targets = torch.cat(all_targets).numpy()\n",
        "\n",
        "        aucpr = average_precision_score(all_targets, all_outputs)\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "\n",
        "        # VALIDATION Step\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_val_targets = []\n",
        "        all_val_outputs = []\n",
        "\n",
        "        with torch.no_grad():  # No gradient computation for validation\n",
        "            for batch in val_dataloader:\n",
        "                input_emb, z_in, f_z_in, f_z_in_2, f_z_in_3, f_z_in_4, target = batch\n",
        "                target = target.unsqueeze(-1)\n",
        "\n",
        "                output = model(input_emb, z_in, f_z_in, f_z_in_2, f_z_in_3, f_z_in_4)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                \n",
        "                all_val_outputs.append(output.cpu())\n",
        "                all_val_targets.append(target.cpu())\n",
        "\n",
        "        # Compute AUCPR for validation set\n",
        "        all_val_outputs = torch.cat(all_val_outputs).numpy()\n",
        "        all_val_targets = torch.cat(all_val_targets).numpy()\n",
        "        val_aucpr = average_precision_score(all_val_targets, all_val_outputs)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], \"\n",
        "              f\"Train Loss: {epoch_loss/len(dataloader):.4f}, Train AUCPR: {aucpr:.4f}, \"\n",
        "              f\"Val Loss: {val_loss/len(val_dataloader):.4f}, Val AUCPR: {val_aucpr:.4f}\")\n",
        "\n",
        "\n",
        "        # print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, AUCPR: {aucpr:.4f}\")\n",
        "\n",
        "        # # Append results to file\n",
        "        # with open(log_file, \"a\") as f:\n",
        "        #     f.write(f\"{epoch+1},{avg_loss:.4f},{aucpr:.4f}\\n\")\n",
        "\n",
        "\n",
        "# Model, optimizer, and loss function\n",
        "input_emb_dim = 182  # Assuming this based on test_df features\n",
        "model = ThreeBlockModel(input_emb_dim=input_emb_dim)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
        "\n",
        "# Call training loop (assuming test_df and test_embeddings are available)\n",
        "train_model(train_df, train_embeddings, model, optimizer, criterion, epochs=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
