{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "vLGyxxRwcNFq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import faiss\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCdTUn0Zbt7O",
        "outputId": "319cd6c1-35bb-49e0-d242-9ba54812b565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(118108, 32)\n"
          ]
        }
      ],
      "source": [
        "# Loading all embeddings (which were gotten from SAINT)\n",
        "\n",
        "path = \"../5_embeddings/cls_embeddings_time.struct_time(tm_year=2025, tm_mon=2, tm_mday=7, tm_hour=19, tm_min=40, tm_sec=22, tm_wday=4, tm_yday=38, tm_isdst=0).npy\"\n",
        "\n",
        "cls_embeddings = np.load(path)\n",
        "print(cls_embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cls</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card4</th>\n",
              "      <th>card6</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>...</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.291883</td>\n",
              "      <td>-0.329939</td>\n",
              "      <td>0.108390</td>\n",
              "      <td>-0.145421</td>\n",
              "      <td>-0.399322</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0.892993</td>\n",
              "      <td>0.871243</td>\n",
              "      <td>-0.359702</td>\n",
              "      <td>0.680504</td>\n",
              "      <td>-0.412094</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.030054</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.594876</td>\n",
              "      <td>-1.467121</td>\n",
              "      <td>8.134522</td>\n",
              "      <td>-0.109308</td>\n",
              "      <td>0.711822</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.123148</td>\n",
              "      <td>-0.156138</td>\n",
              "      <td>-0.422421</td>\n",
              "      <td>1.487250</td>\n",
              "      <td>-0.265218</td>\n",
              "      <td>...</td>\n",
              "      <td>0.341765</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>1.611964</td>\n",
              "      <td>1.677853</td>\n",
              "      <td>-0.317889</td>\n",
              "      <td>-0.081355</td>\n",
              "      <td>-0.265218</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76765</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>-1.327921</td>\n",
              "      <td>-1.260784</td>\n",
              "      <td>-0.113217</td>\n",
              "      <td>-0.653259</td>\n",
              "      <td>-1.606254</td>\n",
              "      <td>...</td>\n",
              "      <td>1.009878</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76766</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>25</td>\n",
              "      <td>0.675641</td>\n",
              "      <td>0.648266</td>\n",
              "      <td>-0.075376</td>\n",
              "      <td>-0.002802</td>\n",
              "      <td>0.756523</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76767</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>0.418154</td>\n",
              "      <td>0.377752</td>\n",
              "      <td>0.150412</td>\n",
              "      <td>-1.485918</td>\n",
              "      <td>-0.226903</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>0.393449</td>\n",
              "      <td>0.090945</td>\n",
              "      <td>0.276274</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76768</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>0.605578</td>\n",
              "      <td>0.576883</td>\n",
              "      <td>-0.322279</td>\n",
              "      <td>-0.182963</td>\n",
              "      <td>0.577719</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76769</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>53</td>\n",
              "      <td>0.944849</td>\n",
              "      <td>0.929392</td>\n",
              "      <td>-0.317889</td>\n",
              "      <td>1.061025</td>\n",
              "      <td>1.229079</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>0.393449</td>\n",
              "      <td>0.090945</td>\n",
              "      <td>0.276274</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>76770 rows × 182 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       cls  ProductCD  card4  card6  P_emaildomain  Unnamed: 0  TransactionDT  \\\n",
              "0        0          4      2      1              2   -0.291883      -0.329939   \n",
              "1        0          4      3      2             16    0.892993       0.871243   \n",
              "2        0          4      2      2              1   -1.594876      -1.467121   \n",
              "3        0          4      3      2             19   -0.123148      -0.156138   \n",
              "4        0          4      3      2             16    1.611964       1.677853   \n",
              "...    ...        ...    ...    ...            ...         ...            ...   \n",
              "76765    0          4      3      2             16   -1.327921      -1.260784   \n",
              "76766    0          4      2      2             25    0.675641       0.648266   \n",
              "76767    0          4      1      1             16    0.418154       0.377752   \n",
              "76768    0          4      3      2             19    0.605578       0.576883   \n",
              "76769    0          4      3      2             53    0.944849       0.929392   \n",
              "\n",
              "       TransactionAmt     card1     card2  ...      V312      V313      V314  \\\n",
              "0            0.108390 -0.145421 -0.399322  ... -0.227583 -0.222385 -0.249222   \n",
              "1           -0.359702  0.680504 -0.412094  ... -0.030054 -0.222385 -0.249222   \n",
              "2            8.134522 -0.109308  0.711822  ... -0.227583 -0.222385 -0.249222   \n",
              "3           -0.422421  1.487250 -0.265218  ...  0.341765 -0.222385 -0.249222   \n",
              "4           -0.317889 -0.081355 -0.265218  ... -0.227583 -0.222385 -0.249222   \n",
              "...               ...       ...       ...  ...       ...       ...       ...   \n",
              "76765       -0.113217 -0.653259 -1.606254  ...  1.009878 -0.222385 -0.249222   \n",
              "76766       -0.075376 -0.002802  0.756523  ... -0.227583 -0.222385 -0.249222   \n",
              "76767        0.150412 -1.485918 -0.226903  ... -0.227583  0.393449  0.090945   \n",
              "76768       -0.322279 -0.182963  0.577719  ... -0.227583 -0.222385 -0.249222   \n",
              "76769       -0.317889  1.061025  1.229079  ... -0.227583  0.393449  0.090945   \n",
              "\n",
              "           V315      V316      V317      V318      V319      V320      V321  \n",
              "0     -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "1     -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "2     -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "3     -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "4     -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "76765 -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "76766 -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "76767  0.276274 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "76768 -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "76769  0.276274 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "\n",
              "[76770 rows x 182 columns]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"../2_dataset/final/train_df.csv\")\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(76770, 1)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_y = pd.read_csv(\"../2_dataset/final/train_y_df.csv\")\n",
        "train_y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../2_dataset/final/test_df.csv\")\n",
        "test_y = pd.read_csv(\"../2_dataset/final/test_y_df.csv\")\n",
        "test_y['isFraud'] = test_y['isFraud'].astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_df = pd.read_csv(\"../2_dataset/final/val_df.csv\")\n",
        "val_y = pd.read_csv(\"../2_dataset/final/val_y_df.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((76770, 1), (76770, 182))"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_y.shape, train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((23622, 1), (23622, 182))"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_y.shape, test_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((17716, 1), (17716, 182))"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_y.shape, val_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cls\n",
            "ProductCD\n",
            "card4\n",
            "card6\n",
            "P_emaildomain\n",
            "Unnamed: 0\n",
            "TransactionDT\n",
            "TransactionAmt\n",
            "card1\n",
            "card2\n",
            "card3\n",
            "card5\n",
            "addr1\n",
            "addr2\n",
            "C1\n",
            "C2\n",
            "C3\n",
            "C4\n",
            "C5\n",
            "C6\n",
            "C7\n",
            "C8\n",
            "C9\n",
            "C10\n",
            "C11\n",
            "C12\n",
            "C13\n",
            "C14\n",
            "D1\n",
            "D10\n",
            "D15\n",
            "V12\n",
            "V13\n",
            "V14\n",
            "V15\n",
            "V16\n",
            "V17\n",
            "V18\n",
            "V19\n",
            "V20\n",
            "V21\n",
            "V22\n",
            "V23\n",
            "V24\n",
            "V25\n",
            "V26\n",
            "V27\n",
            "V28\n",
            "V29\n",
            "V30\n",
            "V31\n",
            "V32\n",
            "V33\n",
            "V34\n",
            "V53\n",
            "V54\n",
            "V55\n",
            "V56\n",
            "V57\n",
            "V58\n",
            "V59\n",
            "V60\n",
            "V61\n",
            "V62\n",
            "V63\n",
            "V64\n",
            "V65\n",
            "V66\n",
            "V67\n",
            "V68\n",
            "V69\n",
            "V70\n",
            "V71\n",
            "V72\n",
            "V73\n",
            "V74\n",
            "V75\n",
            "V76\n",
            "V77\n",
            "V78\n",
            "V79\n",
            "V80\n",
            "V81\n",
            "V82\n",
            "V83\n",
            "V84\n",
            "V85\n",
            "V86\n",
            "V87\n",
            "V88\n",
            "V89\n",
            "V90\n",
            "V91\n",
            "V92\n",
            "V93\n",
            "V94\n",
            "V95\n",
            "V96\n",
            "V97\n",
            "V98\n",
            "V99\n",
            "V100\n",
            "V101\n",
            "V102\n",
            "V103\n",
            "V104\n",
            "V105\n",
            "V106\n",
            "V107\n",
            "V108\n",
            "V109\n",
            "V110\n",
            "V111\n",
            "V112\n",
            "V113\n",
            "V114\n",
            "V115\n",
            "V116\n",
            "V117\n",
            "V118\n",
            "V119\n",
            "V120\n",
            "V121\n",
            "V122\n",
            "V123\n",
            "V124\n",
            "V125\n",
            "V126\n",
            "V127\n",
            "V128\n",
            "V129\n",
            "V130\n",
            "V131\n",
            "V132\n",
            "V133\n",
            "V134\n",
            "V135\n",
            "V136\n",
            "V137\n",
            "V279\n",
            "V280\n",
            "V281\n",
            "V282\n",
            "V283\n",
            "V284\n",
            "V285\n",
            "V286\n",
            "V287\n",
            "V288\n",
            "V289\n",
            "V290\n",
            "V291\n",
            "V292\n",
            "V293\n",
            "V294\n",
            "V295\n",
            "V296\n",
            "V297\n",
            "V298\n",
            "V299\n",
            "V300\n",
            "V301\n",
            "V302\n",
            "V303\n",
            "V304\n",
            "V305\n",
            "V306\n",
            "V307\n",
            "V308\n",
            "V309\n",
            "V310\n",
            "V311\n",
            "V312\n",
            "V313\n",
            "V314\n",
            "V315\n",
            "V316\n",
            "V317\n",
            "V318\n",
            "V319\n",
            "V320\n",
            "V321\n",
            "isFraud\n"
          ]
        }
      ],
      "source": [
        "train_combined = pd.concat([train_df, train_y], axis=1)\n",
        "for i in train_combined.columns:\n",
        "    print(i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation Calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "correlation_matrix = train_combined.corr()\n",
        "\n",
        "# Extract correlation with target variable 'isFraud'\n",
        "target_correlation = correlation_matrix[\"isFraud\"].sort_values(ascending=False)\n",
        "\n",
        "# Plot the heatmap\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.barh(target_correlation.index, target_correlation.values, color=\"skyblue\")\n",
        "# plt.xlabel(\"Correlation Coefficient\")\n",
        "# plt.ylabel(\"Features\")\n",
        "# plt.title(\"Feature Correlation with isFraud\")\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "\n",
        "# Display the correlation matrix as a table\n",
        "target_correlation = correlation_matrix[\"isFraud\"].sort_values(ascending=False)\n",
        "\n",
        "# Print correlation with target variable\n",
        "# print(\"Correlation with isFraud:\\n\", target_correlation[0:15])\n",
        "\n",
        "# for i in target_correlation:\n",
        "# print(i)\n",
        "\n",
        "\n",
        "# V86          0.238604\n",
        "# V87          0.233544\n",
        "# V79          0.178765\n",
        "# V94          0.172088\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_combined[\"V86\"]\n",
        "column_range = train_df[\"V86\"].max() - train_df[\"V86\"].min()\n",
        "# Range of 'V86': 69.6747708 low: -2.7227058  high: 66.952065"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.14215863,  5.0189357 ,  2.4383886 , 10.18003   ,  7.5994825 ,\n",
              "       30.824408  , -2.7227058 , 17.92167   , 12.760577  , 28.24386   ,\n",
              "       20.502218  , 23.082766  , 15.341125  , 35.9855    , 41.146595  ,\n",
              "       25.663313  , 54.04933   , 51.468784  , 66.952065  ])"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_combined[\"V86\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.3627873,  2.7560682,  5.8749237])"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_combined[\"V94\"].unique()\n",
        "\n",
        "# V86          0.238604\n",
        "# V87          0.233544\n",
        "# V79          0.178765\n",
        "# V94          0.172088\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Splitting Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total embeddings shape: (118108, 32)\n",
            "Train embeddings shape: (76770, 32)\n",
            "Val embeddings shape: (17716, 32)\n",
            "Test embeddings shape: (23622, 32)\n"
          ]
        }
      ],
      "source": [
        "# Compute 65% of the total rows\n",
        "total_rows = cls_embeddings.shape[0]\n",
        "train_size = int(0.65 * total_rows)  # 65% of 118108\n",
        "test_size = int(0.8 * total_rows)\n",
        "\n",
        "# Slice the top 65%\n",
        "train_embeddings = cls_embeddings[:train_size]\n",
        "val_embeddings = cls_embeddings[train_size:test_size]\n",
        "test_embeddings = cls_embeddings[test_size:]\n",
        "\n",
        "print(f\"Total embeddings shape: {cls_embeddings.shape}\")\n",
        "print(f\"Train embeddings shape: {train_embeddings.shape}\")\n",
        "print(f\"Val embeddings shape: {val_embeddings.shape}\")\n",
        "print(f\"Test embeddings shape: {test_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### faiss index and similarity search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA6ULff8bxV-",
        "outputId": "33b81774-3e8c-48c4-cd3a-f81370fc6775"
      },
      "outputs": [],
      "source": [
        "def create_index(num_embeddings, dimension):\n",
        "    # num_embeddings = 76770\n",
        "    # dimension = 32\n",
        "    index = faiss.IndexFlatL2(dimension)  # L2 similarity\n",
        "    index.add(train_embeddings)  # index of pre-computed embeddings\n",
        "\n",
        "    return index\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_faiss(query_vector):\n",
        "    index = create_index(76770, 32)\n",
        "\n",
        "    # Convert PyTorch tensor to NumPy\n",
        "    if isinstance(query_vector, torch.Tensor):\n",
        "        query_vector = query_vector.detach().cpu().numpy()\n",
        "\n",
        "    query_vector = query_vector.astype(\"float32\").reshape(1, -1)\n",
        "    query_vector = query_vector.astype(\"float32\").reshape(1, -1)\n",
        "\n",
        "    # k = 120, As best result for 120\n",
        "    distances, indices = index.search(\n",
        "        query_vector, k=120\n",
        "    )  # by default using euclidean distance for similarity\n",
        "\n",
        "    indices = indices.flatten()\n",
        "    return distances, indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### L2 distance component\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "WJLNSJdcbznQ"
      },
      "outputs": [],
      "source": [
        "def compute_similarity(distances, dropout=0.2):\n",
        "\n",
        "    distances = distances.flatten()\n",
        "    # Apply softmax to the negative distances\n",
        "    similarities = np.exp(-distances)\n",
        "    softmax_scores = similarities / np.sum(similarities)\n",
        "\n",
        "    # Apply dropout (randomly zero out some softmax scores)\n",
        "    dropout_mask = np.random.binomial(1, 1 - dropout, size=softmax_scores.shape)\n",
        "    dropped_softmax_scores = softmax_scores * dropout_mask\n",
        "\n",
        "    # Renormilizing softmax scores so that they sum to 1 again.\n",
        "    final_softmax = dropped_softmax_scores / np.sum(dropped_softmax_scores)\n",
        "    \n",
        "    # how to weigh in the final embedding? -> weigh emebedding more if they are closer in vector space.\n",
        "    return final_softmax, distances\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mask To Drop The Dropped Out Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_by_mask(arr1, arr2, arr3):\n",
        "    \"\"\"\n",
        "    Removes elements from arr2 and arr3 where corresponding indices in arr1 are zero.\n",
        "    \"\"\"\n",
        "    mask = arr1 != 0  # Create a boolean mask where arr1 is nonzero\n",
        "    return arr1[mask], arr2[mask], arr3[mask]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Trasnform The Value Component and Correlation Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlB6RGt6cLid",
        "outputId": "27a8197e-bb5c-4bd2-f934-6fcc9eb8cb6f"
      },
      "outputs": [],
      "source": [
        "class MLP_Wv(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MLP_Wv, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "        self.activation1 = nn.SiLU()\n",
        "\n",
        "        self.layer2 = nn.Linear(32, 32)\n",
        "        self.activation2 = nn.SiLU()\n",
        "        self.dropout2 = nn.Dropout(p=0.2)\n",
        "\n",
        "        self.layer3 = nn.Linear(32, 32)  # Fix: Output should be 32\n",
        "        self.activation3 = nn.SiLU()  # Fix: Apply SiLU activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation1(self.layer1(x))\n",
        "        x = self.dropout2(self.activation2(self.layer2(x)))\n",
        "        x = self.activation3(self.layer3(x))  # Fix: Apply activation & dropout\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXAE9Vzwcn2T",
        "outputId": "5541a00b-ec38-4c32-e4c2-0f6f8998f8fc"
      },
      "outputs": [],
      "source": [
        "def compute_l1(distances):\n",
        "\n",
        "    l1_dist = np.sqrt(distances)\n",
        "    model = MLP_Wv(l1_dist.shape[0])\n",
        "    # Convert input to tensor and pass it through the model\n",
        "    l1_dist_tensor = torch.tensor(l1_dist, dtype=torch.float32)\n",
        "    value_Wv = model(l1_dist_tensor).detach().numpy()\n",
        "\n",
        "    return value_Wv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP_Wy(nn.Module):\n",
        "    \"\"\"\n",
        "    Instantiate MLP with input_dim=32 (from Wy)\n",
        "    Expected output shape: (60,)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "        super(MLP_Wy, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)  \n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)  # Shape (60, 1)\n",
        "        return x.squeeze(1)  # Shape (60,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_wy(indices):\n",
        "\n",
        "    y_i = train_y[\"isFraud\"].loc[indices].values\n",
        "    feature_target_tensor = torch.tensor(y_i, dtype=torch.long)\n",
        "\n",
        "    # Define Wy: An embedding layer to map to 32-dim space\n",
        "    embedding_dim = 32\n",
        "    num_classes = 2  # Since input values are 0 or 1\n",
        "\n",
        "    Wy = nn.Embedding(num_classes, embedding_dim)\n",
        "    mlp = MLP_Wy(input_dim=embedding_dim)\n",
        "\n",
        "    # Compute embeddings using Wy\n",
        "    embeddings = Wy(feature_target_tensor)  # Shape: (60, 32)\n",
        "\n",
        "    # Pass embeddings through MLP\n",
        "    value_Wy = mlp(embeddings)  # Shape: (60,)\n",
        "\n",
        "    return value_Wy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_imp_features(indices):\n",
        "    \"\"\"\n",
        "    Transform the features V86, V87, V79, and V94 using MLP_Wy\n",
        "    \"\"\"\n",
        "\n",
        "    v_86 = train_df[\"V86\"].loc[indices].values\n",
        "    feature_target_tensor = torch.tensor(v_86, dtype=torch.float32).unsqueeze(-1)\n",
        "\n",
        "    v_87 = train_df[\"V87\"].loc[indices].values\n",
        "    feature_target_tensor_1 = torch.tensor(v_87, dtype=torch.float32).unsqueeze(-1)\n",
        "\n",
        "    v_79 = train_df[\"V79\"].loc[indices].values\n",
        "    feature_target_tensor_2 = torch.tensor(v_79, dtype=torch.float32).unsqueeze(-1)\n",
        "\n",
        "    v_94 = train_df[\"V94\"].loc[indices].values\n",
        "    feature_target_tensor_3 = torch.tensor(v_94, dtype=torch.float32).unsqueeze(-1)\n",
        "\n",
        "    # Convert to PyTorch tensor\n",
        "    input_dim = 1  # Since each input is a single continuous value\n",
        "    embedding_dim = 32\n",
        "    W_feat = nn.Linear(input_dim, embedding_dim)\n",
        "    mlp = MLP_Wy(input_dim=embedding_dim)\n",
        "\n",
        "    # Process first variable\n",
        "    embeddings_1 = W_feat(feature_target_tensor)\n",
        "    feat_x1 = mlp(embeddings_1)\n",
        "    feat_x1 = feat_x1.detach().numpy()\n",
        "    # Process second variable\n",
        "    embeddings_2 = W_feat(feature_target_tensor_1)\n",
        "    feat_x2 = mlp(embeddings_2)\n",
        "    feat_x2 = feat_x2.detach().numpy()\n",
        "    # Process third variable\n",
        "    embeddings_3 = W_feat(feature_target_tensor_2)\n",
        "    feat_x3 = mlp(embeddings_3)\n",
        "    feat_x3 = feat_x3.detach().numpy()\n",
        "    # Process fourth variable\n",
        "    embeddings_4 = W_feat(feature_target_tensor_3)\n",
        "    feat_x4 = mlp(embeddings_4)\n",
        "    feat_x4 = feat_x4.detach().numpy()\n",
        "\n",
        "    return feat_x1, feat_x2, feat_x3, feat_x4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_value(value_Wv, value_Wy):\n",
        "    \"\"\"\n",
        "    Compute the dot product of value_Wv and value_Wy\n",
        "    \"\"\"    \n",
        "    value_Wy_npy = value_Wy.detach().numpy()\n",
        "    value = value_Wy_npy + value_Wv\n",
        "    return value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Reshaping S to do S \\* V\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_z_in(similarity, value):\n",
        "    \"\"\"\n",
        "    z_in is the intergrated(weighted sum) result of value and similarity componenet\n",
        "    \"\"\"\n",
        "    similarity = similarity.reshape(1, -1)\n",
        "\n",
        "    numerator = np.sum(similarity @ value)  # Sum the weighted contributions (scalar)\n",
        "    denominator = np.sum(similarity)        # Total sum of weights (scalar)\n",
        "    z_in = numerator / denominator          # Weighted average as a single scala\n",
        "    \n",
        "    return z_in\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Process Samples for Training and Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test_i = test_df.iloc[10].values #shape (182,)\n",
        "# query_vector = test_embeddings[10] #shape (32,)\n",
        "# distances, indices = search_faiss(query_vector) # both shape (120,) and flatten\n",
        "# similarity, distances = compute_similarity(distances)\n",
        "# similarity, indices, distances = filter_by_mask(similarity, indices, distances)\n",
        "# value_Wv = compute_l1(distances) #shape (32,)\n",
        "# value_Wy = compute_wy(indices) #shape (32,)\n",
        "\n",
        "\n",
        "# feat_x_1, feat_x2, feat_x3, feat_x4 = compute_imp_features(indices=indices)\n",
        "# #z_in\n",
        "# f_z_1 = compute_z_in(similarity, feat_x_1)\n",
        "# f_z_2 = compute_z_in(similarity, feat_x2)\n",
        "# f_z_3 = compute_z_in(similarity, feat_x3)\n",
        "# f_z_4 = compute_z_in(similarity, feat_x4)\n",
        "\n",
        "# f_z_1, f_z_2, f_z_3, f_z_4\n",
        "\n",
        "# value = compute_value(value_Wv, value_Wy)\n",
        "# value.shape\n",
        "\n",
        "\n",
        "# labels = test_y['isFraud']\n",
        "# labels = torch.tensor(test_y['isFraud'].values, dtype=torch.float32)\n",
        "# type(labels)\n",
        "\n",
        "# type(test_df.iloc[34].values)\n",
        "\n",
        "# def process_samples(test_df, test_embeddings):\n",
        "#     \"\"\"Processes all samples and returns input tensor and labels.\"\"\"\n",
        "#     input_list = []\n",
        "#     labels = []\n",
        "\n",
        "\n",
        "#     for i in range(len(test_df)):  # Process all samples\n",
        "#         test_i = torch.tensor(test_df.iloc[i].values, dtype=torch.float32)  # shape (182,)\n",
        "#         query_embedding = torch.tensor(test_embeddings[i], dtype=torch.float32)  # shape (32,)\n",
        "\n",
        "#         distances, indices = search_faiss(query_embedding)\n",
        "#         similarity, distances = compute_similarity(distances)\n",
        "#         similarity, indices, distances = filter_by_mask(similarity, indices, distances)\n",
        "#         value_Wv = compute_l1(distances)  # shape (32,)\n",
        "#         value_Wy = compute_wy(indices)  # shape (32,)\n",
        "#         value = compute_value(value_Wv, value_Wy)\n",
        "#         z_in = compute_z_in(similarity, value)  # Scalar (single value)\n",
        "\n",
        "#         feat_x_1, feat_x2, feat_x3, feat_x4 = compute_imp_features(indices=indices) #v86 feature\n",
        "#         f_z_in_1 = compute_z_in(similarity, feat_x_1)\n",
        "\n",
        "#         f_z_in_2 = compute_z_in(similarity, feat_x2)\n",
        "\n",
        "#         f_z_in_3 = compute_z_in(similarity, feat_x3)\n",
        "\n",
        "#         f_z_in_4 = compute_z_in(similarity, feat_x4)\n",
        "\n",
        "#         # Append only test_i (input_emb) and z_in (weighted_avg), not query_embedding\n",
        "#         z_in = torch.tensor(z_in, dtype=torch.float32)\n",
        "#         f_z_in_1 = torch.tensor(f_z_in_1, dtype=torch.float32)\n",
        "#         f_z_in_2 = torch.tensor(f_z_in_2, dtype=torch.float32)\n",
        "#         f_z_in_3 = torch.tensor(f_z_in_3, dtype=torch.float32)\n",
        "#         f_z_in_4 = torch.tensor(f_z_in_4, dtype=torch.float32)\n",
        "\n",
        "#         input_list.append((test_i, z_in, f_z_in_1, f_z_in_2, f_z_in_3, f_z_in_4))\n",
        "\n",
        "#         labels.append(torch.tensor(test_y.iloc[i]['isFraud'], dtype=torch.float32))  # Assuming label is in df\n",
        "\n",
        "\n",
        "#     return input_list, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_samples(train_df, train_embeddings, batch_size=1000, delay=0.5):\n",
        "    \"\"\"\n",
        "    Processes samples in batches to prevent memory overflow.\n",
        "    \"\"\"\n",
        "    input_list = []\n",
        "    labels = []\n",
        "\n",
        "    total_samples = len(train_df)\n",
        "    num_batches = (\n",
        "        total_samples + batch_size - 1\n",
        "    ) // batch_size  # Compute number of batches\n",
        "\n",
        "    for batch_idx in range(num_batches):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min((batch_idx + 1) * batch_size, total_samples)\n",
        "\n",
        "        batch_inputs = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for i in range(start_idx, end_idx):  # Process each sample in batch\n",
        "            test_i = torch.tensor(train_df.iloc[i].values, dtype=torch.float32)  # shape (182,)\n",
        "            query_embedding = torch.tensor(train_embeddings[i], dtype=torch.float32)  # shape (32,)\n",
        "\n",
        "            distances, indices = search_faiss(query_embedding)\n",
        "            similarity, distances = compute_similarity(distances)\n",
        "            similarity, indices, distances = filter_by_mask(similarity, indices, distances)\n",
        "            value_Wv = compute_l1(distances)  # shape (32,)\n",
        "            value_Wy = compute_wy(indices)  # shape (32,)\n",
        "            value = compute_value(value_Wv, value_Wy)\n",
        "            z_in = compute_z_in(similarity, value)  # Scalar (single value)\n",
        "\n",
        "            feat_x_1, feat_x2, feat_x3, feat_x4 = compute_imp_features(\n",
        "                indices=indices\n",
        "            )\n",
        "            f_z_in_1 = compute_z_in(similarity, feat_x_1)\n",
        "            f_z_in_2 = compute_z_in(similarity, feat_x2)\n",
        "            f_z_in_3 = compute_z_in(similarity, feat_x3)\n",
        "            f_z_in_4 = compute_z_in(similarity, feat_x4)\n",
        "\n",
        "            # Convert to tensors\n",
        "            z_in = torch.tensor(z_in, dtype=torch.float32)\n",
        "            f_z_in_1 = torch.tensor(f_z_in_1, dtype=torch.float32)\n",
        "            f_z_in_2 = torch.tensor(f_z_in_2, dtype=torch.float32)\n",
        "            f_z_in_3 = torch.tensor(f_z_in_3, dtype=torch.float32)\n",
        "            f_z_in_4 = torch.tensor(f_z_in_4, dtype=torch.float32)\n",
        "\n",
        "            batch_inputs.append((test_i, z_in, f_z_in_1, f_z_in_2, f_z_in_3, f_z_in_4))\n",
        "            batch_labels.append(\n",
        "                torch.tensor(train_y.iloc[i][\"isFraud\"], dtype=torch.float32)\n",
        "            )\n",
        "\n",
        "        input_list.extend(batch_inputs)\n",
        "        labels.extend(batch_labels)\n",
        "\n",
        "        time.sleep(delay)  # Add delay to avoid system overload\n",
        "\n",
        "    return input_list, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def val_process_samples(val_df, val_embeddings, batch_size=1000, delay=0.5):\n",
        "    \"\"\"\n",
        "    Processes samples in batches to prevent memory overflow.\n",
        "    \"\"\"\n",
        "    input_list = []\n",
        "    labels = []\n",
        "\n",
        "    total_samples = len(val_df)\n",
        "    num_batches = (\n",
        "        total_samples + batch_size - 1\n",
        "    ) // batch_size  # Compute number of batches\n",
        "\n",
        "    for batch_idx in range(num_batches):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min((batch_idx + 1) * batch_size, total_samples)\n",
        "\n",
        "        batch_inputs = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for i in range(start_idx, end_idx):  # Process each sample in batch\n",
        "            test_i = torch.tensor(val_df.iloc[i].values, dtype=torch.float32)  # shape (182,)\n",
        "            query_embedding = torch.tensor(val_embeddings[i], dtype=torch.float32)  # shape (32,)\n",
        "\n",
        "            distances, indices = search_faiss(query_embedding)\n",
        "            similarity, distances = compute_similarity(distances)\n",
        "            similarity, indices, distances = filter_by_mask(similarity, indices, distances)\n",
        "            value_Wv = compute_l1(distances)  # shape (32,)\n",
        "            value_Wy = compute_wy(indices)  # shape (32,)\n",
        "            value = compute_value(value_Wv, value_Wy)\n",
        "            z_in = compute_z_in(similarity, value)  # Scalar (single value)\n",
        "\n",
        "            feat_x_1, feat_x2, feat_x3, feat_x4 = compute_imp_features(\n",
        "                indices=indices\n",
        "            )  # v86 feature\n",
        "            f_z_in_1 = compute_z_in(similarity, feat_x_1)\n",
        "            f_z_in_2 = compute_z_in(similarity, feat_x2)\n",
        "            f_z_in_3 = compute_z_in(similarity, feat_x3)\n",
        "            f_z_in_4 = compute_z_in(similarity, feat_x4)\n",
        "\n",
        "            # Convert to tensors\n",
        "            z_in = torch.tensor(z_in, dtype=torch.float32)\n",
        "            f_z_in_1 = torch.tensor(f_z_in_1, dtype=torch.float32)\n",
        "            f_z_in_2 = torch.tensor(f_z_in_2, dtype=torch.float32)\n",
        "            f_z_in_3 = torch.tensor(f_z_in_3, dtype=torch.float32)\n",
        "            f_z_in_4 = torch.tensor(f_z_in_4, dtype=torch.float32)\n",
        "\n",
        "            batch_inputs.append((test_i, z_in, f_z_in_1, f_z_in_2, f_z_in_3, f_z_in_4))\n",
        "            batch_labels.append(torch.tensor(val_y.iloc[i][\"isFraud\"], dtype=torch.float32))\n",
        "\n",
        "        input_list.extend(batch_inputs)\n",
        "        labels.extend(batch_labels)\n",
        "\n",
        "        time.sleep(delay)  # Add delay to avoid system overload\n",
        "\n",
        "    return input_list, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Predictor(nn.Module):\n",
        "    def __init__(self, input_emb_dim, hidden_dim=32, dropout_prob=0.2):\n",
        "        super(Predictor, self).__init__()\n",
        "\n",
        "        self.input_dim = input_emb_dim + 4  # Adding 2 for weighted_avg and Corr features\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.LayerNorm(self.input_dim),\n",
        "            nn.Linear(self.input_dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "        )\n",
        "\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "        )\n",
        "\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "        )\n",
        "\n",
        "        self.output_layer = nn.Sequential(nn.Linear(hidden_dim, 1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, input_emb, weighted_avg, f_z_in, f_z_in_2, f_z_in_3, f_z_in_4):\n",
        "        # Ensure correct shape for scalar inputs\n",
        "        weighted_avg = weighted_avg.unsqueeze(-1)\n",
        "        \n",
        "        f_z_in = f_z_in.unsqueeze(-1)\n",
        "        f_z_in_2 = f_z_in_2.unsqueeze(-1)\n",
        "        f_z_in_3 = f_z_in_3.unsqueeze(-1)\n",
        "        f_z_in_4 = f_z_in_4.unsqueeze(-1)\n",
        "\n",
        "        # Concatenate all inputs\n",
        "        combined = torch.cat(\n",
        "            [input_emb, weighted_avg, f_z_in, f_z_in_3, f_z_in_4], dim=-1\n",
        "        )\n",
        "\n",
        "        # Pass through MLP blocks\n",
        "        x = self.block1(combined)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Train Loss: 0.2887, Train AUCPR: 0.0381, Val Loss: 0.1387, Val AUCPR: 0.1668\n",
            "Epoch [2/100], Train Loss: 0.1351, Train AUCPR: 0.1887, Val Loss: 0.1297, Val AUCPR: 0.2511\n",
            "Epoch [3/100], Train Loss: 0.1284, Train AUCPR: 0.2509, Val Loss: 0.1216, Val AUCPR: 0.3021\n",
            "Epoch [4/100], Train Loss: 0.1214, Train AUCPR: 0.2946, Val Loss: 0.1171, Val AUCPR: 0.3263\n",
            "Epoch [5/100], Train Loss: 0.1178, Train AUCPR: 0.3179, Val Loss: 0.1151, Val AUCPR: 0.3354\n",
            "Epoch [6/100], Train Loss: 0.1157, Train AUCPR: 0.3320, Val Loss: 0.1140, Val AUCPR: 0.3405\n",
            "Epoch [7/100], Train Loss: 0.1145, Train AUCPR: 0.3415, Val Loss: 0.1133, Val AUCPR: 0.3441\n",
            "Epoch [8/100], Train Loss: 0.1136, Train AUCPR: 0.3490, Val Loss: 0.1128, Val AUCPR: 0.3471\n",
            "Epoch [9/100], Train Loss: 0.1128, Train AUCPR: 0.3553, Val Loss: 0.1124, Val AUCPR: 0.3506\n",
            "Epoch [10/100], Train Loss: 0.1121, Train AUCPR: 0.3609, Val Loss: 0.1121, Val AUCPR: 0.3530\n",
            "Epoch [11/100], Train Loss: 0.1115, Train AUCPR: 0.3660, Val Loss: 0.1118, Val AUCPR: 0.3556\n",
            "Epoch [12/100], Train Loss: 0.1110, Train AUCPR: 0.3709, Val Loss: 0.1116, Val AUCPR: 0.3579\n",
            "Epoch [13/100], Train Loss: 0.1104, Train AUCPR: 0.3758, Val Loss: 0.1114, Val AUCPR: 0.3603\n",
            "Epoch [14/100], Train Loss: 0.1100, Train AUCPR: 0.3799, Val Loss: 0.1112, Val AUCPR: 0.3625\n",
            "Epoch [15/100], Train Loss: 0.1095, Train AUCPR: 0.3839, Val Loss: 0.1110, Val AUCPR: 0.3642\n",
            "Epoch [16/100], Train Loss: 0.1090, Train AUCPR: 0.3878, Val Loss: 0.1109, Val AUCPR: 0.3660\n",
            "Epoch [17/100], Train Loss: 0.1086, Train AUCPR: 0.3917, Val Loss: 0.1107, Val AUCPR: 0.3676\n",
            "Epoch [18/100], Train Loss: 0.1082, Train AUCPR: 0.3954, Val Loss: 0.1106, Val AUCPR: 0.3691\n",
            "Epoch [19/100], Train Loss: 0.1078, Train AUCPR: 0.3990, Val Loss: 0.1104, Val AUCPR: 0.3707\n",
            "Epoch [20/100], Train Loss: 0.1074, Train AUCPR: 0.4024, Val Loss: 0.1103, Val AUCPR: 0.3724\n",
            "Epoch [21/100], Train Loss: 0.1070, Train AUCPR: 0.4059, Val Loss: 0.1102, Val AUCPR: 0.3739\n",
            "Epoch [22/100], Train Loss: 0.1066, Train AUCPR: 0.4094, Val Loss: 0.1100, Val AUCPR: 0.3751\n",
            "Epoch [23/100], Train Loss: 0.1062, Train AUCPR: 0.4128, Val Loss: 0.1099, Val AUCPR: 0.3765\n",
            "Epoch [24/100], Train Loss: 0.1058, Train AUCPR: 0.4162, Val Loss: 0.1098, Val AUCPR: 0.3777\n",
            "Epoch [25/100], Train Loss: 0.1054, Train AUCPR: 0.4196, Val Loss: 0.1097, Val AUCPR: 0.3789\n",
            "Epoch [26/100], Train Loss: 0.1050, Train AUCPR: 0.4230, Val Loss: 0.1096, Val AUCPR: 0.3804\n",
            "Epoch [27/100], Train Loss: 0.1046, Train AUCPR: 0.4264, Val Loss: 0.1095, Val AUCPR: 0.3818\n",
            "Epoch [28/100], Train Loss: 0.1042, Train AUCPR: 0.4297, Val Loss: 0.1094, Val AUCPR: 0.3834\n",
            "Epoch [29/100], Train Loss: 0.1038, Train AUCPR: 0.4331, Val Loss: 0.1093, Val AUCPR: 0.3848\n",
            "Epoch [30/100], Train Loss: 0.1035, Train AUCPR: 0.4364, Val Loss: 0.1092, Val AUCPR: 0.3862\n",
            "Epoch [31/100], Train Loss: 0.1031, Train AUCPR: 0.4396, Val Loss: 0.1091, Val AUCPR: 0.3874\n",
            "Epoch [32/100], Train Loss: 0.1027, Train AUCPR: 0.4428, Val Loss: 0.1090, Val AUCPR: 0.3886\n",
            "Epoch [33/100], Train Loss: 0.1024, Train AUCPR: 0.4460, Val Loss: 0.1090, Val AUCPR: 0.3898\n",
            "Epoch [34/100], Train Loss: 0.1020, Train AUCPR: 0.4490, Val Loss: 0.1089, Val AUCPR: 0.3909\n",
            "Epoch [35/100], Train Loss: 0.1017, Train AUCPR: 0.4521, Val Loss: 0.1088, Val AUCPR: 0.3923\n",
            "Epoch [36/100], Train Loss: 0.1013, Train AUCPR: 0.4551, Val Loss: 0.1087, Val AUCPR: 0.3934\n",
            "Epoch [37/100], Train Loss: 0.1010, Train AUCPR: 0.4580, Val Loss: 0.1087, Val AUCPR: 0.3946\n",
            "Epoch [38/100], Train Loss: 0.1007, Train AUCPR: 0.4610, Val Loss: 0.1086, Val AUCPR: 0.3957\n",
            "Epoch [39/100], Train Loss: 0.1003, Train AUCPR: 0.4638, Val Loss: 0.1086, Val AUCPR: 0.3965\n",
            "Epoch [40/100], Train Loss: 0.1000, Train AUCPR: 0.4666, Val Loss: 0.1085, Val AUCPR: 0.3975\n",
            "Epoch [41/100], Train Loss: 0.0997, Train AUCPR: 0.4693, Val Loss: 0.1085, Val AUCPR: 0.3982\n",
            "Epoch [42/100], Train Loss: 0.0994, Train AUCPR: 0.4720, Val Loss: 0.1084, Val AUCPR: 0.3989\n",
            "Epoch [43/100], Train Loss: 0.0991, Train AUCPR: 0.4746, Val Loss: 0.1084, Val AUCPR: 0.3997\n",
            "Epoch [44/100], Train Loss: 0.0988, Train AUCPR: 0.4773, Val Loss: 0.1084, Val AUCPR: 0.4003\n",
            "Epoch [45/100], Train Loss: 0.0985, Train AUCPR: 0.4799, Val Loss: 0.1084, Val AUCPR: 0.4007\n",
            "Epoch [46/100], Train Loss: 0.0982, Train AUCPR: 0.4825, Val Loss: 0.1084, Val AUCPR: 0.4014\n",
            "Epoch [47/100], Train Loss: 0.0979, Train AUCPR: 0.4850, Val Loss: 0.1083, Val AUCPR: 0.4019\n",
            "Epoch [48/100], Train Loss: 0.0976, Train AUCPR: 0.4874, Val Loss: 0.1083, Val AUCPR: 0.4024\n",
            "Epoch [49/100], Train Loss: 0.0973, Train AUCPR: 0.4898, Val Loss: 0.1083, Val AUCPR: 0.4031\n",
            "Epoch [50/100], Train Loss: 0.0971, Train AUCPR: 0.4921, Val Loss: 0.1083, Val AUCPR: 0.4034\n",
            "Epoch [51/100], Train Loss: 0.0968, Train AUCPR: 0.4945, Val Loss: 0.1083, Val AUCPR: 0.4038\n",
            "Epoch [52/100], Train Loss: 0.0965, Train AUCPR: 0.4968, Val Loss: 0.1083, Val AUCPR: 0.4044\n",
            "Epoch [53/100], Train Loss: 0.0962, Train AUCPR: 0.4990, Val Loss: 0.1083, Val AUCPR: 0.4048\n",
            "Epoch [54/100], Train Loss: 0.0960, Train AUCPR: 0.5012, Val Loss: 0.1083, Val AUCPR: 0.4053\n",
            "Epoch [55/100], Train Loss: 0.0957, Train AUCPR: 0.5033, Val Loss: 0.1083, Val AUCPR: 0.4055\n",
            "Epoch [56/100], Train Loss: 0.0955, Train AUCPR: 0.5054, Val Loss: 0.1083, Val AUCPR: 0.4060\n",
            "Epoch [57/100], Train Loss: 0.0952, Train AUCPR: 0.5075, Val Loss: 0.1084, Val AUCPR: 0.4064\n",
            "Epoch [58/100], Train Loss: 0.0950, Train AUCPR: 0.5095, Val Loss: 0.1084, Val AUCPR: 0.4068\n",
            "Epoch [59/100], Train Loss: 0.0947, Train AUCPR: 0.5114, Val Loss: 0.1084, Val AUCPR: 0.4070\n",
            "Epoch [60/100], Train Loss: 0.0945, Train AUCPR: 0.5134, Val Loss: 0.1084, Val AUCPR: 0.4074\n",
            "Epoch [61/100], Train Loss: 0.0943, Train AUCPR: 0.5153, Val Loss: 0.1084, Val AUCPR: 0.4076\n",
            "Epoch [62/100], Train Loss: 0.0940, Train AUCPR: 0.5172, Val Loss: 0.1084, Val AUCPR: 0.4080\n",
            "Epoch [63/100], Train Loss: 0.0938, Train AUCPR: 0.5191, Val Loss: 0.1085, Val AUCPR: 0.4084\n",
            "Epoch [64/100], Train Loss: 0.0936, Train AUCPR: 0.5210, Val Loss: 0.1085, Val AUCPR: 0.4087\n",
            "Epoch [65/100], Train Loss: 0.0933, Train AUCPR: 0.5229, Val Loss: 0.1085, Val AUCPR: 0.4089\n",
            "Epoch [66/100], Train Loss: 0.0931, Train AUCPR: 0.5248, Val Loss: 0.1086, Val AUCPR: 0.4089\n",
            "Epoch [67/100], Train Loss: 0.0929, Train AUCPR: 0.5266, Val Loss: 0.1086, Val AUCPR: 0.4091\n",
            "Epoch [68/100], Train Loss: 0.0926, Train AUCPR: 0.5284, Val Loss: 0.1086, Val AUCPR: 0.4096\n",
            "Epoch [69/100], Train Loss: 0.0924, Train AUCPR: 0.5303, Val Loss: 0.1087, Val AUCPR: 0.4099\n",
            "Epoch [70/100], Train Loss: 0.0922, Train AUCPR: 0.5321, Val Loss: 0.1087, Val AUCPR: 0.4102\n",
            "Epoch [71/100], Train Loss: 0.0920, Train AUCPR: 0.5339, Val Loss: 0.1088, Val AUCPR: 0.4105\n",
            "Epoch [72/100], Train Loss: 0.0918, Train AUCPR: 0.5358, Val Loss: 0.1088, Val AUCPR: 0.4107\n",
            "Epoch [73/100], Train Loss: 0.0915, Train AUCPR: 0.5376, Val Loss: 0.1089, Val AUCPR: 0.4109\n",
            "Epoch [74/100], Train Loss: 0.0913, Train AUCPR: 0.5394, Val Loss: 0.1090, Val AUCPR: 0.4110\n",
            "Epoch [75/100], Train Loss: 0.0911, Train AUCPR: 0.5411, Val Loss: 0.1090, Val AUCPR: 0.4113\n",
            "Epoch [76/100], Train Loss: 0.0909, Train AUCPR: 0.5429, Val Loss: 0.1091, Val AUCPR: 0.4115\n",
            "Epoch [77/100], Train Loss: 0.0907, Train AUCPR: 0.5446, Val Loss: 0.1092, Val AUCPR: 0.4116\n",
            "Epoch [78/100], Train Loss: 0.0904, Train AUCPR: 0.5463, Val Loss: 0.1092, Val AUCPR: 0.4118\n",
            "Epoch [79/100], Train Loss: 0.0902, Train AUCPR: 0.5481, Val Loss: 0.1093, Val AUCPR: 0.4120\n",
            "Epoch [80/100], Train Loss: 0.0900, Train AUCPR: 0.5498, Val Loss: 0.1094, Val AUCPR: 0.4123\n",
            "Epoch [81/100], Train Loss: 0.0898, Train AUCPR: 0.5514, Val Loss: 0.1095, Val AUCPR: 0.4124\n",
            "Epoch [82/100], Train Loss: 0.0896, Train AUCPR: 0.5531, Val Loss: 0.1096, Val AUCPR: 0.4126\n",
            "Epoch [83/100], Train Loss: 0.0894, Train AUCPR: 0.5548, Val Loss: 0.1097, Val AUCPR: 0.4127\n",
            "Epoch [84/100], Train Loss: 0.0892, Train AUCPR: 0.5565, Val Loss: 0.1098, Val AUCPR: 0.4128\n",
            "Epoch [85/100], Train Loss: 0.0890, Train AUCPR: 0.5582, Val Loss: 0.1099, Val AUCPR: 0.4129\n",
            "Epoch [86/100], Train Loss: 0.0888, Train AUCPR: 0.5598, Val Loss: 0.1100, Val AUCPR: 0.4128\n",
            "Epoch [87/100], Train Loss: 0.0886, Train AUCPR: 0.5614, Val Loss: 0.1101, Val AUCPR: 0.4130\n",
            "Epoch [88/100], Train Loss: 0.0884, Train AUCPR: 0.5631, Val Loss: 0.1103, Val AUCPR: 0.4133\n",
            "Epoch [89/100], Train Loss: 0.0881, Train AUCPR: 0.5647, Val Loss: 0.1104, Val AUCPR: 0.4130\n",
            "Epoch [90/100], Train Loss: 0.0879, Train AUCPR: 0.5663, Val Loss: 0.1105, Val AUCPR: 0.4130\n",
            "Epoch [91/100], Train Loss: 0.0877, Train AUCPR: 0.5679, Val Loss: 0.1107, Val AUCPR: 0.4130\n",
            "Epoch [92/100], Train Loss: 0.0875, Train AUCPR: 0.5695, Val Loss: 0.1109, Val AUCPR: 0.4130\n",
            "Epoch [93/100], Train Loss: 0.0874, Train AUCPR: 0.5710, Val Loss: 0.1110, Val AUCPR: 0.4131\n",
            "Epoch [94/100], Train Loss: 0.0872, Train AUCPR: 0.5725, Val Loss: 0.1112, Val AUCPR: 0.4131\n",
            "Epoch [95/100], Train Loss: 0.0870, Train AUCPR: 0.5741, Val Loss: 0.1114, Val AUCPR: 0.4132\n",
            "Epoch [96/100], Train Loss: 0.0868, Train AUCPR: 0.5756, Val Loss: 0.1115, Val AUCPR: 0.4130\n",
            "Epoch [97/100], Train Loss: 0.0866, Train AUCPR: 0.5771, Val Loss: 0.1117, Val AUCPR: 0.4129\n",
            "Epoch [98/100], Train Loss: 0.0864, Train AUCPR: 0.5786, Val Loss: 0.1119, Val AUCPR: 0.4129\n",
            "Epoch [99/100], Train Loss: 0.0862, Train AUCPR: 0.5802, Val Loss: 0.1121, Val AUCPR: 0.4127\n",
            "Epoch [100/100], Train Loss: 0.0860, Train AUCPR: 0.5816, Val Loss: 0.1123, Val AUCPR: 0.4126\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import average_precision_score\n",
        "import torch\n",
        "\n",
        "def train_model(test_df, test_embeddings, model, optimizer, criterion, batch_size=256, epochs=25, log_file=\"training_log.txt\"):\n",
        "    model.train()\n",
        "    \n",
        "    # Process data\n",
        "    input_list, labels = process_samples(train_df, train_embeddings)\n",
        "    val_input_list, val_labels = val_process_samples(val_df, val_embeddings)\n",
        "\n",
        "\n",
        "    # Create DataLoader with `f_z_in`\n",
        "    train_dataset = TensorDataset(\n",
        "        torch.stack([item[0] for item in input_list]),  # test_i (input_emb)\n",
        "        torch.stack([item[1] for item in input_list]),  # z_in (weighted_avg)\n",
        "        torch.stack([item[2] for item in input_list]),  # f_z_in (new scalar input)\n",
        "        torch.stack([item[3] for item in input_list]),  # f_z_in_2 (new scalar input)\n",
        "        torch.stack([item[4] for item in input_list]),  # f_z_in_3 (new scalar input)\n",
        "        torch.stack([item[5] for item in input_list]),  # f_z_in_4 (new scalar input)\n",
        "        torch.stack(labels)\n",
        "    )\n",
        "\n",
        "    val_dataset = TensorDataset(\n",
        "        torch.stack([item[0] for item in val_input_list]),  # test_i (input_emb)\n",
        "        torch.stack([item[1] for item in val_input_list]),  # z_in (weighted_avg)\n",
        "        torch.stack([item[2] for item in val_input_list]),  # f_z_in (new scalar input)\n",
        "        torch.stack([item[3] for item in val_input_list]),  # f_z_in_2 (new scalar input)\n",
        "        torch.stack([item[4] for item in val_input_list]),  # f_z_in_3 (new scalar input)\n",
        "        torch.stack([item[5] for item in val_input_list]),  # f_z_in_4 (new scalar input)\n",
        "        torch.stack(val_labels)\n",
        "    )\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0  # Track loss for each epoch\n",
        "        all_targets = []\n",
        "        all_outputs = []\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            input_emb, z_in, f_z_in, f_z_in_2, f_z_in_3, f_z_in_4, target = batch\n",
        "            target = target.unsqueeze(-1)  # Make target shape (batch_size, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_emb, z_in, f_z_in, f_z_in_2, f_z_in_3, f_z_in_4)  # Pass f_z_in to model\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()  # Accumulate loss for epoch\n",
        "            \n",
        "            # Collect predictions & targets for AUCPR\n",
        "            all_outputs.append(output.detach().cpu())  # Move to CPU to avoid memory issues\n",
        "            all_targets.append(target.detach().cpu())\n",
        "\n",
        "        # Compute AUCPR at the end of the epoch\n",
        "        all_outputs = torch.cat(all_outputs).numpy()\n",
        "        all_targets = torch.cat(all_targets).numpy()\n",
        "        aucpr = average_precision_score(all_targets, all_outputs)\n",
        "\n",
        "        # Validation Step\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_val_targets = []\n",
        "        all_val_outputs = []\n",
        "\n",
        "        with torch.no_grad():  # No gradient computation for validation\n",
        "            for batch in val_dataloader:\n",
        "                input_emb, z_in, f_z_in, f_z_in_2, f_z_in_3, f_z_in_4, target = batch\n",
        "                target = target.unsqueeze(-1)\n",
        "\n",
        "                output = model(input_emb, z_in, f_z_in, f_z_in_2, f_z_in_3, f_z_in_4)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                \n",
        "                all_val_outputs.append(output.cpu())\n",
        "                all_val_targets.append(target.cpu())\n",
        "\n",
        "        # Compute AUCPR for validation set\n",
        "        all_val_outputs = torch.cat(all_val_outputs).numpy()\n",
        "        all_val_targets = torch.cat(all_val_targets).numpy()\n",
        "        val_aucpr = average_precision_score(all_val_targets, all_val_outputs)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], \"\n",
        "              f\"Train Loss: {epoch_loss/len(train_dataloader):.4f}, Train AUCPR: {aucpr:.4f}, \"\n",
        "              f\"Val Loss: {val_loss/len(val_dataloader):.4f}, Val AUCPR: {val_aucpr:.4f}\")\n",
        "\n",
        "# Model, optimizer, and loss function\n",
        "input_emb_dim = 182  # Assuming this based on test_df features\n",
        "model = Predictor(input_emb_dim=input_emb_dim)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "train_model(train_df, train_embeddings, model, optimizer, criterion, epochs=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_process_samples(test_df, test_embeddings, batch_size=1000, delay=0.5):\n",
        "    \"\"\"\n",
        "    Processes samples in batches to prevent memory overflow.\n",
        "    \"\"\"\n",
        "    input_list = []\n",
        "    labels = []\n",
        "\n",
        "    total_samples = len(test_df)\n",
        "    num_batches = (\n",
        "        total_samples + batch_size - 1\n",
        "    ) // batch_size  # Compute number of batches\n",
        "\n",
        "    for batch_idx in range(num_batches):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min((batch_idx + 1) * batch_size, total_samples)\n",
        "\n",
        "        batch_inputs = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for i in range(start_idx, end_idx):  # Process each sample in batch\n",
        "            test_i = torch.tensor(test_df.iloc[i].values, dtype=torch.float32)  # shape (182,)\n",
        "            query_embedding = torch.tensor(test_embeddings[i], dtype=torch.float32)  # shape (32,)\n",
        "\n",
        "            distances, indices = search_faiss(query_embedding)\n",
        "            similarity, distances = compute_similarity(distances)\n",
        "            similarity, indices, distances = filter_by_mask(similarity, indices, distances)\n",
        "            value_Wv = compute_l1(distances)  # shape (32,)\n",
        "            value_Wy = compute_wy(indices)  # shape (32,)\n",
        "            value = compute_value(value_Wv, value_Wy)\n",
        "            z_in = compute_z_in(similarity, value)  # Scalar (single value)\n",
        "\n",
        "            feat_x_1, feat_x2, feat_x3, feat_x4 = compute_imp_features(\n",
        "                indices=indices\n",
        "            )  # v86 feature\n",
        "            f_z_in_1 = compute_z_in(similarity, feat_x_1)\n",
        "            f_z_in_2 = compute_z_in(similarity, feat_x2)\n",
        "            f_z_in_3 = compute_z_in(similarity, feat_x3)\n",
        "            f_z_in_4 = compute_z_in(similarity, feat_x4)\n",
        "\n",
        "            # Convert to tensors\n",
        "            z_in = torch.tensor(z_in, dtype=torch.float32)\n",
        "            f_z_in_1 = torch.tensor(f_z_in_1, dtype=torch.float32)\n",
        "            f_z_in_2 = torch.tensor(f_z_in_2, dtype=torch.float32)\n",
        "            f_z_in_3 = torch.tensor(f_z_in_3, dtype=torch.float32)\n",
        "            f_z_in_4 = torch.tensor(f_z_in_4, dtype=torch.float32)\n",
        "\n",
        "            batch_inputs.append((test_i, z_in, f_z_in_1, f_z_in_2, f_z_in_3, f_z_in_4))\n",
        "            batch_labels.append(torch.tensor(test_y.iloc[i][\"isFraud\"], dtype=torch.float32))\n",
        "\n",
        "        input_list.extend(batch_inputs)\n",
        "        labels.extend(batch_labels)\n",
        "\n",
        "        time.sleep(delay)  # Add delay to avoid system overload\n",
        "\n",
        "    return input_list, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test AUCPR: 0.4140655186866458\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def inference(model, test_df, test_embeddings, batch_size=256):\n",
        "    \"\"\"\n",
        "    Perform inference on test data using a trained model.\n",
        "    \n",
        "    Args:\n",
        "        model (nn.Module): Trained model.\n",
        "        test_df (pd.DataFrame): Your test dataframe.\n",
        "        test_embeddings (np.ndarray or torch.Tensor): Precomputed embeddings for test.\n",
        "        batch_size (int): Batch size for inference.\n",
        "\n",
        "    Returns:\n",
        "        all_outputs (np.ndarray): Model outputs (probabilities).\n",
        "        all_labels (np.ndarray): Ground truth labels.\n",
        "    \"\"\"\n",
        "    test_input_list, test_input_labels = test_process_samples(test_df=test_df, test_embeddings=test_embeddings)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    test_dataset = TensorDataset(\n",
        "        torch.stack([item[0] for item in test_input_list]),  # test_i (input_emb)\n",
        "        torch.stack([item[1] for item in test_input_list]),  # z_in (weighted_avg)\n",
        "        torch.stack([item[2] for item in test_input_list]),  # f_z_in (new scalar input)\n",
        "        torch.stack([item[3] for item in test_input_list]),  # f_z_in_2 (new scalar input)\n",
        "        torch.stack([item[4] for item in test_input_list]),  # f_z_in_3 (new scalar input)\n",
        "        torch.stack([item[5] for item in test_input_list]),  # f_z_in_4 (new scalar input)\n",
        "        torch.stack(test_input_labels)\n",
        "    )\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Lists to store outputs and labels\n",
        "    all_outputs = []\n",
        "    all_labels = []\n",
        "\n",
        "    # No gradient updates during inference\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_emb, z_in, f1, f2, f3, f4, labels = batch\n",
        "            outputs = model(input_emb, z_in, f1, f2, f3, f4)\n",
        "            \n",
        "            # Move outputs and labels to CPU, then store\n",
        "            all_outputs.append(outputs.cpu())\n",
        "            all_labels.append(labels.cpu())\n",
        "\n",
        "    # Concatenate everything into a single array/tensor\n",
        "    all_outputs = torch.cat(all_outputs).numpy()\n",
        "    all_labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "    return all_outputs, all_labels\n",
        "\n",
        "all_test_outputs, all_test_labels = inference(model, test_df, test_embeddings, batch_size=256)\n",
        "\n",
        "# Optionally, compute metrics (e.g., AUCPR) on your test set:\n",
        "from sklearn.metrics import average_precision_score\n",
        "test_aucpr = average_precision_score(all_test_labels, all_test_outputs)\n",
        "print(\"Test AUCPR:\", test_aucpr)\n",
        "\n",
        "# Or create a final binary prediction if desired:\n",
        "binary_predictions = (all_test_outputs >= 0.5).astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value 0: 23258 occurrences\n",
            "Value 1: 364 occurrences\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Count occurrences of each unique value (0s and 1s)\n",
        "unique_values, counts = np.unique(binary_predictions, return_counts=True)\n",
        "\n",
        "# Print results\n",
        "for value, count in zip(unique_values, counts):\n",
        "    print(f\"Value {value}: {count} occurrences\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "saint_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
