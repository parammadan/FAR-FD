{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "vLGyxxRwcNFq",
    "ExecuteTime": {
     "end_time": "2025-04-09T13:30:33.755925Z",
     "start_time": "2025-04-09T13:30:26.253116Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import faiss\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Loading the embeddings that have been generated using the SAINT encoder as detailed in the paper. [G. Somepalli 2021] "
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCdTUn0Zbt7O",
    "outputId": "319cd6c1-35bb-49e0-d242-9ba54812b565",
    "ExecuteTime": {
     "end_time": "2025-04-09T13:32:50.467918Z",
     "start_time": "2025-04-09T13:32:50.448806Z"
    }
   },
   "source": [
    "saint_embeddings_path = \"../5_embeddings/cls_embeddings_time.struct_time(tm_year=2025, tm_mon=2, tm_mday=7, tm_hour=19, tm_min=40, tm_sec=22, tm_wday=4, tm_yday=38, tm_isdst=0).npy\"\n",
    "\n",
    "saint_cls_embeddings = np.load(saint_embeddings_path)\n",
    "print(saint_cls_embeddings.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118108, 32)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:33:00.009294Z",
     "start_time": "2025-04-09T13:32:59.831097Z"
    }
   },
   "source": [
    "train_df = pd.read_csv(\"../2_dataset/final/train_df.csv\")\n",
    "train_df.head(10)\n"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../2_dataset/final/train_df.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../2_dataset/final/train_df.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m train_df\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m10\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\finance-rag\\finance-rag\\new-reranked\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1014\u001B[0m     dialect,\n\u001B[0;32m   1015\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1023\u001B[0m )\n\u001B[0;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\finance-rag\\finance-rag\\new-reranked\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\Documents\\finance-rag\\finance-rag\\new-reranked\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\finance-rag\\finance-rag\\new-reranked\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1887\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1889\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\Documents\\finance-rag\\finance-rag\\new-reranked\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../2_dataset/final/train_df.csv'"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76770, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = pd.read_csv(\"../2_dataset/final/train_y_df.csv\")\n",
    "test_df = pd.read_csv(\"../2_dataset/final/test_df.csv\")\n",
    "\n",
    "print(f\"test_df.shape: {test_df.shape}\\ttrain_y.shape{train_y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_df = pd.read_csv(\"../2_dataset/final/test_y_df.csv\")\n",
    "test_y_df['isFraud'] = test_y_df['isFraud'].astype(np.float32)\n",
    "test_y_df['isFraud']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76770, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv(\"../2_dataset/final/val_df.csv\")\n",
    "val_y_df = pd.read_csv(\"../2_dataset/final/val_y_df.csv\")\n",
    "\n",
    "val_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17711</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17712</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17713</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17714</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17715</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17716 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       isFraud\n",
       "0          0.0\n",
       "1          0.0\n",
       "2          0.0\n",
       "3          0.0\n",
       "4          0.0\n",
       "...        ...\n",
       "17711      0.0\n",
       "17712      0.0\n",
       "17713      0.0\n",
       "17714      0.0\n",
       "17715      0.0\n",
       "\n",
       "[17716 rows x 1 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting SAINT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total embed shape: {saint_cls_embeddings.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute 65% of the total rows\n",
    "total_rows = saint_cls_embeddings.shape[0]\n",
    "train_size = int(0.65 * total_rows)  # 65% of 118108\n",
    "\n",
    "# Slice the top 65%\n",
    "train_embeddings = saint_cls_embeddings[:train_size]  # First 65%\n",
    "print(f\"Train embed shape: {train_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = saint_cls_embeddings.shape[0]\n",
    "test_size = int(0.8 * total_rows) \n",
    "\n",
    "val_embeddings = saint_cls_embeddings[train_size:test_size]\n",
    "print(len(val_embeddings)) # 15% of total rows.)\n",
    "      \n",
    "test_embeddings = saint_cls_embeddings[test_size:]\n",
    "# print(f\"Total embed shape: {saint_cls_embeddings.shape}\")\n",
    "# print(f\"Train embed shape: {test_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FAISS Index and Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OA6ULff8bxV-",
    "outputId": "33b81774-3e8c-48c4-cd3a-f81370fc6775"
   },
   "outputs": [],
   "source": [
    "def create_index(num_embeddings, dimension):\n",
    "    # num_embeddings = 76770\n",
    "    # dimension = 32\n",
    "\n",
    "    index = faiss.IndexFlatL2(dimension)  # L2 similarity\n",
    "    index.add(train_embeddings)  # index of pre-computed embeddings\n",
    "\n",
    "    # k = 120  # as best result for 120\n",
    "    return index\n",
    "\n",
    "# query_vector = np.random.random((1, dimension)).astype(\"float32\") #random for now\n",
    "# print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss(query_vector):\n",
    "\n",
    "    index = create_index(76770, 32)\n",
    "\n",
    "    # Convert PyTorch tensor to NumPy\n",
    "    if isinstance(query_vector, torch.Tensor):\n",
    "        query_vector = query_vector.detach().cpu().numpy()\n",
    "\n",
    "    query_vector = query_vector.astype(\"float32\").reshape(1, -1)\n",
    "\n",
    "    distances, indices = index.search(query_vector, k=120) # by default using euclidean distance for similarity\n",
    "    indices = indices.flatten()\n",
    "    \n",
    "    return distances, indices\n",
    "\n",
    "# distances, indices = index.search(\n",
    "#     query_vector, k\n",
    "# )  \n",
    "\n",
    "# print(\"Input Sample embedding:\", query_vector)\n",
    "# print(\"Indices of nearest neighbors:\", indices)\n",
    "# print(\"L2 norm distances\", distances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 distance component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WJLNSJdcbznQ"
   },
   "outputs": [],
   "source": [
    "def compute_similarity(distances, dropout=0.2):\n",
    "\n",
    "    # print(f\"distances.shape before: {distances.shape}\")\n",
    "    distances = distances.flatten()\n",
    "    # print(f\"distances.shape after: {distances.shape}\")\n",
    "\n",
    "    # Apply softmax to the negative distances\n",
    "    similarities = np.exp(-distances)\n",
    "    softmax_scores = similarities / np.sum(similarities)\n",
    "\n",
    "    # Apply dropout (randomly zero out some softmax scores)\n",
    "    dropout_mask = np.random.binomial(1, 1 - dropout, size=softmax_scores.shape)\n",
    "    dropped_softmax_scores = softmax_scores * dropout_mask\n",
    "\n",
    "    final_softmax = dropped_softmax_scores / np.sum(dropped_softmax_scores)\n",
    "    # how to weigh in the final embedding?\n",
    "    return final_softmax, distances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mask to drop the dropped out values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_mask(arr1, arr2, arr3):\n",
    "    \"\"\"\n",
    "    Removes elements from arr2 and arr3 where corresponding indices in arr1 are zero.\n",
    "    \"\"\"\n",
    "    mask = arr1 != 0  # Create a boolean mask where arr1 is nonzero\n",
    "    return arr1[mask], arr2[mask], arr3[mask]\n",
    "\n",
    "# S_x_xi, indices, distances = filter_by_mask(S_x_xi, indices, distances)\n",
    "\n",
    "# print(\"Filtered S_x_xi:\", S_x_xi)\n",
    "# print(\"Filtered indices:\", indices)\n",
    "# print(\"Filtered distances:\", distances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linear trasnform the value component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KlB6RGt6cLid",
    "outputId": "27a8197e-bb5c-4bd2-f934-6fcc9eb8cb6f"
   },
   "outputs": [],
   "source": [
    "class MLP_L1(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP_L1, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 32)\n",
    "        self.activation1 = nn.SiLU()\n",
    "\n",
    "        self.layer2 = nn.Linear(32, 32)\n",
    "        self.activation2 = nn.SiLU()\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.layer3 = nn.Linear(32, 32)  # Fix: Output should be 32\n",
    "        self.activation3 = nn.SiLU()  # Fix: Apply SiLU activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation1(self.layer1(x))\n",
    "        x = self.dropout2(self.activation2(self.layer2(x)))\n",
    "        x = self.activation3(self.layer3(x))  # Fix: Apply activation & dropout\n",
    "        return x\n",
    "\n",
    "\n",
    "# # Create model instance\n",
    "# model = MLP_L1(l1_dist.shape[0])\n",
    "\n",
    "# # Convert input to tensor and pass it through the model\n",
    "# l1_dist_tensor = torch.tensor(l1_dist, dtype=torch.float32)\n",
    "\n",
    "# w_v_l1 = model(l1_dist_tensor).detach().numpy()\n",
    "# print(w_v_l1)\n",
    "\n",
    "# print(w_v_l1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXAE9Vzwcn2T",
    "outputId": "5541a00b-ec38-4c32-e4c2-0f6f8998f8fc"
   },
   "outputs": [],
   "source": [
    "def compute_l1(distances):\n",
    "    l1_dist = np.sqrt(distances)\n",
    "    # print(l1_dist, l1_dist.shape)\n",
    "\n",
    "    model = MLP_L1(l1_dist.shape[0])\n",
    "\n",
    "    # Convert input to tensor and pass it through the model\n",
    "    l1_dist_tensor = torch.tensor(l1_dist, dtype=torch.float32)\n",
    "\n",
    "    w_v_l1 = model(l1_dist_tensor).detach().numpy()\n",
    "    # print(w_v_l1)\n",
    "\n",
    "    # print(w_v_l1.shape)\n",
    "\n",
    "    return w_v_l1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified MLP to output shape (60,)\n",
    "class MLP_Wy(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP_Wy, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)  # First layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 32)  # Output layer (1 neuron)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)  # Shape (60, 1)\n",
    "        return x.squeeze(1)  # Shape (60,)\n",
    "\n",
    "# Instantiate MLP with input_dim=32 (from Wy)\n",
    " # Expected: (60,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76765</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76766</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76767</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76768</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76769</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76770 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       isFraud\n",
       "0          0.0\n",
       "1          0.0\n",
       "2          0.0\n",
       "3          0.0\n",
       "4          0.0\n",
       "...        ...\n",
       "76765      0.0\n",
       "76766      0.0\n",
       "76767      0.0\n",
       "76768      1.0\n",
       "76769      0.0\n",
       "\n",
       "[76770 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wy(indices):\n",
    "    y_i = train_y['isFraud'].loc[indices].values\n",
    "    # print(f\"y_i.shape: {y_i.shape}\")\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "    feature_target_tensor = torch.tensor(y_i, dtype=torch.long)\n",
    "\n",
    "# Define Wy: An embedding layer to map to 32-dim space\n",
    "    embedding_dim = 32\n",
    "    num_classes = 2  # Since input values are 0 or 1\n",
    "\n",
    "    Wy = nn.Embedding(num_classes, embedding_dim)\n",
    "\n",
    "    mlp = MLP_Wy(input_dim=embedding_dim)\n",
    "\n",
    "# Compute embeddings using Wy\n",
    "    embeddings = Wy(feature_target_tensor)  # Shape: (60, 32)\n",
    "\n",
    "    # Pass embeddings through MLP\n",
    "    w_y = mlp(embeddings)  # Shape: (60,)\n",
    "\n",
    "    # print(\"MLP Output Shape:\", w_y.shape) \n",
    "\n",
    "    return w_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_value(w_v_l1, w_y):\n",
    "    # Compute the dot product of w_v_l1 and w_y\n",
    "    w_y_npy = w_y.detach().numpy()\n",
    "    value = w_y_npy + w_v_l1\n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping S to do S * V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_z_in(S_x_xi, value):\n",
    "    S_x_xi = S_x_xi.reshape(1, -1)\n",
    "    # print(S_x_xi.shape)  # Output should be (1, 95)\n",
    "\n",
    "    # result = (S_x_xi @ value)\n",
    "\n",
    "    # # Summation over all elements (since it's 1D)\n",
    "    # Z = np.sum(result)\n",
    "\n",
    "    # Assuming S_x_xi is (1, 95) and value is (95, 32)\n",
    "    numerator = np.sum(S_x_xi @ value)  # Sum the weighted contributions (scalar)\n",
    "    denominator = np.sum(S_x_xi)        # Total sum of weights (scalar)\n",
    "    z_in = numerator / denominator         # Weighted average as a single scalar\n",
    "\n",
    "    # print(\"Z:\", z_in)\n",
    "    # print(\"Z Shape:\", z_in.shape)  # Expected: torch.Size([])\n",
    "\n",
    "\n",
    "    return z_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = test_df.iloc[44].values\n",
    "x_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, input_emb_dim, hidden_dim=32, dropout_prob=0.2):\n",
    "        super(Predictor, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_emb_dim + 1  # Adding 2 for weighted_avg and f_z_in\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.LayerNorm(self.input_dim),\n",
    "            nn.Linear(self.input_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        \n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_prob),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_emb, weighted_avg):\n",
    "        # Ensure correct shape for scalar inputs\n",
    "        weighted_avg = weighted_avg.unsqueeze(-1)  \n",
    "    \n",
    "        # Concatenate all inputs\n",
    "        combined = torch.cat([input_emb, weighted_avg], dim=-1)\n",
    "\n",
    "        # Pass through MLP blocks\n",
    "        x = self.block1(combined)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_samples(test_df, test_embeddings):\n",
    "#     test_i = test_df.iloc[i].values #shape (182,)\n",
    "#     query_vector = test_embeddings[i] #shape (32,)\n",
    "#     distances, indices = search_faiss(query_vector) # both shape (120,) and flatten\n",
    "#     S_x_xi, distances = compute_similarity(distances)\n",
    "#     S_x_xi, indices, distances = filter_by_mask(S_x_xi, indices, distances)\n",
    "#     w_v_l1 = compute_l1(distances) #shape (32,)\n",
    "#     w_y = compute_wy(indices) #shape (32,)\n",
    "#     value = compute_value(w_v_l1, w_y)\n",
    "#     z_in = compute_z_in(S_x_xi, value)\n",
    "\n",
    "#     input_to_mlp = test_i * z_in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_samples(train_df, train_embeddings):\n",
    "    \"\"\"Processes all samples and returns input tensor and labels.\"\"\"\n",
    "    input_list = []\n",
    "    labels = []\n",
    "\n",
    "\n",
    "    for i in range(len(train_df)):  # Process all samples\n",
    "        test_i = torch.tensor(train_df.iloc[i].values, dtype=torch.float32)  # shape (182,)\n",
    "        query_embedding = torch.tensor(train_embeddings[i], dtype=torch.float32)  # shape (32,)\n",
    "\n",
    "        distances, indices = search_faiss(query_embedding)\n",
    "        S_x_xi, distances = compute_similarity(distances)\n",
    "        S_x_xi, indices, distances = filter_by_mask(S_x_xi, indices, distances)\n",
    "        w_v_l1 = compute_l1(distances)  # shape (32,)\n",
    "        w_y = compute_wy(indices)  # shape (32,)\n",
    "        value = compute_value(w_v_l1, w_y)\n",
    "        z_in = compute_z_in(S_x_xi, value)  # Scalar (single value)\n",
    "\n",
    "\n",
    "        # Append only test_i (input_emb) and z_in (weighted_avg), not query_embedding\n",
    "        z_in = torch.tensor(z_in, dtype=torch.float32)\n",
    "\n",
    "        input_list.append((test_i, z_in))\n",
    "\n",
    "        labels.append(torch.tensor(train_y.iloc[i]['isFraud'], dtype=torch.float32))  # Assuming label is in df\n",
    "\n",
    "\n",
    "\n",
    "    return input_list, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_process_samples(test_df, test_embeddings):\n",
    "    \"\"\"Processes all samples and returns input tensor and labels.\"\"\"\n",
    "    input_list = []\n",
    "    labels = []\n",
    "\n",
    "\n",
    "    for i in range(len(test_df)):  # Process all samples\n",
    "        test_i = torch.tensor(test_df.iloc[i].values, dtype=torch.float32)  # shape (182,)\n",
    "        query_embedding = torch.tensor(test_embeddings[i], dtype=torch.float32)  # shape (32,)\n",
    "\n",
    "        distances, indices = search_faiss(query_embedding)\n",
    "        S_x_xi, distances = compute_similarity(distances)\n",
    "        S_x_xi, indices, distances = filter_by_mask(S_x_xi, indices, distances)\n",
    "        w_v_l1 = compute_l1(distances)  # shape (32,)\n",
    "        w_y = compute_wy(indices)  # shape (32,)\n",
    "        value = compute_value(w_v_l1, w_y)\n",
    "        z_in = compute_z_in(S_x_xi, value)  # Scalar (single value)\n",
    "\n",
    "\n",
    "        # Append only test_i (input_emb) and z_in (weighted_avg), not query_embedding\n",
    "        z_in = torch.tensor(z_in, dtype=torch.float32)\n",
    "\n",
    "        input_list.append((test_i, z_in))\n",
    "\n",
    "        labels.append(torch.tensor(val_y_df.iloc[i]['isFraud'], dtype=torch.float32))  # Assuming label is in df\n",
    "\n",
    "\n",
    "\n",
    "    return input_list, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_list, labels = process_samples(train_df, train_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.3525, Train AUCPR: 0.0367, Val Loss: 0.1571, Val AUCPR: 0.0353\n",
      "Epoch [2/100], Train Loss: 0.1382, Train AUCPR: 0.1674, Val Loss: 0.1625, Val AUCPR: 0.0365\n",
      "Epoch [3/100], Train Loss: 0.1275, Train AUCPR: 0.2569, Val Loss: 0.1660, Val AUCPR: 0.0374\n",
      "Epoch [4/100], Train Loss: 0.1210, Train AUCPR: 0.2979, Val Loss: 0.1709, Val AUCPR: 0.0378\n",
      "Epoch [5/100], Train Loss: 0.1180, Train AUCPR: 0.3164, Val Loss: 0.1746, Val AUCPR: 0.0379\n",
      "Epoch [6/100], Train Loss: 0.1162, Train AUCPR: 0.3284, Val Loss: 0.1772, Val AUCPR: 0.0378\n",
      "Epoch [7/100], Train Loss: 0.1149, Train AUCPR: 0.3374, Val Loss: 0.1792, Val AUCPR: 0.0378\n",
      "Epoch [8/100], Train Loss: 0.1140, Train AUCPR: 0.3450, Val Loss: 0.1808, Val AUCPR: 0.0377\n",
      "Epoch [9/100], Train Loss: 0.1132, Train AUCPR: 0.3512, Val Loss: 0.1821, Val AUCPR: 0.0376\n",
      "Epoch [10/100], Train Loss: 0.1126, Train AUCPR: 0.3571, Val Loss: 0.1831, Val AUCPR: 0.0376\n",
      "Epoch [11/100], Train Loss: 0.1120, Train AUCPR: 0.3624, Val Loss: 0.1840, Val AUCPR: 0.0375\n",
      "Epoch [12/100], Train Loss: 0.1114, Train AUCPR: 0.3672, Val Loss: 0.1848, Val AUCPR: 0.0374\n",
      "Epoch [13/100], Train Loss: 0.1109, Train AUCPR: 0.3716, Val Loss: 0.1855, Val AUCPR: 0.0374\n",
      "Epoch [14/100], Train Loss: 0.1105, Train AUCPR: 0.3759, Val Loss: 0.1862, Val AUCPR: 0.0373\n",
      "Epoch [15/100], Train Loss: 0.1100, Train AUCPR: 0.3798, Val Loss: 0.1868, Val AUCPR: 0.0372\n",
      "Epoch [16/100], Train Loss: 0.1096, Train AUCPR: 0.3837, Val Loss: 0.1874, Val AUCPR: 0.0372\n",
      "Epoch [17/100], Train Loss: 0.1092, Train AUCPR: 0.3874, Val Loss: 0.1880, Val AUCPR: 0.0371\n",
      "Epoch [18/100], Train Loss: 0.1088, Train AUCPR: 0.3910, Val Loss: 0.1886, Val AUCPR: 0.0371\n",
      "Epoch [19/100], Train Loss: 0.1084, Train AUCPR: 0.3945, Val Loss: 0.1892, Val AUCPR: 0.0370\n",
      "Epoch [20/100], Train Loss: 0.1080, Train AUCPR: 0.3978, Val Loss: 0.1897, Val AUCPR: 0.0370\n",
      "Epoch [21/100], Train Loss: 0.1076, Train AUCPR: 0.4011, Val Loss: 0.1902, Val AUCPR: 0.0370\n",
      "Epoch [22/100], Train Loss: 0.1072, Train AUCPR: 0.4043, Val Loss: 0.1908, Val AUCPR: 0.0369\n",
      "Epoch [23/100], Train Loss: 0.1069, Train AUCPR: 0.4076, Val Loss: 0.1913, Val AUCPR: 0.0369\n",
      "Epoch [24/100], Train Loss: 0.1065, Train AUCPR: 0.4107, Val Loss: 0.1918, Val AUCPR: 0.0369\n",
      "Epoch [25/100], Train Loss: 0.1062, Train AUCPR: 0.4139, Val Loss: 0.1922, Val AUCPR: 0.0369\n",
      "Epoch [26/100], Train Loss: 0.1058, Train AUCPR: 0.4170, Val Loss: 0.1927, Val AUCPR: 0.0369\n",
      "Epoch [27/100], Train Loss: 0.1055, Train AUCPR: 0.4202, Val Loss: 0.1931, Val AUCPR: 0.0369\n",
      "Epoch [28/100], Train Loss: 0.1051, Train AUCPR: 0.4233, Val Loss: 0.1936, Val AUCPR: 0.0369\n",
      "Epoch [29/100], Train Loss: 0.1048, Train AUCPR: 0.4265, Val Loss: 0.1940, Val AUCPR: 0.0369\n",
      "Epoch [30/100], Train Loss: 0.1044, Train AUCPR: 0.4295, Val Loss: 0.1944, Val AUCPR: 0.0369\n",
      "Epoch [31/100], Train Loss: 0.1041, Train AUCPR: 0.4327, Val Loss: 0.1948, Val AUCPR: 0.0369\n",
      "Epoch [32/100], Train Loss: 0.1037, Train AUCPR: 0.4359, Val Loss: 0.1952, Val AUCPR: 0.0369\n",
      "Epoch [33/100], Train Loss: 0.1034, Train AUCPR: 0.4390, Val Loss: 0.1956, Val AUCPR: 0.0369\n",
      "Epoch [34/100], Train Loss: 0.1030, Train AUCPR: 0.4422, Val Loss: 0.1960, Val AUCPR: 0.0369\n",
      "Epoch [35/100], Train Loss: 0.1027, Train AUCPR: 0.4454, Val Loss: 0.1964, Val AUCPR: 0.0368\n",
      "Epoch [36/100], Train Loss: 0.1023, Train AUCPR: 0.4486, Val Loss: 0.1968, Val AUCPR: 0.0368\n",
      "Epoch [37/100], Train Loss: 0.1020, Train AUCPR: 0.4517, Val Loss: 0.1972, Val AUCPR: 0.0368\n",
      "Epoch [38/100], Train Loss: 0.1016, Train AUCPR: 0.4548, Val Loss: 0.1976, Val AUCPR: 0.0368\n",
      "Epoch [39/100], Train Loss: 0.1013, Train AUCPR: 0.4578, Val Loss: 0.1980, Val AUCPR: 0.0368\n",
      "Epoch [40/100], Train Loss: 0.1010, Train AUCPR: 0.4609, Val Loss: 0.1985, Val AUCPR: 0.0367\n",
      "Epoch [41/100], Train Loss: 0.1006, Train AUCPR: 0.4638, Val Loss: 0.1989, Val AUCPR: 0.0367\n",
      "Epoch [42/100], Train Loss: 0.1003, Train AUCPR: 0.4667, Val Loss: 0.1993, Val AUCPR: 0.0367\n",
      "Epoch [43/100], Train Loss: 0.1000, Train AUCPR: 0.4696, Val Loss: 0.1998, Val AUCPR: 0.0366\n",
      "Epoch [44/100], Train Loss: 0.0996, Train AUCPR: 0.4723, Val Loss: 0.2002, Val AUCPR: 0.0366\n",
      "Epoch [45/100], Train Loss: 0.0993, Train AUCPR: 0.4751, Val Loss: 0.2007, Val AUCPR: 0.0366\n",
      "Epoch [46/100], Train Loss: 0.0990, Train AUCPR: 0.4777, Val Loss: 0.2011, Val AUCPR: 0.0365\n",
      "Epoch [47/100], Train Loss: 0.0987, Train AUCPR: 0.4804, Val Loss: 0.2016, Val AUCPR: 0.0365\n",
      "Epoch [48/100], Train Loss: 0.0984, Train AUCPR: 0.4830, Val Loss: 0.2021, Val AUCPR: 0.0365\n",
      "Epoch [49/100], Train Loss: 0.0981, Train AUCPR: 0.4855, Val Loss: 0.2026, Val AUCPR: 0.0365\n",
      "Epoch [50/100], Train Loss: 0.0978, Train AUCPR: 0.4881, Val Loss: 0.2031, Val AUCPR: 0.0364\n",
      "Epoch [51/100], Train Loss: 0.0975, Train AUCPR: 0.4905, Val Loss: 0.2036, Val AUCPR: 0.0364\n",
      "Epoch [52/100], Train Loss: 0.0972, Train AUCPR: 0.4929, Val Loss: 0.2041, Val AUCPR: 0.0364\n",
      "Epoch [53/100], Train Loss: 0.0969, Train AUCPR: 0.4952, Val Loss: 0.2046, Val AUCPR: 0.0364\n",
      "Epoch [54/100], Train Loss: 0.0966, Train AUCPR: 0.4976, Val Loss: 0.2051, Val AUCPR: 0.0364\n",
      "Epoch [55/100], Train Loss: 0.0964, Train AUCPR: 0.4998, Val Loss: 0.2057, Val AUCPR: 0.0363\n",
      "Epoch [56/100], Train Loss: 0.0961, Train AUCPR: 0.5021, Val Loss: 0.2062, Val AUCPR: 0.0363\n",
      "Epoch [57/100], Train Loss: 0.0958, Train AUCPR: 0.5043, Val Loss: 0.2068, Val AUCPR: 0.0363\n",
      "Epoch [58/100], Train Loss: 0.0956, Train AUCPR: 0.5064, Val Loss: 0.2073, Val AUCPR: 0.0363\n",
      "Epoch [59/100], Train Loss: 0.0953, Train AUCPR: 0.5085, Val Loss: 0.2079, Val AUCPR: 0.0363\n",
      "Epoch [60/100], Train Loss: 0.0951, Train AUCPR: 0.5106, Val Loss: 0.2084, Val AUCPR: 0.0363\n",
      "Epoch [61/100], Train Loss: 0.0948, Train AUCPR: 0.5126, Val Loss: 0.2090, Val AUCPR: 0.0363\n",
      "Epoch [62/100], Train Loss: 0.0946, Train AUCPR: 0.5147, Val Loss: 0.2095, Val AUCPR: 0.0362\n",
      "Epoch [63/100], Train Loss: 0.0943, Train AUCPR: 0.5167, Val Loss: 0.2101, Val AUCPR: 0.0362\n",
      "Epoch [64/100], Train Loss: 0.0941, Train AUCPR: 0.5186, Val Loss: 0.2107, Val AUCPR: 0.0362\n",
      "Epoch [65/100], Train Loss: 0.0939, Train AUCPR: 0.5205, Val Loss: 0.2112, Val AUCPR: 0.0362\n",
      "Epoch [66/100], Train Loss: 0.0936, Train AUCPR: 0.5224, Val Loss: 0.2118, Val AUCPR: 0.0362\n",
      "Epoch [67/100], Train Loss: 0.0934, Train AUCPR: 0.5243, Val Loss: 0.2124, Val AUCPR: 0.0362\n",
      "Epoch [68/100], Train Loss: 0.0932, Train AUCPR: 0.5261, Val Loss: 0.2130, Val AUCPR: 0.0362\n",
      "Epoch [69/100], Train Loss: 0.0929, Train AUCPR: 0.5279, Val Loss: 0.2135, Val AUCPR: 0.0362\n",
      "Epoch [70/100], Train Loss: 0.0927, Train AUCPR: 0.5297, Val Loss: 0.2141, Val AUCPR: 0.0362\n",
      "Epoch [71/100], Train Loss: 0.0925, Train AUCPR: 0.5314, Val Loss: 0.2147, Val AUCPR: 0.0362\n",
      "Epoch [72/100], Train Loss: 0.0923, Train AUCPR: 0.5331, Val Loss: 0.2153, Val AUCPR: 0.0362\n",
      "Epoch [73/100], Train Loss: 0.0921, Train AUCPR: 0.5348, Val Loss: 0.2158, Val AUCPR: 0.0362\n",
      "Epoch [74/100], Train Loss: 0.0919, Train AUCPR: 0.5365, Val Loss: 0.2164, Val AUCPR: 0.0362\n",
      "Epoch [75/100], Train Loss: 0.0917, Train AUCPR: 0.5381, Val Loss: 0.2170, Val AUCPR: 0.0362\n",
      "Epoch [76/100], Train Loss: 0.0915, Train AUCPR: 0.5397, Val Loss: 0.2175, Val AUCPR: 0.0362\n",
      "Epoch [77/100], Train Loss: 0.0913, Train AUCPR: 0.5413, Val Loss: 0.2181, Val AUCPR: 0.0362\n",
      "Epoch [78/100], Train Loss: 0.0911, Train AUCPR: 0.5429, Val Loss: 0.2187, Val AUCPR: 0.0362\n",
      "Epoch [79/100], Train Loss: 0.0909, Train AUCPR: 0.5445, Val Loss: 0.2193, Val AUCPR: 0.0362\n",
      "Epoch [80/100], Train Loss: 0.0907, Train AUCPR: 0.5461, Val Loss: 0.2198, Val AUCPR: 0.0362\n",
      "Epoch [81/100], Train Loss: 0.0905, Train AUCPR: 0.5476, Val Loss: 0.2204, Val AUCPR: 0.0362\n",
      "Epoch [82/100], Train Loss: 0.0903, Train AUCPR: 0.5491, Val Loss: 0.2210, Val AUCPR: 0.0362\n",
      "Epoch [83/100], Train Loss: 0.0901, Train AUCPR: 0.5506, Val Loss: 0.2216, Val AUCPR: 0.0362\n",
      "Epoch [84/100], Train Loss: 0.0899, Train AUCPR: 0.5521, Val Loss: 0.2222, Val AUCPR: 0.0362\n",
      "Epoch [85/100], Train Loss: 0.0897, Train AUCPR: 0.5535, Val Loss: 0.2228, Val AUCPR: 0.0362\n",
      "Epoch [86/100], Train Loss: 0.0896, Train AUCPR: 0.5550, Val Loss: 0.2234, Val AUCPR: 0.0362\n",
      "Epoch [87/100], Train Loss: 0.0894, Train AUCPR: 0.5564, Val Loss: 0.2240, Val AUCPR: 0.0362\n",
      "Epoch [88/100], Train Loss: 0.0892, Train AUCPR: 0.5579, Val Loss: 0.2246, Val AUCPR: 0.0363\n",
      "Epoch [89/100], Train Loss: 0.0890, Train AUCPR: 0.5593, Val Loss: 0.2252, Val AUCPR: 0.0363\n",
      "Epoch [90/100], Train Loss: 0.0889, Train AUCPR: 0.5606, Val Loss: 0.2258, Val AUCPR: 0.0363\n",
      "Epoch [91/100], Train Loss: 0.0887, Train AUCPR: 0.5620, Val Loss: 0.2264, Val AUCPR: 0.0363\n",
      "Epoch [92/100], Train Loss: 0.0885, Train AUCPR: 0.5634, Val Loss: 0.2270, Val AUCPR: 0.0363\n",
      "Epoch [93/100], Train Loss: 0.0884, Train AUCPR: 0.5647, Val Loss: 0.2276, Val AUCPR: 0.0363\n",
      "Epoch [94/100], Train Loss: 0.0882, Train AUCPR: 0.5660, Val Loss: 0.2281, Val AUCPR: 0.0363\n",
      "Epoch [95/100], Train Loss: 0.0880, Train AUCPR: 0.5673, Val Loss: 0.2287, Val AUCPR: 0.0363\n",
      "Epoch [96/100], Train Loss: 0.0879, Train AUCPR: 0.5686, Val Loss: 0.2293, Val AUCPR: 0.0363\n",
      "Epoch [97/100], Train Loss: 0.0877, Train AUCPR: 0.5698, Val Loss: 0.2299, Val AUCPR: 0.0363\n",
      "Epoch [98/100], Train Loss: 0.0875, Train AUCPR: 0.5711, Val Loss: 0.2304, Val AUCPR: 0.0364\n",
      "Epoch [99/100], Train Loss: 0.0874, Train AUCPR: 0.5723, Val Loss: 0.2310, Val AUCPR: 0.0364\n",
      "Epoch [100/100], Train Loss: 0.0872, Train AUCPR: 0.5736, Val Loss: 0.2316, Val AUCPR: 0.0364\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import average_precision_score\n",
    "import torch\n",
    "\n",
    "def train_model(test_df, test_embeddings, model, optimizer, criterion, batch_size=256, epochs=25):\n",
    "    model.train()\n",
    "    \n",
    "    # Process data\n",
    "    input_list, labels = process_samples(train_df, train_embeddings)\n",
    "    val_input_list, val_labels = process_samples(val_df, val_embeddings)\n",
    "\n",
    "    # Create DataLoader\n",
    "    dataset = TensorDataset(\n",
    "        torch.stack([item[0] for item in input_list]),  # test_i (input_emb)\n",
    "        torch.stack([item[1] for item in input_list]),  # z_in (weighted_avg)\n",
    "        torch.stack(labels)  # Labels\n",
    "    )\n",
    "\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.stack([item[0] for item in val_input_list]),  # test_i (input_emb)\n",
    "        torch.stack([item[1] for item in val_input_list]),  # z_in (weighted_avg)\n",
    "        torch.stack(val_labels)  # Labels\n",
    "    )\n",
    "\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Training loop for multiple epochs\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0  # Track loss for each epoch\n",
    "        all_targets = []\n",
    "        all_outputs = []\n",
    "\n",
    "        for batch in dataloader:\n",
    "            input_emb, z_in, target = batch\n",
    "            target = target.unsqueeze(-1)  # Make target shape (batch_size, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_emb, z_in)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()  # Accumulate loss for epoch\n",
    "            \n",
    "            # Collect predictions & targets for AUCPR\n",
    "            all_outputs.append(output.detach().cpu())  # Move to CPU to avoid memory issues\n",
    "            all_targets.append(target.detach().cpu())\n",
    "\n",
    "        # Compute AUCPR at the end of the epoch\n",
    "        all_outputs = torch.cat(all_outputs).numpy()\n",
    "        all_targets = torch.cat(all_targets).numpy()\n",
    "        aucpr = average_precision_score(all_targets, all_outputs)\n",
    "        # print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(dataloader):.4f}, AUCPR: {aucpr:.4f}\")\n",
    "\n",
    "        # Validation Step\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_val_targets = []\n",
    "        all_val_outputs = []\n",
    "\n",
    "        with torch.no_grad():  # No gradient computation for validation\n",
    "            for batch in val_dataloader:\n",
    "                input_emb, z_in, target = batch\n",
    "                target = target.unsqueeze(-1)\n",
    "\n",
    "                output = model(input_emb, z_in)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                all_val_outputs.append(output.cpu())\n",
    "                all_val_targets.append(target.cpu())\n",
    "\n",
    "        # Compute AUCPR for validation set\n",
    "        all_val_outputs = torch.cat(all_val_outputs).numpy()\n",
    "        all_val_targets = torch.cat(all_val_targets).numpy()\n",
    "        val_aucpr = average_precision_score(all_val_targets, all_val_outputs)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], \"\n",
    "              f\"Train Loss: {epoch_loss/len(dataloader):.4f}, Train AUCPR: {aucpr:.4f}, \"\n",
    "              f\"Val Loss: {val_loss/len(val_dataloader):.4f}, Val AUCPR: {val_aucpr:.4f}\")\n",
    "\n",
    "\n",
    "# Model, optimizer, and loss function\n",
    "input_emb_dim = 182  # Assuming this based on test_df features\n",
    "model = Predictor(input_emb_dim=input_emb_dim)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "\n",
    "# Call training loop (assuming test_df and test_embeddings are available)\n",
    "train_model(train_df, train_embeddings, model, optimizer, criterion, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
