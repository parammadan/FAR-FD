{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vLGyxxRwcNFq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import average_precision_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCdTUn0Zbt7O",
        "outputId": "319cd6c1-35bb-49e0-d242-9ba54812b565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(118108, 32)\n"
          ]
        }
      ],
      "source": [
        "# loading the entire test embeddings\n",
        "path = \"../5_embeddings/cls_embeddings_time.struct_time(tm_year=2025, tm_mon=2, tm_mday=7, tm_hour=19, tm_min=40, tm_sec=22, tm_wday=4, tm_yday=38, tm_isdst=0).npy\"\n",
        "\n",
        "cls_embeddings = np.load(path)\n",
        "print(cls_embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cls</th>\n",
              "      <th>ProductCD</th>\n",
              "      <th>card4</th>\n",
              "      <th>card6</th>\n",
              "      <th>P_emaildomain</th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>TransactionDT</th>\n",
              "      <th>TransactionAmt</th>\n",
              "      <th>card1</th>\n",
              "      <th>card2</th>\n",
              "      <th>...</th>\n",
              "      <th>V312</th>\n",
              "      <th>V313</th>\n",
              "      <th>V314</th>\n",
              "      <th>V315</th>\n",
              "      <th>V316</th>\n",
              "      <th>V317</th>\n",
              "      <th>V318</th>\n",
              "      <th>V319</th>\n",
              "      <th>V320</th>\n",
              "      <th>V321</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.291883</td>\n",
              "      <td>-0.329939</td>\n",
              "      <td>0.108390</td>\n",
              "      <td>-0.145421</td>\n",
              "      <td>-0.399322</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>0.892993</td>\n",
              "      <td>0.871243</td>\n",
              "      <td>-0.359702</td>\n",
              "      <td>0.680504</td>\n",
              "      <td>-0.412094</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.030054</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>-1.594876</td>\n",
              "      <td>-1.467121</td>\n",
              "      <td>8.134522</td>\n",
              "      <td>-0.109308</td>\n",
              "      <td>0.711822</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.123148</td>\n",
              "      <td>-0.156138</td>\n",
              "      <td>-0.422421</td>\n",
              "      <td>1.487250</td>\n",
              "      <td>-0.265218</td>\n",
              "      <td>...</td>\n",
              "      <td>0.341765</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>1.611964</td>\n",
              "      <td>1.677853</td>\n",
              "      <td>-0.317889</td>\n",
              "      <td>-0.081355</td>\n",
              "      <td>-0.265218</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76765</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>-1.327921</td>\n",
              "      <td>-1.260784</td>\n",
              "      <td>-0.113217</td>\n",
              "      <td>-0.653259</td>\n",
              "      <td>-1.606254</td>\n",
              "      <td>...</td>\n",
              "      <td>1.009878</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76766</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>25</td>\n",
              "      <td>0.675641</td>\n",
              "      <td>0.648266</td>\n",
              "      <td>-0.075376</td>\n",
              "      <td>-0.002802</td>\n",
              "      <td>0.756523</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76767</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>0.418154</td>\n",
              "      <td>0.377752</td>\n",
              "      <td>0.150412</td>\n",
              "      <td>-1.485918</td>\n",
              "      <td>-0.226903</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>0.393449</td>\n",
              "      <td>0.090945</td>\n",
              "      <td>0.276274</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76768</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>0.605578</td>\n",
              "      <td>0.576883</td>\n",
              "      <td>-0.322279</td>\n",
              "      <td>-0.182963</td>\n",
              "      <td>0.577719</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>-0.222385</td>\n",
              "      <td>-0.249222</td>\n",
              "      <td>-0.229148</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76769</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>53</td>\n",
              "      <td>0.944849</td>\n",
              "      <td>0.929392</td>\n",
              "      <td>-0.317889</td>\n",
              "      <td>1.061025</td>\n",
              "      <td>1.229079</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.227583</td>\n",
              "      <td>0.393449</td>\n",
              "      <td>0.090945</td>\n",
              "      <td>0.276274</td>\n",
              "      <td>-0.048377</td>\n",
              "      <td>-0.062211</td>\n",
              "      <td>-0.058049</td>\n",
              "      <td>-0.055287</td>\n",
              "      <td>-0.088855</td>\n",
              "      <td>-0.074142</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>76770 rows Ã— 182 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       cls  ProductCD  card4  card6  P_emaildomain  Unnamed: 0  TransactionDT  \\\n",
              "0        0          4      2      1              2   -0.291883      -0.329939   \n",
              "1        0          4      3      2             16    0.892993       0.871243   \n",
              "2        0          4      2      2              1   -1.594876      -1.467121   \n",
              "3        0          4      3      2             19   -0.123148      -0.156138   \n",
              "4        0          4      3      2             16    1.611964       1.677853   \n",
              "...    ...        ...    ...    ...            ...         ...            ...   \n",
              "76765    0          4      3      2             16   -1.327921      -1.260784   \n",
              "76766    0          4      2      2             25    0.675641       0.648266   \n",
              "76767    0          4      1      1             16    0.418154       0.377752   \n",
              "76768    0          4      3      2             19    0.605578       0.576883   \n",
              "76769    0          4      3      2             53    0.944849       0.929392   \n",
              "\n",
              "       TransactionAmt     card1     card2  ...      V312      V313      V314  \\\n",
              "0            0.108390 -0.145421 -0.399322  ... -0.227583 -0.222385 -0.249222   \n",
              "1           -0.359702  0.680504 -0.412094  ... -0.030054 -0.222385 -0.249222   \n",
              "2            8.134522 -0.109308  0.711822  ... -0.227583 -0.222385 -0.249222   \n",
              "3           -0.422421  1.487250 -0.265218  ...  0.341765 -0.222385 -0.249222   \n",
              "4           -0.317889 -0.081355 -0.265218  ... -0.227583 -0.222385 -0.249222   \n",
              "...               ...       ...       ...  ...       ...       ...       ...   \n",
              "76765       -0.113217 -0.653259 -1.606254  ...  1.009878 -0.222385 -0.249222   \n",
              "76766       -0.075376 -0.002802  0.756523  ... -0.227583 -0.222385 -0.249222   \n",
              "76767        0.150412 -1.485918 -0.226903  ... -0.227583  0.393449  0.090945   \n",
              "76768       -0.322279 -0.182963  0.577719  ... -0.227583 -0.222385 -0.249222   \n",
              "76769       -0.317889  1.061025  1.229079  ... -0.227583  0.393449  0.090945   \n",
              "\n",
              "           V315      V316      V317      V318      V319      V320      V321  \n",
              "0     -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "1     -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "2     -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "3     -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "4     -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "76765 -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "76766 -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "76767  0.276274 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "76768 -0.229148 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "76769  0.276274 -0.048377 -0.062211 -0.058049 -0.055287 -0.088855 -0.074142  \n",
              "\n",
              "[76770 rows x 182 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"../2_dataset/final/train_df.csv\")\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(76770, 1)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_y = pd.read_csv(\"../2_dataset/final/train_y_df.csv\")\n",
        "train_y.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(\"../2_dataset/final/test_df.csv\")\n",
        "test_y = pd.read_csv(\"../2_dataset/final/test_y_df.csv\")\n",
        "test_y['isFraud'] = test_y['isFraud'].astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_df = pd.read_csv(\"../2_dataset/final/val_df.csv\")\n",
        "val_y = pd.read_csv(\"../2_dataset/final/val_y_df.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((76770, 1), (76770, 182))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_y.shape, train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((23622, 1), (23622, 182))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_y.shape, test_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((17716, 1), (17716, 182))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_y.shape, val_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Splitting Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total embeddings shape: (118108, 32)\n",
            "Train embeddings shape: (76770, 32)\n",
            "Val embeddings shape: (17716, 32)\n",
            "Test embeddings shape: (23622, 32)\n"
          ]
        }
      ],
      "source": [
        "# Compute 65% of the total rows\n",
        "total_rows = cls_embeddings.shape[0]\n",
        "train_size = int(0.65 * total_rows)  # 65% of 118108\n",
        "test_size = int(0.8 * total_rows)\n",
        "\n",
        "# Slice the top 65%\n",
        "train_embeddings = cls_embeddings[:train_size]\n",
        "val_embeddings = cls_embeddings[train_size:test_size]\n",
        "test_embeddings = cls_embeddings[test_size:]\n",
        "\n",
        "print(f\"Total embeddings shape: {cls_embeddings.shape}\")\n",
        "print(f\"Train embeddings shape: {train_embeddings.shape}\")\n",
        "print(f\"Val embeddings shape: {val_embeddings.shape}\")\n",
        "print(f\"Test embeddings shape: {test_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Important Shap features calculation using Random Forest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/saint_env/lib/python3.8/site-packages/sklearn/base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return fit_method(estimator, *args, **kwargs)\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'shap'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[45], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     20\u001b[0m rf_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Create an explainer object\u001b[39;00m\n\u001b[1;32m     25\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mExplainer(rf_model)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
          ]
        }
      ],
      "source": [
        "X_train = pd.read_csv(\"../2_dataset/final/train_df.csv\")\n",
        "y_train = pd.read_csv(\"../2_dataset/final/train_y_df.csv\")\n",
        "X_test = pd.read_csv(\"../2_dataset/final/test_df.csv\")\n",
        "y_test = pd.read_csv(\"../2_dataset/final/test_y_df.csv\")\n",
        "X_valid = pd.read_csv(\"../2_dataset/final/val_df.csv\")\n",
        "y_valid = pd.read_csv(\"../2_dataset/final/val_y_df.csv\")\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize Random Forest model\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,  # Number of trees\n",
        "    max_depth=15,  # Limit tree depth to avoid overfitting\n",
        "    class_weight=\"balanced\",  # Handle class imbalance\n",
        "    random_state=42,\n",
        "    n_jobs=-1  # Use all CPU cores\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "import shap\n",
        "\n",
        "# Create an explainer object\n",
        "explainer = shap.Explainer(rf_model)\n",
        "# Calculate SHAP values for the test set\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# PLOTTING THE IMPORTANT FEATURES\n",
        "shap_values_fixed = shap_values[:, :, 1]  # Select SHAP values for class 1\n",
        "# Ensure correct shape\n",
        "print(f\"Fixed SHAP values shape: {shap_values_fixed.shape}\")  # Should match (23622, 182)\n",
        "# Generate SHAP summary plot\n",
        "shap.summary_plot(shap_values_fixed, X_test)\n",
        "\n",
        "print(f\"Shape of SHAP values: {shap_values_fixed.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Faiss Index and Similarity Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA6ULff8bxV-",
        "outputId": "33b81774-3e8c-48c4-cd3a-f81370fc6775"
      },
      "outputs": [],
      "source": [
        "def create_index(num_embeddings, dimension):\n",
        "    # num_embeddings = 76770\n",
        "    # dimension = 32\n",
        "    index = faiss.IndexFlatL2(dimension)  # L2 similarity\n",
        "    index.add(train_embeddings)  # index of pre-computed embeddings\n",
        "\n",
        "    return index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_faiss(query_vector):\n",
        "    index = create_index(76770, 32)\n",
        "\n",
        "    # Convert PyTorch tensor to NumPy\n",
        "    if isinstance(query_vector, torch.Tensor):\n",
        "        query_vector = query_vector.detach().cpu().numpy()\n",
        "\n",
        "    query_vector = query_vector.astype(\"float32\").reshape(1, -1)\n",
        "    query_vector = query_vector.astype(\"float32\").reshape(1, -1)\n",
        "\n",
        "    # k = 120, As best result for 120\n",
        "    distances, indices = index.search(\n",
        "        query_vector, k=120\n",
        "    )  # by default using euclidean distance for similarity\n",
        "\n",
        "    indices = indices.flatten()\n",
        "    return distances, indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## L2 Distance Component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WJLNSJdcbznQ"
      },
      "outputs": [],
      "source": [
        "def compute_similarity(distances, dropout=0.2):\n",
        "\n",
        "    distances = distances.flatten()\n",
        "    # Apply softmax to the negative distances\n",
        "    similarities = np.exp(-distances)\n",
        "    softmax_scores = similarities / np.sum(similarities)\n",
        "\n",
        "    # Apply dropout (randomly zero out some softmax scores)\n",
        "    dropout_mask = np.random.binomial(1, 1 - dropout, size=softmax_scores.shape)\n",
        "    dropped_softmax_scores = softmax_scores * dropout_mask\n",
        "\n",
        "    # Renormilizing softmax scores so that they sum to 1 again.\n",
        "    final_softmax = dropped_softmax_scores / np.sum(dropped_softmax_scores)\n",
        "    \n",
        "    # how to weigh in the final embedding? -> weigh emebedding more if they are closer in vector space.\n",
        "    return final_softmax, distances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mask To Drop The Dropped Out Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_by_mask(arr1, arr2, arr3):\n",
        "    \"\"\"\n",
        "    Removes elements from arr2 and arr3 where corresponding indices in arr1 are zero.\n",
        "    \"\"\"\n",
        "    mask = arr1 != 0  # Create a boolean mask where arr1 is nonzero\n",
        "    return arr1[mask], arr2[mask], arr3[mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Trasnform, Value Component and Shap Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlB6RGt6cLid",
        "outputId": "27a8197e-bb5c-4bd2-f934-6fcc9eb8cb6f"
      },
      "outputs": [],
      "source": [
        "class MLP_Wv(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MLP_Wv, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "        self.activation1 = nn.SiLU()\n",
        "\n",
        "        self.layer2 = nn.Linear(32, 32)\n",
        "        self.activation2 = nn.SiLU()\n",
        "        self.dropout2 = nn.Dropout(p=0.2)\n",
        "\n",
        "        self.layer3 = nn.Linear(32, 32)  # Fix: Output should be 32\n",
        "        self.activation3 = nn.SiLU()  # Fix: Apply SiLU activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation1(self.layer1(x))\n",
        "        x = self.dropout2(self.activation2(self.layer2(x)))\n",
        "        x = self.activation3(self.layer3(x))  # Fix: Apply activation & dropout\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXAE9Vzwcn2T",
        "outputId": "5541a00b-ec38-4c32-e4c2-0f6f8998f8fc"
      },
      "outputs": [],
      "source": [
        "def compute_l1(distances):\n",
        "\n",
        "    l1_dist = np.sqrt(distances)\n",
        "    model = MLP_Wv(l1_dist.shape[0])\n",
        "    # Convert input to tensor and pass it through the model\n",
        "    l1_dist_tensor = torch.tensor(l1_dist, dtype=torch.float32)\n",
        "    value_Wv = model(l1_dist_tensor).detach().numpy()\n",
        "\n",
        "    return value_Wv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP_Wy(nn.Module):\n",
        "    \"\"\"\n",
        "    Instantiate MLP with input_dim=32 (from Wy)\n",
        "    Expected output shape: (60,)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "        super(MLP_Wy, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)  \n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)  # Shape (60, 1)\n",
        "        return x.squeeze(1)  # Shape (60,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_wy(indices):\n",
        "\n",
        "    y_i = train_y[\"isFraud\"].loc[indices].values\n",
        "    feature_target_tensor = torch.tensor(y_i, dtype=torch.long)\n",
        "\n",
        "    # Define Wy: An embedding layer to map to 32-dim space\n",
        "    embedding_dim = 32\n",
        "    num_classes = 2  # Since input values are 0 or 1\n",
        "\n",
        "    Wy = nn.Embedding(num_classes, embedding_dim)\n",
        "    mlp = MLP_Wy(input_dim=embedding_dim)\n",
        "\n",
        "    # Compute embeddings using Wy\n",
        "    embeddings = Wy(feature_target_tensor)  # Shape: (60, 32)\n",
        "\n",
        "    # Pass embeddings through MLP\n",
        "    value_Wy = mlp(embeddings)  # Shape: (60,)\n",
        "\n",
        "    return value_Wy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_imp_features(indices):\n",
        "    \"\"\"\n",
        "    Transform the features C13, V90, D1, and C1 using MLP_Wy\n",
        "    \"\"\"\n",
        "\n",
        "    v_86 = train_df['C13'].loc[indices].values\n",
        "    feature_target_tensor = torch.tensor(v_86, dtype=torch.float32).unsqueeze(-1)\n",
        "\n",
        "    v_87 = train_df['V90'].loc[indices].values\n",
        "    feature_target_tensor_1 = torch.tensor(v_87, dtype=torch.float32).unsqueeze(-1)\n",
        "\n",
        "    v_79 = train_df['D1'].loc[indices].values\n",
        "    feature_target_tensor_2 = torch.tensor(v_79, dtype=torch.float32).unsqueeze(-1)\n",
        "\n",
        "    v_94 = train_df['C1'].loc[indices].values\n",
        "    feature_target_tensor_3 = torch.tensor(v_94, dtype=torch.float32).unsqueeze(-1)\n",
        "\n",
        "    # Convert to PyTorch tensor\n",
        "    input_dim = 1  # Since each input is a single continuous value\n",
        "    embedding_dim = 32\n",
        "    W_feat = nn.Linear(input_dim, embedding_dim)\n",
        "    mlp = MLP_Wy(input_dim=embedding_dim)\n",
        "\n",
        "    # Process first variable\n",
        "    embeddings_1 = W_feat(feature_target_tensor)  \n",
        "    feat_x1 = mlp(embeddings_1)  \n",
        "    feat_x1 = feat_x1.detach().numpy()\n",
        "    # Process second variable\n",
        "    embeddings_2 = W_feat(feature_target_tensor_1)  \n",
        "    feat_x2 = mlp(embeddings_2)  \n",
        "    feat_x2 = feat_x2.detach().numpy()\n",
        "    # Process third variable\n",
        "    embeddings_3 = W_feat(feature_target_tensor_2)  \n",
        "    feat_x3 = mlp(embeddings_3)  \n",
        "    feat_x3 = feat_x3.detach().numpy()\n",
        "    # Process fourth variable\n",
        "    embeddings_4 = W_feat(feature_target_tensor_3)  \n",
        "    feat_x4 = mlp(embeddings_4)  \n",
        "    feat_x4 = feat_x4.detach().numpy()\n",
        "\n",
        "\n",
        "    return feat_x1, feat_x2, feat_x3, feat_x4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_value(value_Wv, value_Wy):\n",
        "    \"\"\"\n",
        "    Compute the dot product of value_Wv and value_Wy\n",
        "    \"\"\"    \n",
        "    value_Wy_npy = value_Wy.detach().numpy()\n",
        "    value = value_Wy_npy + value_Wv\n",
        "    return value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Reshaping S to do S * V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_z_in(S_x_xi, value):\n",
        "    \"\"\"\n",
        "    z_in is the intergrated(weighted sum) result of value and similarity componenet\n",
        "    \"\"\"\n",
        "    S_x_xi = S_x_xi.reshape(1, -1)\n",
        "\n",
        "    numerator = np.sum(S_x_xi @ value)  # Sum the weighted contributions (scalar)\n",
        "    denominator = np.sum(S_x_xi)        # Total sum of weights (scalar)\n",
        "    z_in = numerator / denominator      # Weighted average as a single scalar\n",
        "\n",
        "    return z_in\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Processing Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# test_i = test_df.iloc[10].values #shape (182,)\n",
        "# query_vector = test_embeddings[10] #shape (32,)\n",
        "# distances, indices = search_faiss(query_vector) # both shape (120,) and flatten\n",
        "# S_x_xi, distances = compute_similarity(distances)\n",
        "# S_x_xi, indices, distances = filter_by_mask(S_x_xi, indices, distances)\n",
        "# value_Wv = compute_l1(distances) #shape (32,)\n",
        "# value_Wy = compute_wy(indices) #shape (32,)\n",
        "\n",
        "\n",
        "# feat_x_1, feat_x2, feat_x3, feat_x4 = compute_imp_features(indices=indices)\n",
        "# #z_in \n",
        "# f_z_1 = compute_z_in(S_x_xi, feat_x_1)\n",
        "# f_z_2 = compute_z_in(S_x_xi, feat_x2)\n",
        "# f_z_3 = compute_z_in(S_x_xi, feat_x3)\n",
        "# f_z_4 = compute_z_in(S_x_xi, feat_x4)\n",
        "\n",
        "#f_z_1, f_z_2, f_z_3, f_z_4\n",
        "\n",
        "# value = compute_value(value_Wv, value_Wy)\n",
        "# value.shape\n",
        "\n",
        "# labels = test_y['isFraud']\n",
        "# labels = torch.tensor(test_y['isFraud'].values, dtype=torch.float32)\n",
        "# type(labels)\n",
        "\n",
        "# type(test_df.iloc[34].values)\n",
        "\n",
        "\n",
        "#----------------Without Batch Processing----------------\n",
        "# def process_samples(test_df, test_embeddings):\n",
        "#     \"\"\"Processes all samples and returns input tensor and labels.\"\"\"\n",
        "#     input_list = []\n",
        "#     labels = []\n",
        "\n",
        "\n",
        "#     for i in range(len(test_df)):  # Process all samples\n",
        "#         test_i = torch.tensor(test_df.iloc[i].values, dtype=torch.float32)  # shape (182,)\n",
        "#         query_embedding = torch.tensor(test_embeddings[i], dtype=torch.float32)  # shape (32,)\n",
        "\n",
        "#         distances, indices = search_faiss(query_embedding)\n",
        "#         S_x_xi, distances = compute_similarity(distances)\n",
        "#         S_x_xi, indices, distances = filter_by_mask(S_x_xi, indices, distances)\n",
        "#         value_Wv = compute_l1(distances)  # shape (32,)\n",
        "#         value_Wy = compute_wy(indices)  # shape (32,)\n",
        "#         value = compute_value(value_Wv, value_Wy)\n",
        "#         z_in = compute_z_in(S_x_xi, value)  # Scalar (single value)\n",
        "\n",
        "#         feat_x_1, feat_x2, feat_x3, feat_x4 = compute_imp_features(indices=indices) #v86 feature\n",
        "#         f_z_in_1 = compute_z_in(S_x_xi, feat_x_1)\n",
        "\n",
        "#         f_z_in_2 = compute_z_in(S_x_xi, feat_x2)\n",
        "\n",
        "#         f_z_in_3 = compute_z_in(S_x_xi, feat_x3)\n",
        "\n",
        "#         f_z_in_4 = compute_z_in(S_x_xi, feat_x4)\n",
        "\n",
        "#         # Append only test_i (input_emb) and z_in (weighted_avg), not query_embedding\n",
        "#         z_in = torch.tensor(z_in, dtype=torch.float32)\n",
        "#         f_z_in_1 = torch.tensor(f_z_in_1, dtype=torch.float32)\n",
        "#         f_z_in_2 = torch.tensor(f_z_in_2, dtype=torch.float32)\n",
        "#         f_z_in_3 = torch.tensor(f_z_in_3, dtype=torch.float32)\n",
        "#         f_z_in_4 = torch.tensor(f_z_in_4, dtype=torch.float32)\n",
        "\n",
        "#         input_list.append((test_i, z_in, f_z_in_1, f_z_in_2, f_z_in_3, f_z_in_4))\n",
        "\n",
        "#         labels.append(torch.tensor(test_y.iloc[i]['isFraud'], dtype=torch.float32))  # Assuming label is in df\n",
        "\n",
        "\n",
        "\n",
        "#     return input_list, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_samples(train_df, train_embeddings, batch_size=1000, delay=0.5):\n",
        "    \"\"\"\n",
        "    Processes samples in batches to prevent memory overflow.\n",
        "    \"\"\"\n",
        "    input_list = []\n",
        "    labels = []\n",
        "\n",
        "    total_samples = len(train_df)\n",
        "    num_batches = (total_samples + batch_size - 1) // batch_size  # Compute number of batches\n",
        "\n",
        "    for batch_idx in range(num_batches):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min((batch_idx + 1) * batch_size, total_samples)\n",
        "\n",
        "        batch_inputs = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for i in range(start_idx, end_idx):  # Process each sample in batch\n",
        "            test_i = torch.tensor(train_df.iloc[i].values, dtype=torch.float32)  # shape (182,)\n",
        "            query_embedding = torch.tensor(train_embeddings[i], dtype=torch.float32)  # shape (32,)\n",
        "\n",
        "            distances, indices = search_faiss(query_embedding)\n",
        "            S_x_xi, distances = compute_similarity(distances)\n",
        "            S_x_xi, indices, distances = filter_by_mask(S_x_xi, indices, distances)\n",
        "            value_Wv = compute_l1(distances)  # shape (32,)\n",
        "            value_Wy = compute_wy(indices)  # shape (32,)\n",
        "            value = compute_value(value_Wv, value_Wy)\n",
        "            z_in = compute_z_in(S_x_xi, value)  # Scalar (single value)\n",
        "\n",
        "            feat_x_1, feat_x2, feat_x3, feat_x4 = compute_imp_features(indices=indices)\n",
        "            f_z_in_1 = compute_z_in(S_x_xi, feat_x_1)\n",
        "            f_z_in_2 = compute_z_in(S_x_xi, feat_x2)\n",
        "            f_z_in_3 = compute_z_in(S_x_xi, feat_x3)\n",
        "            f_z_in_4 = compute_z_in(S_x_xi, feat_x4)\n",
        "\n",
        "            # Convert to tensors\n",
        "            z_in = torch.tensor(z_in, dtype=torch.float32)\n",
        "            f_z_in_1 = torch.tensor(f_z_in_1, dtype=torch.float32)\n",
        "            f_z_in_2 = torch.tensor(f_z_in_2, dtype=torch.float32)\n",
        "            f_z_in_3 = torch.tensor(f_z_in_3, dtype=torch.float32)\n",
        "            f_z_in_4 = torch.tensor(f_z_in_4, dtype=torch.float32)\n",
        "\n",
        "            batch_inputs.append((test_i, z_in, f_z_in_1, f_z_in_2, f_z_in_3, f_z_in_4))\n",
        "            batch_labels.append(torch.tensor(train_y.iloc[i]['isFraud'], dtype=torch.float32))\n",
        "\n",
        "        input_list.extend(batch_inputs)\n",
        "        labels.extend(batch_labels)\n",
        "        \n",
        "        time.sleep(delay)  # Add delay to avoid system overload\n",
        "    \n",
        "    return input_list, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def val_process_samples(val_df, val_embeddings, batch_size=1000, delay=0.5):\n",
        "    \"\"\"\n",
        "    Processes samples in batches to prevent memory overflow.\n",
        "    \"\"\"\n",
        "    input_list = []\n",
        "    labels = []\n",
        "\n",
        "    total_samples = len(val_df)\n",
        "    num_batches = (total_samples + batch_size - 1) // batch_size  # Compute number of batches\n",
        "\n",
        "    for batch_idx in range(num_batches):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min((batch_idx + 1) * batch_size, total_samples)\n",
        "\n",
        "        batch_inputs = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for i in range(start_idx, end_idx):  # Process each sample in batch\n",
        "            test_i = torch.tensor(val_df.iloc[i].values, dtype=torch.float32)  # shape (182,)\n",
        "            query_embedding = torch.tensor(val_embeddings[i], dtype=torch.float32)  # shape (32,)\n",
        "\n",
        "            distances, indices = search_faiss(query_embedding)\n",
        "            S_x_xi, distances = compute_similarity(distances)\n",
        "            S_x_xi, indices, distances = filter_by_mask(S_x_xi, indices, distances)\n",
        "            value_Wv = compute_l1(distances)  # shape (32,)\n",
        "            value_Wy = compute_wy(indices)  # shape (32,)\n",
        "            value = compute_value(value_Wv, value_Wy)\n",
        "            z_in = compute_z_in(S_x_xi, value)  # Scalar (single value)\n",
        "\n",
        "            feat_x_1, feat_x2, feat_x3, feat_x4 = compute_imp_features(indices=indices) #v86 feature\n",
        "            f_z_in_1 = compute_z_in(S_x_xi, feat_x_1)\n",
        "            f_z_in_2 = compute_z_in(S_x_xi, feat_x2)\n",
        "            f_z_in_3 = compute_z_in(S_x_xi, feat_x3)\n",
        "            f_z_in_4 = compute_z_in(S_x_xi, feat_x4)\n",
        "\n",
        "            # Convert to tensors\n",
        "            z_in = torch.tensor(z_in, dtype=torch.float32)\n",
        "            f_z_in_1 = torch.tensor(f_z_in_1, dtype=torch.float32)\n",
        "            f_z_in_2 = torch.tensor(f_z_in_2, dtype=torch.float32)\n",
        "            f_z_in_3 = torch.tensor(f_z_in_3, dtype=torch.float32)\n",
        "            f_z_in_4 = torch.tensor(f_z_in_4, dtype=torch.float32)\n",
        "\n",
        "            batch_inputs.append((test_i, z_in, f_z_in_1, f_z_in_2, f_z_in_3, f_z_in_4))\n",
        "            batch_labels.append(torch.tensor(val_y.iloc[i]['isFraud'], dtype=torch.float32))\n",
        "\n",
        "        input_list.extend(batch_inputs)\n",
        "        labels.extend(batch_labels)\n",
        "        \n",
        "        time.sleep(delay)  # Add delay to avoid system overload\n",
        "    \n",
        "    return input_list, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ThreeBlockModel(nn.Module):\n",
        "    def __init__(self, input_emb_dim, hidden_dim=32, dropout_prob=0.2):\n",
        "        super(ThreeBlockModel, self).__init__()\n",
        "\n",
        "        self.input_dim = input_emb_dim + 4  # Adding 2 for weighted_avg and Shap features\n",
        "\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.LayerNorm(self.input_dim),\n",
        "            nn.Linear(self.input_dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "        )\n",
        "        \n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "        )\n",
        "        \n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "        )\n",
        "        \n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, input_emb, weighted_avg, f_z_in, f_z_in_2, f_z_in_3, f_z_in_4):\n",
        "        # Ensure correct shape for scalar inputs\n",
        "        weighted_avg = weighted_avg.unsqueeze(-1)  \n",
        "        f_z_in = f_z_in.unsqueeze(-1)  \n",
        "        f_z_in_2 = f_z_in_2.unsqueeze(-1)\n",
        "        f_z_in_3 = f_z_in_3.unsqueeze(-1)\n",
        "        f_z_in_4 = f_z_in_4.unsqueeze(-1)\n",
        "\n",
        "        # Concatenate all inputs\n",
        "        combined = torch.cat([input_emb, weighted_avg, f_z_in, f_z_in_3, f_z_in_4], dim=-1)\n",
        "\n",
        "        # Pass through MLP blocks\n",
        "        x = self.block1(combined)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Train Loss: 0.2663, Train AUCPR: 0.0416, Val Loss: 0.1347, Val AUCPR: 0.2132\n",
            "Epoch [2/100], Train Loss: 0.1319, Train AUCPR: 0.2240, Val Loss: 0.1249, Val AUCPR: 0.2882\n",
            "Epoch [3/100], Train Loss: 0.1227, Train AUCPR: 0.2833, Val Loss: 0.1178, Val AUCPR: 0.3243\n",
            "Epoch [4/100], Train Loss: 0.1183, Train AUCPR: 0.3123, Val Loss: 0.1156, Val AUCPR: 0.3345\n",
            "Epoch [5/100], Train Loss: 0.1161, Train AUCPR: 0.3262, Val Loss: 0.1143, Val AUCPR: 0.3406\n",
            "Epoch [6/100], Train Loss: 0.1148, Train AUCPR: 0.3351, Val Loss: 0.1135, Val AUCPR: 0.3448\n",
            "Epoch [7/100], Train Loss: 0.1139, Train AUCPR: 0.3421, Val Loss: 0.1129, Val AUCPR: 0.3487\n",
            "Epoch [8/100], Train Loss: 0.1131, Train AUCPR: 0.3478, Val Loss: 0.1125, Val AUCPR: 0.3522\n",
            "Epoch [9/100], Train Loss: 0.1124, Train AUCPR: 0.3530, Val Loss: 0.1121, Val AUCPR: 0.3556\n",
            "Epoch [10/100], Train Loss: 0.1119, Train AUCPR: 0.3578, Val Loss: 0.1118, Val AUCPR: 0.3584\n",
            "Epoch [11/100], Train Loss: 0.1113, Train AUCPR: 0.3624, Val Loss: 0.1115, Val AUCPR: 0.3609\n",
            "Epoch [12/100], Train Loss: 0.1108, Train AUCPR: 0.3669, Val Loss: 0.1112, Val AUCPR: 0.3634\n",
            "Epoch [13/100], Train Loss: 0.1103, Train AUCPR: 0.3712, Val Loss: 0.1110, Val AUCPR: 0.3656\n",
            "Epoch [14/100], Train Loss: 0.1099, Train AUCPR: 0.3753, Val Loss: 0.1108, Val AUCPR: 0.3673\n",
            "Epoch [15/100], Train Loss: 0.1094, Train AUCPR: 0.3794, Val Loss: 0.1106, Val AUCPR: 0.3692\n",
            "Epoch [16/100], Train Loss: 0.1090, Train AUCPR: 0.3834, Val Loss: 0.1104, Val AUCPR: 0.3709\n",
            "Epoch [17/100], Train Loss: 0.1086, Train AUCPR: 0.3874, Val Loss: 0.1102, Val AUCPR: 0.3726\n",
            "Epoch [18/100], Train Loss: 0.1082, Train AUCPR: 0.3911, Val Loss: 0.1101, Val AUCPR: 0.3740\n",
            "Epoch [19/100], Train Loss: 0.1078, Train AUCPR: 0.3947, Val Loss: 0.1099, Val AUCPR: 0.3754\n",
            "Epoch [20/100], Train Loss: 0.1074, Train AUCPR: 0.3983, Val Loss: 0.1098, Val AUCPR: 0.3767\n",
            "Epoch [21/100], Train Loss: 0.1070, Train AUCPR: 0.4017, Val Loss: 0.1096, Val AUCPR: 0.3780\n",
            "Epoch [22/100], Train Loss: 0.1066, Train AUCPR: 0.4051, Val Loss: 0.1095, Val AUCPR: 0.3789\n",
            "Epoch [23/100], Train Loss: 0.1062, Train AUCPR: 0.4085, Val Loss: 0.1094, Val AUCPR: 0.3801\n",
            "Epoch [24/100], Train Loss: 0.1058, Train AUCPR: 0.4118, Val Loss: 0.1093, Val AUCPR: 0.3813\n",
            "Epoch [25/100], Train Loss: 0.1055, Train AUCPR: 0.4151, Val Loss: 0.1092, Val AUCPR: 0.3823\n",
            "Epoch [26/100], Train Loss: 0.1051, Train AUCPR: 0.4184, Val Loss: 0.1091, Val AUCPR: 0.3834\n",
            "Epoch [27/100], Train Loss: 0.1047, Train AUCPR: 0.4217, Val Loss: 0.1090, Val AUCPR: 0.3844\n",
            "Epoch [28/100], Train Loss: 0.1043, Train AUCPR: 0.4249, Val Loss: 0.1089, Val AUCPR: 0.3855\n",
            "Epoch [29/100], Train Loss: 0.1040, Train AUCPR: 0.4282, Val Loss: 0.1088, Val AUCPR: 0.3865\n",
            "Epoch [30/100], Train Loss: 0.1036, Train AUCPR: 0.4314, Val Loss: 0.1087, Val AUCPR: 0.3877\n",
            "Epoch [31/100], Train Loss: 0.1032, Train AUCPR: 0.4346, Val Loss: 0.1086, Val AUCPR: 0.3888\n",
            "Epoch [32/100], Train Loss: 0.1029, Train AUCPR: 0.4378, Val Loss: 0.1085, Val AUCPR: 0.3899\n",
            "Epoch [33/100], Train Loss: 0.1025, Train AUCPR: 0.4409, Val Loss: 0.1084, Val AUCPR: 0.3910\n",
            "Epoch [34/100], Train Loss: 0.1022, Train AUCPR: 0.4441, Val Loss: 0.1083, Val AUCPR: 0.3923\n",
            "Epoch [35/100], Train Loss: 0.1018, Train AUCPR: 0.4473, Val Loss: 0.1082, Val AUCPR: 0.3935\n",
            "Epoch [36/100], Train Loss: 0.1015, Train AUCPR: 0.4504, Val Loss: 0.1081, Val AUCPR: 0.3948\n",
            "Epoch [37/100], Train Loss: 0.1011, Train AUCPR: 0.4534, Val Loss: 0.1081, Val AUCPR: 0.3959\n",
            "Epoch [38/100], Train Loss: 0.1008, Train AUCPR: 0.4564, Val Loss: 0.1080, Val AUCPR: 0.3969\n",
            "Epoch [39/100], Train Loss: 0.1004, Train AUCPR: 0.4593, Val Loss: 0.1079, Val AUCPR: 0.3983\n",
            "Epoch [40/100], Train Loss: 0.1001, Train AUCPR: 0.4622, Val Loss: 0.1078, Val AUCPR: 0.3994\n",
            "Epoch [41/100], Train Loss: 0.0998, Train AUCPR: 0.4650, Val Loss: 0.1078, Val AUCPR: 0.4004\n",
            "Epoch [42/100], Train Loss: 0.0995, Train AUCPR: 0.4678, Val Loss: 0.1077, Val AUCPR: 0.4015\n",
            "Epoch [43/100], Train Loss: 0.0992, Train AUCPR: 0.4705, Val Loss: 0.1076, Val AUCPR: 0.4023\n",
            "Epoch [44/100], Train Loss: 0.0989, Train AUCPR: 0.4732, Val Loss: 0.1076, Val AUCPR: 0.4034\n",
            "Epoch [45/100], Train Loss: 0.0985, Train AUCPR: 0.4758, Val Loss: 0.1075, Val AUCPR: 0.4043\n",
            "Epoch [46/100], Train Loss: 0.0982, Train AUCPR: 0.4784, Val Loss: 0.1074, Val AUCPR: 0.4051\n",
            "Epoch [47/100], Train Loss: 0.0980, Train AUCPR: 0.4809, Val Loss: 0.1074, Val AUCPR: 0.4058\n",
            "Epoch [48/100], Train Loss: 0.0977, Train AUCPR: 0.4835, Val Loss: 0.1073, Val AUCPR: 0.4065\n",
            "Epoch [49/100], Train Loss: 0.0974, Train AUCPR: 0.4860, Val Loss: 0.1073, Val AUCPR: 0.4072\n",
            "Epoch [50/100], Train Loss: 0.0971, Train AUCPR: 0.4884, Val Loss: 0.1072, Val AUCPR: 0.4080\n",
            "Epoch [51/100], Train Loss: 0.0968, Train AUCPR: 0.4908, Val Loss: 0.1072, Val AUCPR: 0.4087\n",
            "Epoch [52/100], Train Loss: 0.0965, Train AUCPR: 0.4931, Val Loss: 0.1072, Val AUCPR: 0.4094\n",
            "Epoch [53/100], Train Loss: 0.0963, Train AUCPR: 0.4954, Val Loss: 0.1071, Val AUCPR: 0.4101\n",
            "Epoch [54/100], Train Loss: 0.0960, Train AUCPR: 0.4977, Val Loss: 0.1071, Val AUCPR: 0.4105\n",
            "Epoch [55/100], Train Loss: 0.0957, Train AUCPR: 0.4999, Val Loss: 0.1071, Val AUCPR: 0.4110\n",
            "Epoch [56/100], Train Loss: 0.0954, Train AUCPR: 0.5022, Val Loss: 0.1071, Val AUCPR: 0.4114\n",
            "Epoch [57/100], Train Loss: 0.0952, Train AUCPR: 0.5043, Val Loss: 0.1071, Val AUCPR: 0.4119\n",
            "Epoch [58/100], Train Loss: 0.0949, Train AUCPR: 0.5065, Val Loss: 0.1070, Val AUCPR: 0.4126\n",
            "Epoch [59/100], Train Loss: 0.0947, Train AUCPR: 0.5087, Val Loss: 0.1070, Val AUCPR: 0.4133\n",
            "Epoch [60/100], Train Loss: 0.0944, Train AUCPR: 0.5108, Val Loss: 0.1070, Val AUCPR: 0.4141\n",
            "Epoch [61/100], Train Loss: 0.0942, Train AUCPR: 0.5130, Val Loss: 0.1070, Val AUCPR: 0.4147\n",
            "Epoch [62/100], Train Loss: 0.0939, Train AUCPR: 0.5151, Val Loss: 0.1070, Val AUCPR: 0.4154\n",
            "Epoch [63/100], Train Loss: 0.0937, Train AUCPR: 0.5173, Val Loss: 0.1070, Val AUCPR: 0.4162\n",
            "Epoch [64/100], Train Loss: 0.0934, Train AUCPR: 0.5194, Val Loss: 0.1070, Val AUCPR: 0.4168\n",
            "Epoch [65/100], Train Loss: 0.0932, Train AUCPR: 0.5214, Val Loss: 0.1070, Val AUCPR: 0.4173\n",
            "Epoch [66/100], Train Loss: 0.0929, Train AUCPR: 0.5235, Val Loss: 0.1070, Val AUCPR: 0.4177\n",
            "Epoch [67/100], Train Loss: 0.0927, Train AUCPR: 0.5256, Val Loss: 0.1070, Val AUCPR: 0.4185\n",
            "Epoch [68/100], Train Loss: 0.0924, Train AUCPR: 0.5276, Val Loss: 0.1070, Val AUCPR: 0.4192\n",
            "Epoch [69/100], Train Loss: 0.0922, Train AUCPR: 0.5296, Val Loss: 0.1070, Val AUCPR: 0.4197\n",
            "Epoch [70/100], Train Loss: 0.0919, Train AUCPR: 0.5317, Val Loss: 0.1070, Val AUCPR: 0.4202\n",
            "Epoch [71/100], Train Loss: 0.0917, Train AUCPR: 0.5337, Val Loss: 0.1070, Val AUCPR: 0.4206\n",
            "Epoch [72/100], Train Loss: 0.0915, Train AUCPR: 0.5357, Val Loss: 0.1071, Val AUCPR: 0.4209\n",
            "Epoch [73/100], Train Loss: 0.0912, Train AUCPR: 0.5376, Val Loss: 0.1071, Val AUCPR: 0.4214\n",
            "Epoch [74/100], Train Loss: 0.0910, Train AUCPR: 0.5396, Val Loss: 0.1071, Val AUCPR: 0.4219\n",
            "Epoch [75/100], Train Loss: 0.0908, Train AUCPR: 0.5415, Val Loss: 0.1071, Val AUCPR: 0.4224\n",
            "Epoch [76/100], Train Loss: 0.0905, Train AUCPR: 0.5435, Val Loss: 0.1071, Val AUCPR: 0.4229\n",
            "Epoch [77/100], Train Loss: 0.0903, Train AUCPR: 0.5453, Val Loss: 0.1072, Val AUCPR: 0.4234\n",
            "Epoch [78/100], Train Loss: 0.0901, Train AUCPR: 0.5472, Val Loss: 0.1072, Val AUCPR: 0.4240\n",
            "Epoch [79/100], Train Loss: 0.0899, Train AUCPR: 0.5491, Val Loss: 0.1072, Val AUCPR: 0.4244\n",
            "Epoch [80/100], Train Loss: 0.0896, Train AUCPR: 0.5510, Val Loss: 0.1073, Val AUCPR: 0.4250\n",
            "Epoch [81/100], Train Loss: 0.0894, Train AUCPR: 0.5528, Val Loss: 0.1073, Val AUCPR: 0.4255\n",
            "Epoch [82/100], Train Loss: 0.0892, Train AUCPR: 0.5547, Val Loss: 0.1074, Val AUCPR: 0.4260\n",
            "Epoch [83/100], Train Loss: 0.0890, Train AUCPR: 0.5565, Val Loss: 0.1074, Val AUCPR: 0.4263\n",
            "Epoch [84/100], Train Loss: 0.0887, Train AUCPR: 0.5584, Val Loss: 0.1075, Val AUCPR: 0.4268\n",
            "Epoch [85/100], Train Loss: 0.0885, Train AUCPR: 0.5602, Val Loss: 0.1075, Val AUCPR: 0.4273\n",
            "Epoch [86/100], Train Loss: 0.0883, Train AUCPR: 0.5621, Val Loss: 0.1076, Val AUCPR: 0.4277\n",
            "Epoch [87/100], Train Loss: 0.0881, Train AUCPR: 0.5639, Val Loss: 0.1077, Val AUCPR: 0.4281\n",
            "Epoch [88/100], Train Loss: 0.0879, Train AUCPR: 0.5657, Val Loss: 0.1078, Val AUCPR: 0.4287\n",
            "Epoch [89/100], Train Loss: 0.0876, Train AUCPR: 0.5675, Val Loss: 0.1078, Val AUCPR: 0.4289\n",
            "Epoch [90/100], Train Loss: 0.0874, Train AUCPR: 0.5693, Val Loss: 0.1079, Val AUCPR: 0.4291\n",
            "Epoch [91/100], Train Loss: 0.0872, Train AUCPR: 0.5710, Val Loss: 0.1080, Val AUCPR: 0.4296\n",
            "Epoch [92/100], Train Loss: 0.0870, Train AUCPR: 0.5728, Val Loss: 0.1081, Val AUCPR: 0.4302\n",
            "Epoch [93/100], Train Loss: 0.0868, Train AUCPR: 0.5745, Val Loss: 0.1082, Val AUCPR: 0.4305\n",
            "Epoch [94/100], Train Loss: 0.0866, Train AUCPR: 0.5762, Val Loss: 0.1083, Val AUCPR: 0.4307\n",
            "Epoch [95/100], Train Loss: 0.0864, Train AUCPR: 0.5779, Val Loss: 0.1085, Val AUCPR: 0.4310\n",
            "Epoch [96/100], Train Loss: 0.0861, Train AUCPR: 0.5796, Val Loss: 0.1086, Val AUCPR: 0.4311\n",
            "Epoch [97/100], Train Loss: 0.0859, Train AUCPR: 0.5813, Val Loss: 0.1087, Val AUCPR: 0.4314\n",
            "Epoch [98/100], Train Loss: 0.0857, Train AUCPR: 0.5830, Val Loss: 0.1088, Val AUCPR: 0.4314\n",
            "Epoch [99/100], Train Loss: 0.0855, Train AUCPR: 0.5847, Val Loss: 0.1090, Val AUCPR: 0.4314\n",
            "Epoch [100/100], Train Loss: 0.0853, Train AUCPR: 0.5864, Val Loss: 0.1091, Val AUCPR: 0.4317\n"
          ]
        }
      ],
      "source": [
        "def train_model(test_df, test_embeddings, model, optimizer, criterion, batch_size=256, epochs=25):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    input_list, labels = process_samples(train_df, train_embeddings)\n",
        "    val_input_list, val_labels = val_process_samples(val_df, val_embeddings)\n",
        "\n",
        "\n",
        "    # Create DataLoader with `f_z_in`\n",
        "    train_dataset = TensorDataset(\n",
        "        torch.stack([item[0] for item in input_list]),  # test_i (input_emb)\n",
        "        torch.stack([item[1] for item in input_list]),  # z_in (weighted_avg)\n",
        "        torch.stack([item[2] for item in input_list]),  # f_z_in (new scalar input)\n",
        "        torch.stack([item[3] for item in input_list]),  # f_z_in_2 (new scalar input)\n",
        "        torch.stack([item[4] for item in input_list]),  # f_z_in_3 (new scalar input)\n",
        "        torch.stack([item[5] for item in input_list]),  # f_z_in_4 (new scalar input)\n",
        "        torch.stack(labels)  # Labels\n",
        "    )\n",
        "\n",
        "    val_dataset = TensorDataset(\n",
        "        torch.stack([item[0] for item in val_input_list]),  # test_i (input_emb)\n",
        "        torch.stack([item[1] for item in val_input_list]),  # z_in (weighted_avg)\n",
        "        torch.stack([item[2] for item in val_input_list]),  # f_z_in (new scalar input)\n",
        "        torch.stack([item[3] for item in val_input_list]),  # f_z_in_2 (new scalar input)\n",
        "        torch.stack([item[4] for item in val_input_list]),  # f_z_in_3 (new scalar input)\n",
        "        torch.stack([item[5] for item in val_input_list]),  # f_z_in_4 (new scalar input)\n",
        "        torch.stack(val_labels)  # Labels\n",
        "    )\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0  # Track loss for each epoch\n",
        "        all_targets = []\n",
        "        all_outputs = []\n",
        "\n",
        "        for batch in train_dataloader:\n",
        "            input_emb, z_in, f_z_in, f_z_in_2, f_z_in_3, f_z_in_4, target = batch\n",
        "            target = target.unsqueeze(-1)  # Make target shape (batch_size, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_emb, z_in, f_z_in, f_z_in_2, f_z_in_3, f_z_in_4)  # Pass f_z_in to model\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()  # Accumulate loss for epoch\n",
        "            \n",
        "            # Collect predictions & targets for AUCPR\n",
        "            all_outputs.append(output.detach().cpu())  # Move to CPU to avoid memory issues\n",
        "            all_targets.append(target.detach().cpu())\n",
        "\n",
        "        # Compute AUCPR at the end of the epoch\n",
        "        all_outputs = torch.cat(all_outputs).numpy()\n",
        "        all_targets = torch.cat(all_targets).numpy()\n",
        "\n",
        "        aucpr = average_precision_score(all_targets, all_outputs)\n",
        "\n",
        "        # VALIDATION Step\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_val_targets = []\n",
        "        all_val_outputs = []\n",
        "\n",
        "        with torch.no_grad():  # No gradient computation for validation\n",
        "            for batch in val_dataloader:\n",
        "                input_emb, z_in, f_z_in, f_z_in_2, f_z_in_3, f_z_in_4, target = batch\n",
        "                target = target.unsqueeze(-1)\n",
        "\n",
        "                output = model(input_emb, z_in, f_z_in, f_z_in_2, f_z_in_3, f_z_in_4)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                \n",
        "                all_val_outputs.append(output.cpu())\n",
        "                all_val_targets.append(target.cpu())\n",
        "\n",
        "        # Compute AUCPR for validation set\n",
        "        all_val_outputs = torch.cat(all_val_outputs).numpy()\n",
        "        all_val_targets = torch.cat(all_val_targets).numpy()\n",
        "        val_aucpr = average_precision_score(all_val_targets, all_val_outputs)\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], \"\n",
        "              f\"Train Loss: {epoch_loss/len(train_dataloader):.4f}, Train AUCPR: {aucpr:.4f}, \"\n",
        "              f\"Val Loss: {val_loss/len(val_dataloader):.4f}, Val AUCPR: {val_aucpr:.4f}\")\n",
        "\n",
        "# Model, optimizer, and loss function\n",
        "input_emb_dim = 182  # Assuming this based on test_df features\n",
        "model = ThreeBlockModel(input_emb_dim=input_emb_dim)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
        "\n",
        "# Call training loop (assuming test_df and test_embeddings are available)\n",
        "train_model(train_df, train_embeddings, model, optimizer, criterion, epochs=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_process_samples(test_df, test_embeddings, batch_size=1000, delay=0.5):\n",
        "    \"\"\"\n",
        "    Processes samples in batches to prevent memory overflow.\n",
        "    \"\"\"\n",
        "    input_list = []\n",
        "    labels = []\n",
        "\n",
        "    total_samples = len(test_df)\n",
        "    num_batches = (\n",
        "        total_samples + batch_size - 1\n",
        "    ) // batch_size  # Compute number of batches\n",
        "\n",
        "    for batch_idx in range(num_batches):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min((batch_idx + 1) * batch_size, total_samples)\n",
        "\n",
        "        batch_inputs = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for i in range(start_idx, end_idx):  # Process each sample in batch\n",
        "            test_i = torch.tensor(test_df.iloc[i].values, dtype=torch.float32)  # shape (182,)\n",
        "            query_embedding = torch.tensor(test_embeddings[i], dtype=torch.float32)  # shape (32,)\n",
        "\n",
        "            distances, indices = search_faiss(query_embedding)\n",
        "            similarity, distances = compute_similarity(distances)\n",
        "            similarity, indices, distances = filter_by_mask(similarity, indices, distances)\n",
        "            value_Wv = compute_l1(distances)  # shape (32,)\n",
        "            value_Wy = compute_wy(indices)  # shape (32,)\n",
        "            value = compute_value(value_Wv, value_Wy)\n",
        "            z_in = compute_z_in(similarity, value)  # Scalar (single value)\n",
        "\n",
        "            feat_x_1, feat_x2, feat_x3, feat_x4 = compute_imp_features(\n",
        "                indices=indices\n",
        "            )  # v86 feature\n",
        "            f_z_in_1 = compute_z_in(similarity, feat_x_1)\n",
        "            f_z_in_2 = compute_z_in(similarity, feat_x2)\n",
        "            f_z_in_3 = compute_z_in(similarity, feat_x3)\n",
        "            f_z_in_4 = compute_z_in(similarity, feat_x4)\n",
        "\n",
        "            # Convert to tensors\n",
        "            z_in = torch.tensor(z_in, dtype=torch.float32)\n",
        "            f_z_in_1 = torch.tensor(f_z_in_1, dtype=torch.float32)\n",
        "            f_z_in_2 = torch.tensor(f_z_in_2, dtype=torch.float32)\n",
        "            f_z_in_3 = torch.tensor(f_z_in_3, dtype=torch.float32)\n",
        "            f_z_in_4 = torch.tensor(f_z_in_4, dtype=torch.float32)\n",
        "\n",
        "            batch_inputs.append((test_i, z_in, f_z_in_1, f_z_in_2, f_z_in_3, f_z_in_4))\n",
        "            batch_labels.append(torch.tensor(test_y.iloc[i][\"isFraud\"], dtype=torch.float32))\n",
        "\n",
        "        input_list.extend(batch_inputs)\n",
        "        labels.extend(batch_labels)\n",
        "\n",
        "        time.sleep(delay)  # Add delay to avoid system overload\n",
        "\n",
        "    return input_list, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test AUCPR: 0.40405246696174973\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def inference(model, test_df, test_embeddings, batch_size=256):\n",
        "    \"\"\"\n",
        "    Perform inference on test data using a trained model.\n",
        "    \n",
        "    Args:\n",
        "        model (nn.Module): Trained model.\n",
        "        test_df (pd.DataFrame): Your test dataframe.\n",
        "        test_embeddings (np.ndarray or torch.Tensor): Precomputed embeddings for test.\n",
        "        batch_size (int): Batch size for inference.\n",
        "\n",
        "    Returns:\n",
        "        all_outputs (np.ndarray): Model outputs (probabilities).\n",
        "        all_labels (np.ndarray): Ground truth labels.\n",
        "    \"\"\"\n",
        "    test_input_list, test_input_labels = test_process_samples(test_df=test_df, test_embeddings=test_embeddings)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    test_dataset = TensorDataset(\n",
        "        torch.stack([item[0] for item in test_input_list]),  # test_i (input_emb)\n",
        "        torch.stack([item[1] for item in test_input_list]),  # z_in (weighted_avg)\n",
        "        torch.stack([item[2] for item in test_input_list]),  # f_z_in (new scalar input)\n",
        "        torch.stack([item[3] for item in test_input_list]),  # f_z_in_2 (new scalar input)\n",
        "        torch.stack([item[4] for item in test_input_list]),  # f_z_in_3 (new scalar input)\n",
        "        torch.stack([item[5] for item in test_input_list]),  # f_z_in_4 (new scalar input)\n",
        "        torch.stack(test_input_labels)\n",
        "    )\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Lists to store outputs and labels\n",
        "    all_outputs = []\n",
        "    all_labels = []\n",
        "\n",
        "    # No gradient updates during inference\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_emb, z_in, f1, f2, f3, f4, labels = batch\n",
        "            outputs = model(input_emb, z_in, f1, f2, f3, f4)\n",
        "            \n",
        "            # Move outputs and labels to CPU, then store\n",
        "            all_outputs.append(outputs.cpu())\n",
        "            all_labels.append(labels.cpu())\n",
        "\n",
        "    # Concatenate everything into a single array/tensor\n",
        "    all_outputs = torch.cat(all_outputs).numpy()\n",
        "    all_labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "    return all_outputs, all_labels\n",
        "\n",
        "all_test_outputs, all_test_labels = inference(model, test_df, test_embeddings, batch_size=256)\n",
        "\n",
        "# Optionally, compute metrics (e.g., AUCPR) on your test set:\n",
        "from sklearn.metrics import average_precision_score\n",
        "test_aucpr = average_precision_score(all_test_labels, all_test_outputs)\n",
        "print(\"Test AUCPR:\", test_aucpr)\n",
        "\n",
        "# Or create a final binary prediction if desired:\n",
        "binary_predictions = (all_test_outputs >= 0.5).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value 0: 23262 occurrences\n",
            "Value 1: 360 occurrences\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Count occurrences of each unique value (0s and 1s)\n",
        "unique_values, counts = np.unique(binary_predictions, return_counts=True)\n",
        "\n",
        "# Print results\n",
        "for value, count in zip(unique_values, counts):\n",
        "    print(f\"Value {value}: {count} occurrences\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "saint_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
